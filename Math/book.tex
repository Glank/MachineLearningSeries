\documentclass[12pt]{book}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
% Use pdflscape for pdf viewers
\usepackage{pdflscape}
% Use lscape for printing as a book
%\usepackage{lscape}
\usepackage{etoolbox}
\usepackage{csquotes}
\usepackage[normalem]{ulem}
\usepackage[toc,page]{appendix}
% package used for \llparenthesis
\usepackage{stmaryrd}

\title{%
  \sout{Machine Learning for Global Conquest} \\
  Multimatrices and Their Derivatives}
\author{Ernest Kirstein}
\date{\today}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{exercise}{Exercise}[chapter]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}[chapter]
\newtheorem{drule}{Rule}

\newtheoremstyle{ppart}{}{}{}{}{}{:}{ }{}
\theoremstyle{ppart}
\newtheorem{ppart}{Part}

\newtheoremstyle{case}{}{}{}{}{}{:}{ }{}
\theoremstyle{case}
\newtheorem{case}{Case}

\newcounter{solutionctr}
\newtheoremstyle{solution}{}{}{}{}{}{:}{\newline}{
  Exercise \refstepcounter{solutionctr}\thesolutionctr~Solution}
\theoremstyle{solution}
\newtheorem{solution}{Solution}
\newcommand{\solutionsection}[1]{
  \section{Chapter~#1 Solutions}\setcounter{solutionctr}{0}}

\newenvironment{argument}{\noindent\textit{Argument.}}{\hfill$\square$}

\AtBeginEnvironment{proof}{\setcounter{ppart}{0}}
\AtBeginEnvironment{proof}{\setcounter{case}{0}}
\AtBeginEnvironment{align}{\setcounter{equation}{0}}
\AtBeginEnvironment{alignat}{\setcounter{equation}{0}}

\DeclareMathOperator{\Dim}{Dim}
\DeclareMathOperator{\Ident}{I}
\DeclareMathOperator{\Tran}{Tran}
\DeclareMathOperator{\SoS}{\mathbb{S}}
\DeclareMathOperator{\Softmax}{Softmax}
\DeclareMathOperator{\Sum}{\Sigma}
\DeclareMathOperator{\remainder}{mod}
\DeclareMathOperator{\RMult}{RMult}
\DeclareMathOperator{\LMult}{LMult}
\DeclareMathOperator{\Idx}{Idx}
\DeclareMathOperator{\size}{size}
%\newcommand{\mmult}[1]{\stackrel{\times}{#1}}
%\newcommand{\mmult}[1]{\underset{#1}{\times}}
\newcommand{\mmult}[1]{\text{\raisebox{1ex}{$\underset{#1}{\times}$}}}
\newcommand{\dmult}[1]{\text{\raisebox{1ex}{$\underset{#1}{\,\bullet\,}$}}}
\newcommand{\shape}[1]{\left|#1\right|}

\begin{document}

\maketitle

\chapter*{Preface}
I was working my way through a hobby programming project, trying to create deep fakes of
\sout{polititians in compromising positions} humorous celebrity mashups, when I ran into a problem.
The existing notation for derivatives of matrix equations is terrible.
It's so deliberately obtuse that I can only conclude that some hidden cabal of
mathematicians is actively working to sabotage \sout{the rise of our glorious AI overlords}
our machine learning research.
I, for one, will not stand for such heresy.

So, I set out to invent a system of my own. Much to my surprise, some of the
results with were rather elegant. For example, I stumbled onto a
multidimensional version of the derivative chain rule (Thm. \ref{mmm_chain_rule})
that's nearly as consise as the same rule for scalars.

The ultimate goal of this notation system was to \sout{bring about the downfall
of humanity} figure out a way to take derivatives
of matrix equations without having to break apart matricies into their constituent
parts. I knew that I had succeeded when I was able to derive the derivative of the
softmax function completely in my own notation (Thm. \ref{softmax_derivative}).

Now, not knowing what else to do with the results, I've written this book
about my exploration into multidimensional matricies,
multimatricies, and how they can elucidate derivatives of matrix equations.

\tableofcontents

\chapter{The Multimatrix and Its Derivative}

\begin{displayquote}
``If you're a bit touched, like I am, you may want to make a dedicated tool.'' -
This Old Tony, `Making Springs At Home' \cite{youtube:tony}
\end{displayquote}

\section{Motivating the Multimatrix}

To demonstrate how the problem with traditional matricies, let's take a look at a
simple matrix equation. 

Let's say we have a small class of undergraduates packed, conveniently in a 3 by 5
grid. We'll let $X$ be the matrix representing number of semesters of science
classes each of those student has taken.
Then let $B$ represent the belief each of those undergradutes has in a higher power,
thier BHP if you will.

As we all know, Weinersmith's hypothisis states that an undergraduate's belief in a
higher power fluctuates with respect to their scientific education in a sinusoidal
pattern (Weinersmith, 2010 \cite{weinersmith:education}).

Let's formally state the shape of these two matricies,
$\shape{B} = \shape{X} = \left< 3,5 \right>$. (Throughout this work, I'll be using
$\shape{M}$ to denote the shape of $M$ rather than the determinate because, frankly, it
is just comes up more frequently.)
Each element of $B$ will be the sine of the corresponding element of
$X$. That is, \[B_{r,c} = \sin(X_{r,c})\]
Which I'll write more simply as, \[B = \sin(X)\]

Now, I would call the derivative of any element of $B$ with respect to any element
of $X$ a partial derivative of that function. Since
$\partial B_{r_1, c_1}/\partial X_{r_2, c_2}$ is defined as the rate of change of
\textit{one} undergraduate's BPH with respect to \textit{one}
undergraduate's education level - it only represents a change to part of the whole
system. 
We can calculate this partial derivative easily enough,
\[
\frac{\partial B_{r_1,c_1}}{\partial X_{r_2,c_2}} = 
\left\{
  \begin{array}{ll}
    \cos(X_{r_2,c_2})  & \mbox{if } r_1 = r_2 \mbox{ and } c_1 = c_2 \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\]
Every student's BHP changes with respect to their own
scientific education level but does not change with respect to other students'
scientific education. No mater how much I teach Jane about photosynthesis, I'm
not going to change Joe's mind about divine mysteries.

This can also be writen more simply using Kronecker deltas \cite{wiki:kronecker},
which should make one feel particularly intellegent as Kronecker deltas were used
by Einstein (and presumably someone named Kronecker) \cite{wiki:einstein}.
\[
\frac{\partial B_{r_1,c_1}}{\partial X_{r_2,c_2}} = 
\cos (X_{r_2, c_2}) \delta_{r_1 r_2} \delta_{c_1 c_2}
\]

But what about the complete deriative?
What sort of mathmatical object would represent the entire derivative
of $B$ with respect to $X$?
I need a mathematical object which perfectly
encapsulates the rate of change of \textit{all} the undergraduates' beliefs in a
higher power with respect to \textit{every} undergraduates' scientific education
and my current toolkit doesn't work!

\[\frac{dB}{dX} = \mbox{ ? }\]

If this were a function that mapped a vector to a scalar, you might say that the
complete derivate was the gradient. But in this case, our function's output is more
than a simple scalar and even our dependent variable is more complex than a vector.

By looking at the partial derivatives, we can come to some understanding of the
dimensionality of the complete derivative. Each combination of input and output
element has its own partial derivative, so it makes sense that the total size
of the complete derivative should be the product of the size of $B$ and the size
of $X$.

And so, the size of the complete derivate would be,
\[
\size\left(\frac{dB}{dX}\right) = \size(B) \size(X)
\]
Perhaps we could even let this help us define the `shape' of the complete
derivative. Note that there seem to be four independent indicies to each element
of this matrix, namely $r_1$, $c_1$, $r_2$, and $c_2$.
And since a complete derivative would need to be composed of partial derivatives
it would make sense to just smush the shapes of the input matrix and the output
matrix together,
\begin{align*}
\shape{\frac{dB}{dX}} &= \shape{B} \oplus \shape{X} \\
 &= \left< 3, 5 \right> \oplus \left< 3, 5 \right> \\
 &= \left< 3, 5, 3, 5 \right>
\end{align*}
Where $\oplus$ is the concatenate operator.

The ordering here is arbitrary. There is no reason why the dimensionality of
of the independent variable \textit{needs} to be concatenated to the right of the
dimensionality of the output. What's important is that the dimensionality of
both output and indepenedent variable are preserved and that our notation is
consistent.

Let us declare this to
be our convention. I've typeset it in \LaTeX ~therefore it can no longer be
changed. Those are the rules that I've just made up and I'm backing them up with
dozens more pages of incoherent rambling.

So anyways, our total derivate is a multidimensional object with four indicies.
Suppose we try to pull a single partial derivative from this complete derivative,
what could our notation look like? Four underscript indicies would be unwieldy,
so maybe something like this:
\[
\frac{dB}{dX}[1,2,3,4] = \frac{\partial B_{1,2}}{\partial X_{3,4}}
\]

And since we're defining our own notation, let's go wild and reference our original
matricies in a similar manner:
\[
\frac{dB}{dX}[1,2,3,4] = \frac{\partial B[1,2]}{\partial X[3,4]}
\]

So what is this object we've defined? In all ways that matter, it seems to be
a multidimensional matrix. It has four dimensions of indices, rather than just
a row and column but each uniquely indexed cell in this object may have a
unique value, just like a standard two dimensional matrix.

Programmers might call this object a `tensor' but mathematitians might
scream at those same programmers, ``WE ALREADY HAVE A DEFINITION OF A TENSOR! DIE!''
And the ensuing war might wipe out most of the eastern seaboard and would
certainly not be conductive to a reasonable discussion. To avoid such a
confrontation I'll be using the incredibly imaginative term `multimatrix'. 

Let's define this bit of notation formaly.

\begin{definition}[Multimatrix]
A multimatrix $M$ is a multidimensional object that has some shape,
$\vec{v} \in \mathbb{N}^n$, where $n \in \mathbb{N} \cup \{0\}$
is the number of dimensions of $M$.

Let $\vec{v}$ be the vector which describes the shape of
$M$, then $\vec{v} = \shape{M}$. And for any other $\bar{v} \in \mathbb{N}^n$, $\bar{v}$
is a valid index of $M$ if and only if each value of $\bar{v}$ is less than or equal
to the value of it's corresponding index in $\vec{v}$.
That is, $\forall i: 1 \le \bar{v}_i \le \vec{v}_i$.

We can further simplify that notation for our set of valid indicies. Let's say we have
some $\vec{v} \in \mathbb{N}^n$ that defines the shape of some matrix. Then we'll
define the set of valid indicies of that matrix to be $\Dim(\vec{v})$. 

Each value of $M$ is a real value, so we'll define the set of all real valued
multidimensional matricies with shape $\vec{v}$ as $\mathbb{R}^{\vec{v}}$.

We'll use the notation $M[\bar{v}]$ to reference any individual scalar value of $M$
at indicies $\bar{v} \in \Dim(\shape{M})$. 
\end{definition}

That's a huge informtion dump so look at an example.

\begin{example}
Let $M$ be one of these newfangled `multimatrix' things. According to our
definition, we also need to define a shape for the multimatrix. Let's
say that our shape is $\left<5,7,9,2,3\right>$. In our notation, that
means that,
\[ \shape{M} = \left<5,7,9,2,3\right> \]
Let's name this shape vector, $\shape{M} = \vec{m}$. Notice that this is
a 5 dimensional multimatrix because $\shape{\vec{m}} = 5$. Also note the
subtle difference in $\shape{\shape{M}}$ which is 5 and $\shape{M}$ which is $\vec{m}$.

Let's look at some individual elements of $M$. We can say that,
\[ M[5,4,3,2,1] = 3 \]
And,
\[ M[4,3,2,1,1] = -8.5 \]
We can say that because we're just making up this multimatrix, but also
because $\left<5,4,3,2,1\right>$ and $\left<4,3,2,1,1\right>$
are both in $\Dim(\vec{m})$. That is to say, they're both valid
indexes of $M$.

Remember that $\Dim(\vec{m})$ is the set of valid indexes of $M$.
You can tell that the size of  $\Dim(\vec{m})$ is
$5 \times 7 \times 9 \times 2 \times 3 = 1890$, which is the number of
elements of $M$.

If $\left<5,4,3,2,1\right> = \bar{a}$ and $\left<4,3,2,1,1\right> = \bar{b}$
then we can say that $\bar{a} \in \Dim(\vec{m})$ and $\bar{b} \in \Dim(\vec{m})$
because both are valid indexes of $M$.

We can uses the indexes to reference $M$ like this,
\[ M[\bar{a}] = 3 \]
And this,
\[ M[\bar{b}] = -8.5 \]
\end{example}

\begin{definition}[Scalar Filled Multimatrices]
For convenience, I'll often refer to multimatricies filled with a single value,
$s \in \mathbb{R}$,
using the notation $s^{\vec{x}}$ where $\vec{x}$ is some multimatrix shape. So that
$\shape{s^{\vec{x}}} = \vec{x}$ and for all $\bar{x} \in \Dim(\vec{x})$,
\[ s^{\vec{x}}[\bar{x}] = s \]

For example, $0^{\left<2,3,2\right>}$ is a multimatrix with shape $\left<2,3,2\right>$
that is filled with zeros.
\end{definition}

Now that you seem to understand.. Don't give me that look... well if you're confused
you should have been asking questions. I don't have time to backtrack now.
Well, seeing as \textit{most of you} seem to have understood, I'll give you a theorem to prove.

\begin{theorem}[Dimensional Set and Index Concatenation]
\label{dim_set_ind_concat_thm}
If $\bar{a} \in \Dim(\vec{a})$ and $\bar{b} \in \Dim(\vec{b})$ then,
\[ \bar{a} \oplus \bar{b} \in \Dim(\vec{a} \oplus \vec{b}) \]
\end{theorem}
\begin{proof}
Left as an exercise for the reader (Exercise \ref{dim_set_ind_concat_ex}).
\end{proof}

\section{Defining the Derivative}

Going back to our original motivating problem, we (that is to say, myself and the voices
in my head whom I reference as to give my writing the percieved authority
of more than one author) can express our derivative notation thusly,

\begin{definition}[Multimatrix on Multimatrix Derivative]
\label{mm_derivative}
Let $F(X)$ be some function from $\mathbb{R}^{\vec{x}}$ to $\mathbb{R}^{\vec{f}}$.
The complete derivative of $F$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{f} \oplus \vec{x}}$. That is,
\[ \shape{F(X)} = \vec{f} \]
\[ \shape{X} = \vec{x} \]
\[ \shape{\frac{dF}{dX}} = \vec{f} \oplus \vec{x} \]
\[
\forall \bar{f} \in \Dim(\vec{f}),
        \bar{x} \in \Dim(\vec{x}):
\]
\[
\frac{dF}{dX}[\bar{f} \oplus \bar{x}] =
\frac{\partial F[\bar{f}]}{\partial X[\bar{x}]}
\]
\end{definition}

But, of course, there are other combinations of functions than from one
multimatrix to another. You might have a function from a multimatrix to a
scalar (such as summing up all the values of a multimatrix), or from a scalar
to a multimatrix (such as filling a multimatrix with a single scalar value).

We can define the total derivative for those cases as well.

\begin{definition}[Scalar on Multimatrix Derivative]
\label{sm_derivative}
Let $f(X)$ be some function from $\mathbb{R}^{\vec{x}}$ to $\mathbb{R}$.
The complete derivative of $f$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{x}}$. That is,
\begin{align*}
f(X) &\in \mathbb{R} \\
\shape{X} &= \vec{x} \\
\shape{\frac{df}{dX}} &= \vec{x}
\end{align*}
\[
\forall \bar{x} \in \Dim(\vec{x}):
        \frac{df}{dX}[\bar{x}] =
        \frac{\partial f}{\partial X[\bar{x}]}
\]
\end{definition}

\begin{definition}[Multimatrix on Scalar Derivative]
\label{ms_derivative}
Let $F(x)$ be some function from $\mathbb{R}$ to $\mathbb{R}^{\vec{f}}$.
The complete derivative of $F$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{f}}$. That is,
\begin{align*}
\shape{F(x)} &= \vec{f} \\
x &\in \mathbb{R} \\
\shape{\frac{dF}{dx}} &= \vec{f}
\end{align*}
\[
\forall \bar{f} \in \Dim(\vec{f}):
        \frac{dF}{dx}[\bar{f}] =
        \frac{\partial F[\bar{f}]}{\partial x}
\]
\end{definition}

Ok, that's neat and all but what does that actually mean? Can we answer our motivating 
question?
\[\frac{dB}{dX} = ? \]
Well... sort of. We have to abuse the notation a little and say that $B$ and $X$ are
just multimatricies with 2 dimensions. Then we can say that the total derivative
of $B$ with respect to $X$ is a multimatrix with shape $\shape{B} \oplus \shape{X}$,
whose elements we can define as,
\[\forall \bar{b} \in \Dim(\shape{B}), \bar{x} \in \Dim(\shape{X}):\]
\begin{align*}
\left( \frac{dB}{dX} \right)[\bar{b} \oplus \bar{x}]
&= \frac{\partial B[\bar{b}]}{\partial X[\bar{x}]} \\
&= \delta_{\bar{b}\bar{x}}\cos(\bar{x})
\end{align*}

Which isn't a huge improvement over the original notation. What would be great is
if we could define the total derivative without going down into the partial derivatives
at all, which we'll be able to do in later chapters. As a sneak peak (a whole
chapter ahead), that'll look like this:
\[
\frac{dB}{dX} = \Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} \cos(X)
\]

But that single line will require defining multimatrix multiplication and
a mysterious, and possibly dangerious, entity call a cubic identity, $\Ident^3(\shape{X})$.

\section{Exercises}

\begin{exercise}
\label{dim_set_ind_concat_ex}
Prove Theorem \ref{dim_set_ind_concat_thm}. Which is that,
\[ \bar{a} \oplus \bar{b} \in \Dim(\vec{a} \oplus \vec{b}) \]
\end{exercise}

\begin{exercise}
True or false?
\begin{enumerate}
\item $\left<1,1,1\right> \in \Dim(\left<5,5,5\right>)$
\item $\left<1,1\right> \in \Dim(\left<5,5,5\right>)$
\item $\left<1,2,3\right> \in \Dim(\left<3,2,1\right>)$
\item $\left<3,7\right> \in \Dim(\left<3,7\right>)$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $F(X)$ be a function from $\mathbb{R}^{\left<3,7,2\right>}$ to
$\mathbb{R}^{\left<6, 7\right>}$. That is, $\shape{F(X)} = \left<6,7\right>$
and for any valid input to $F(X)$, $\shape{X} = \left<3,7,2\right>$.
Then what is the shape of $dF/dX$?
\end{exercise}

\begin{exercise}
Let $f(X)$ be the sum of all values of $X$, where
$X \in \mathbb{R}^{\left<2,3,2\right>}$. That is,
\[ f(X) = \sum_{\forall \bar{x} \in \Dim(\left<2,3,2\right>)} X[\bar{x}] \]
What is the shape of $df/dX$? If $\bar{v} \in \Dim(\shape{df/dX})$ then what is
\[ \frac{df}{dX}[\bar{v}] = ? \]
\end{exercise}

\chapter{Multimatrix Multiplication}

\begin{displayquote}
``The universe is a cruel, uncaring void. The key to being happy isn't a search
for meaning. It's to just keep yourself busy with unimportant nonsense, and eventually,
you'll be dead.'' - Mr. Peanutbutter (BoJack Horseman Season 1, Episode 12
\cite{bojack})
\end{displayquote}

\section{Defining Multiplication}

The multiplication of traditional matricies is often considered a rather confusing
proposition, especially when one first encounters it.

After seeing the method used in practice, one slowly comes to understand the
reason behind the madness. That is, that madness is the natural state of things
and matrix multiplication is used to punish those few brave souls who dare to
fight against the darkness.

Multimatrix multiplication is much the same.

Just to review, for two matricies $A$ and $B$, multipication is only defined
for $A \times B = C$ if $A$ has the same number of columns as $B$ has rows.
That is, if $\shape{A} = \left< r_a, c_a \right>$ and $\shape{B} = \left< r_b, c_b \right>$
then $A \times B$ is valid if and only if $c_a = r_b$. And furthermore,
the remaining dimensions of $A$ and $B$ define the diminsions of $C$,
$\shape{C} = \left< r_a, c_b \right>$. Each element of $C$ is a sum of the product of
every combination of the corresponding row of $A$ and the corresponding column of
$B$,

\[ C[r_c, c_c] = \sum_{\forall i} A[r_c, i] B[i, c_c] \]

Easily done. The hard part isn't doing the calculation. The hard part getting over
your naive belief that mathematical operations should be rooted in some concrete
concept rather than ``this is the way it's done because it makes the equations
work.''

So now consider how we would multiply two matricies $A$ and $B$ if they had more
than two dimensions. Let's use a concrete example to make the problems more
structurally sound.

Let $\shape{A} = \left<5,7,7\right>$ and $\shape{B} = \left<7,7,2,4\right>$.
First of all, can we even multiply the two together? Hm... maybe? $\shape{A}$ ends with
$\left<\ldots, 7\right>$ and $\shape{B}$ starts with $\left<7,\ldots\right>$ so it would
make sense that we can somehow squish them together into something. By that reasoning,
the shape of our after-multiplication output would use the remaining dimensions
$\left<5, 7, 7, 2, 4\right> = \shape{A \times B}$.

But $\shape{A}$ also ends with $\left<\ldots, 7, 7\right>$ and $\shape{B}$ starts with
$\left<7, 7\ldots\right>$ so maybe our multiplication operation should work on that
additional dimension, leaving us with $\left<5, 2, 4\right> = \shape{A \times B}$.
But the first definition looks like it could still be useful! It could let us cover 
two dimensional matrix multiplication completely within our definition. This second
definition seems more... er... multidimensional... but it also seems to
arbitrarily depend on the shape of the two matricies.

What we really need is an operation which covers both lines of reasoning. To do
so, we need to compleately abandon conventional algebras, and indeed any hope of
returning to sanity. We need... a ternary operator.

I would argue that there shall always be some component of, 
``How many dimensions are you combining?'' inherent
in the operation. So we can distinguish our two types of multiplication, and indeed
an infinite number of types of multiplication, with a new operator, $\mmult{n}$ where
$\shape{A \mmult{1} B} = \left<5, 7, 7, 2, 4\right>$ and 
$\shape{A \mmult{2} B} = \left<5, 2, 4\right>$.

But the shape is only a small component of the multipliation  operation. Let's
just jump into the formal definition and I'll explain with an example:

\begin{definition}[Multimatrix Multiplication]
\label{mm_mult}
Let $A$ and $B$ be multimatricies such that $\shape{A} = \vec{a} \oplus \vec{c}$,
$\shape{B} = \vec{c} \oplus \vec{b}$, and $\shape{\vec{c}} = n$ with
$n \in \{0\} \cup \mathbb{N}$. We shall call $n$ the 'size' or the 'width'
or the 'overlap' of the multiplication operation.
The shape of the result is defined, $\shape{A \mmult{n} B} = \vec{a} \oplus \vec{b}$
and the value of the result is defined,
\[
\forall \bar{a} \in \Dim(\vec{a}), \bar{b} \in \Dim(\vec{b}):
(A \mmult{n}  B)[\bar{a} \oplus \bar{b}] =
\sum_{\forall \bar{c} \in \Dim(\vec{c})}
  A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}]
\]
\end{definition}

\newpage
\begin{example}
Let $\shape{A} = \left<2, 3, 2\right>$ and $\shape{B} = \left<3, 2, 2\right>$. I would
define their values in a three dimensional grid but I fear that my three dimensional
\LaTeX skills aren't up to the task (and three dimensional monitors are quite hard to
come by). So here is where we leave behind nice graphical representations of
things and I'll just define the elements of $A$ and $B$ in the tables below.
\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c | c}
$\bar{a}_1$ & $\bar{a}_2$ & $\bar{a}_3$ & $A[\bar{a}]$ \\
\hline
1           & 1           & 1           & -0.5         \\
1           & 1           & 2           & 1            \\
1           & 2           & 1           & 2            \\
1           & 2           & 2           & -1           \\
1           & 3           & 1           & 5            \\
1           & 3           & 2           & 1            \\
2           & 1           & 1           & 3            \\
2           & 1           & 2           & -7           \\
2           & 2           & 1           & 5            \\
2           & 2           & 2           & 2            \\
2           & 3           & 1           & -3           \\
2           & 3           & 2           & -1
\end{tabular}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c | c}
$\bar{b}_1$ & $\bar{b}_2$ & $\bar{b}_3$ & $B[\bar{b}]$ \\
\hline
1           & 1           & 1           & 1            \\
1           & 1           & 2           & 0            \\
1           & 2           & 1           & 1            \\
1           & 2           & 2           & 0            \\
2           & 1           & 1           & 1            \\
2           & 1           & 2           & 0            \\
2           & 2           & 1           & 1            \\
2           & 2           & 2           & 0            \\
3           & 1           & 1           & 1            \\
3           & 1           & 2           & 0            \\
3           & 2           & 1           & 1            \\
3           & 2           & 2           & 0
\end{tabular}
\end{center}
\end{table}

Given $\shape{A}$ and $\shape{B}$, and by our definition of multimatrix multiplication,
you can see that $\shape{A \mmult{2} B} = \left<2, 2\right>$. And the values of the
resulting multimatrix are defined:

\begin{table}[h!]
\begin{center}
\begin{tabular}{c c | c | c | c}
$\bar{x}_1$ & $\bar{x}_2$ & $(A \mmult{2} B)[\bar{x}]$ & = & = \\
\hline
1 & 1 &
  $A[1,1,1]B[1,1,1] + A[1,1,2]B[1,2,1] +$ & $(-0.5)(1) + (1)(1) +$ & 7.5 \\
&&$A[1,2,1]B[2,1,1] + A[1,2,2]B[2,2,1] +$ & $(2)(1) + (-1)(1) +$ & \\
&&$A[1,3,1]B[3,1,1] + A[1,3,2]B[3,2,1]$   & $(5)(1) + (1)(1)$ & \\
1 & 2 &
  $A[1,1,1]B[1,1,2] + A[1,1,2]B[1,2,2] +$ & $(-0.5)(0) + (1)(0) +$ & 0 \\
&&$A[1,2,1]B[2,1,2] + A[1,2,2]B[2,2,2] +$ & $(2)(0) + (-1)(0) +$ & \\
&&$A[1,3,1]B[3,1,2] + A[1,3,2]B[3,2,2]$   & $(5)(0) + (1)(0)$ & \\
2 & 1 &
  $A[2,1,1]B[1,1,1] + A[2,1,2]B[1,2,1] +$ & $(3)(1) + (-7)(1) +$ & 1 \\
&&$A[2,2,1]B[2,1,1] + A[2,2,2]B[2,2,1] +$ & $(5)(1) + (2)(1) +$ & \\
&&$A[2,3,1]B[3,1,1] + A[2,3,2]B[3,2,1]$   & $(-3)(1) + (1)(1)$ & \\
2 & 2 &
  $A[2,1,1]B[1,1,2] + A[2,1,2]B[1,2,2] +$ & $(3)(0) + (-7)(0) +$ & 0 \\
&&$A[2,2,1]B[2,1,2] + A[2,2,2]B[2,2,2] +$ & $(5)(0) + (2)(0) +$ & \\
&&$A[2,3,1]B[3,1,2] + A[2,3,2]B[3,2,2]$   & $(-3)(0) + (1)(0)$ &
\end{tabular}
\end{center}
\end{table}

\end{example}
\newpage

It's important to notice that this operation is identital to standard
matrix multiplication for two dimensional multimatrices. For instance,
if $A$ and $B$ are two dimensional multimatricies with shapes
$\shape{A} = \left<l_a, r_a\right>$ and $\shape{B} = \left<l_b, r_b\right>$
their product $A \mmult{1} B$ is valid if and only if $r_a = l_b$ and
the result is of shape $\shape{A \mmult{1} B} = \left<l_a, r_b\right>$.
And of course, the value at each index
$\left<l_c, r_c\right> \in \Dim(\shape{A \mmult{1} B})$ is,
\[
  (A \mmult{1} B)[l_c, r_c]
  =
  \sum_{\forall 1 \le x \le r_a} 
  A[l_c, x] B[x, r_c]
\]
All of which would be the same if this was regular matrix multiplication.

\section{Rules of Multimatrix Multiplication}

Like two dimensional matrix multiplication, multimatrix multiplication is
not (usually) communicative. And it is only weakly associative. That sounds like
nonsense, and indeed it is, but I shall prove it to you.

Multimatrix multiplication may not even make sense dimensionally when you move
around parentheses. Take for example multimatricies $A$, $B$, and $C$ such that,
$\shape{A} = \left<5,3,2\right>$,
$\shape{B} = \left<2,7\right>$, and
$\shape{C} = \left<3,7,2\right>$ where you should be able to see that
$(A \mmult{1} B) \mmult{2} C$ is a valid operation since
$\shape{A \mmult{1} B} = \left<5,3,7\right>$ and
$\shape{(A \mmult{1} B) \mmult{2} C} = \left<5,2\right>$. But
$A \mmult{1} (B \mmult{2} C)$ isn't a valid set of operations since even
$B \mmult{2} C$ is an invalid operation.

But even when the dimensional analysis makes sense, multimatrix multiplication
may not be associative (Honestly, how could you be so naive?). 
Take, for example, multimatricies $A$, $B$, and $C$ such
that $\shape{A} = \left<2,2,2,2\right>$, $\shape{B} = \left<2,2,2\right>$, and
$\shape{C} = \left<2,2,2,2\right>$. Let,
\[
 A[\bar{a}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{a} = \left<1,1,1,2\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
\[
 B[\bar{b}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{b} = \left<1,2,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
And
\[
 C[\bar{c}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{c} = \left<1,1,1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
Then,
\[
 (A \mmult{2} B)[\bar{x}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{x} = \left<1,1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
So,
\[
 ((A \mmult{2} B) \mmult{2} C)[\bar{x}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{x} = \left<1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
But, $B \mmult{2} C = 0^{\left<2,2,2\right>}$ so $A \mmult{2} (B \mmult{2} C) = 0^{\left<2,2,2\right>}$
Which goes to show that even though both $(A \mmult{2} B) \mmult{2} C$ and
$A \mmult{2} (B \mmult{2} C)$ are valid calculations which result in multimatrixes
of the same dimensionality, they are not equal so multiplicative associativity
doesn't hold. It also goes to show that your intuition is bad and you should feel bad.

However, there are a broad catagory of cases where associativity \textit{does} hold:

\begin{theorem}[Associativity of Multimatrix Multiplication]
\label{mm_associativity}
Let $A$, $B$, and $C$ be multimatricies and $m$ and $n$ in
$\{0\} \cup \mathbb{N}$.
If both operations $(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$
are valid dimensionally and $m+n \le \shape{\shape{B}}$, then,
\[ (A \mmult{m} B) \mmult{n} C = A \mmult{m} (B \mmult{n} C) \]
\end{theorem}
\begin{proof}
Let $A$, $B$, and $C$ be matricies such that the calculations
$(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$ are both valid
and $m + n \le \shape{\shape{B}}$.
Then the following must be true by the definiton of multimatrix multiplication
(Def. \ref{mm_mult}).

Since $A \mmult{m} B$ is defined, there exist vectors $\vec{a}, \vec{m},$ and
$\vec{b_1}$ such that 
$\shape{\vec{m}} = m$,
$\shape{A} = \vec{a} \oplus \vec{m}$,
and
$\shape{B} = \vec{m} \oplus \vec{b_1}$

Since $B \mmult{n} C$ is defined, there exist vectors $\vec{b_2}, \vec{n},$ and
$\vec{c}$ such that
$\shape{\vec{n}} = n$,
$\shape{B} = \vec{b_2} \oplus \vec{n}$
and
$\shape{C} = \vec{n} \oplus \vec{c}$

Since $\shape{B} = \vec{m} \oplus \vec{b_1} = \vec{b_2} \oplus \vec{n}$ and
$\shape{\shape{B}} \ge \shape{\vec{m}} + \shape{\vec{n}}$, it must follow that, there is some vector $\vec{b}$
such that,
\[
 \shape{B} = \vec{m} \oplus \vec{b} \oplus \vec{n}
\]
Even if $\vec{b}$ is an empty vector. Then,
\[ \shape{A \mmult{m} B} = \vec{a} \oplus \vec{b} \oplus \vec{n} \]
\[ \shape{(A \mmult{m} B) \mmult{n} C} = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
\[ \shape{B \mmult{n} C} = \vec{m} \oplus \vec{b} \oplus \vec{c} \]
\[ \shape{A \mmult{m} (B \mmult{n} C)} = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
So,
\[ \shape{(A \mmult{m} B) \mmult{n} C} = \shape{A \mmult{m} (B \mmult{n} C)} \]
Which is necessary, but not sufficient, for our proof.

Looking at each element of the result matricies, let $j$ and $k$ be the values
of some parallel cells in $(A \mmult{m} B) \mmult{n} C$ and
$A \mmult{m} (B \mmult{n} C)$ respectively. Then for some indicies of those cells,
$\bar{a} \oplus \bar{b} \oplus \bar{c} \in
\Dim(\vec{a} \oplus \vec{b} \oplus \vec{c})$,
\begin{align*}
 j
 &= ((A \mmult{m} B) \mmult{n} C)[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
 (A \mmult{m} B)[\bar{a} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n}}
 \left(
  \sum_{\forall \bar{m} \in \Dim(\vec{m})}
  A[\bar{a} \oplus \bar{m}]B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 \right)
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n}, \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{m}, \bar{n}}
 A[\bar{a} \oplus \bar{m}]
 B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 \left(
 \sum_{\forall \bar{n}}
  B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
  C[\bar{n} \oplus \bar{c}]
 \right) \\
 &= \sum_{\forall \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 (B \mmult{n} C)[\bar{m} \oplus \bar{b} \oplus \bar{c}] \\
 &= (A \mmult{m} (B \mmult{n} C))[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
 &= k
\end{align*}
Since $(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$ are both
the same shape and every cell with the same index in each is equal, the two
values must be equal.
\end{proof}

Along the same vein, it's also true that there is a consistent time when
multimatrix multiplication is communicative - when both multimatricies are
the same shape and the multiplication is combining them completely into a
scalar by combining their full width.

\begin{theorem}[Communicativity of Multiplication]
Let $\shape{A} = \vec{x}$ and $\shape{B} = \vec{x}$. Then,
\[ A \mmult{\shape{\vec{x}}} B = B \mmult{\shape{\vec{x}}} A \]
\end{theorem}
\begin{proof}
Let $A$, $B$, and $\vec{x}$ be as described above. Notice that then $\shape{A \mmult{\shape{\vec{x}}} B}$
results in a zero dimensional multimatrix - a multimatrix with one element which is just a scalar.
\begin{align*}
(A \mmult{\shape{\vec{x}}} B)[\left<\right>]
  &= \sum_{\forall \bar{x} \in \Dim(\vec{x})} A[\bar{x}]B[\bar{x}] \\
  &= \sum_{\forall \bar{x} \in \Dim(\vec{x})} B[\bar{x}]A[\bar{x}] \\
  &= (B \mmult{\shape{\vec{x}}} A)[\left<\right>]
\end{align*}
\end{proof}

\section{Multiplicative Identities}

Multiplicative identities for multimatricies depend on the shape of the
multimatrix they are being multiplied against, just as for regular matricies.
However, the multiplicative identity also greatly depends on the size of
the particular multiplication operation. Making a confusing and frightening
situation even more so, like loosing a troup of clowns in children's cancer ward.

\begin{definition}[Multimatrix Identities]
\label{mm_mult_ident}
Let us define $\Ident^n(\vec{v})$ as the $n^{\text{th}}$ order identity with base shape
$\vec{v}$. This $\Ident^n(\vec{v})$ is a multimatrix with shape,
\[ \shape{\Ident^n(\vec{v})} = \bigoplus_{i = 1}^n \vec{v}  = n * \vec{v} \]
That is, it's shape is $n$ successive concatinations of $\vec{v}$.
Let each element of $\Ident^n(\vec{v})$ is defined as,
\[ \forall \bar{v_1}, \bar{v_2}, \ldots, \bar{v_n} \text{ each in } \Dim(\vec{v}) \]
\[ \bigoplus_{i = 1}^n \bar{v_i} = \bar{v_I} \]
\[
 \Ident^n(\vec{v})[\bar{v_I}]
 = \left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{v_1} = \bar{v_2} = \ldots = \bar{v_n} \\
    0 & \mbox{otherwise}
  \end{array}
 \right.
\]

In particular, let us call $\Ident^2(\vec{v})$ the square identity of $\vec{v}$ and
$\Ident^3(\vec{v})$ the cubic identity of $\vec{v}$.
\end{definition}

That definition seems rather arbitrary but stick with me, I know what I'm
talking about. What credentials do I have? Totally irrelevant. Stop asking questions.

Now, please direct your attention to this example of how the square identity relates
to a multiplication operation.

\begin{theorem}[Multimatrix Multiplicative Identity]
\label{mm_ident}
For all multimaticies, $A$, and all vectors, $\vec{m}$ and $\vec{n}$,  on which
these operations are valid,
\[
 A \mmult{\shape{\vec{m}}} \Ident^2(\vec{m}) = A
\]
and
\[
 \Ident^2(\vec{n}) \mmult{\shape{\vec{n}}} A = A
\]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $A$ be a matrix with shape $\shape{A} = \vec{a} \oplus \vec{m}$.
Then by the definition of multimatrix multiplication (Def. \ref{mm_mult}),
\[ \forall \bar{a} \in \Dim(\vec{a}), \bar{m_1} \in \Dim(\vec{m}) : \]
\[
 (A \mmult{\shape{\vec{m}}} \Ident^2(\vec{m}))[\bar{a} \oplus \bar{m_1}]
 =
 \sum_{\forall \bar{m_2} \in \Dim(\vec{m})}
 A[\bar{a} \oplus \bar{m_2}] \Ident^2(\vec{m})[\bar{m_2} \oplus \bar{m_1}]
\]
Then, by the definition of $\Ident^2(\vec{m})$ (Def. \ref{mm_ident}) it follows
that,
\begin{align*}
 (A \mmult{\shape{\vec{m}}} \Ident^2(\vec{m}))[\bar{a} \oplus \bar{m_1}]
 &=
 \left(
  \sum_{\forall \bar{m_2} \ne \bar{m_1}}
  A[\bar{a} \oplus \bar{m_2}] (0)
 \right)
 +
 \left(
  \sum_{\forall \bar{m_2} = \bar{m_1}}
  A[\bar{a} \oplus \bar{m_2}] (1)
 \right) \\
 \\
 &= 0 + A[\bar{a} \oplus \bar{m_1}] \\
 &= A[\bar{a} \oplus \bar{m_1}]
\end{align*}
Therefore $A \mmult{\shape{\vec{m}}} \Ident^2(\vec{m}) = A$
\end{ppart}
\begin{ppart}
Let $A$ be a matrix with shape $\shape{A} = \vec{n} \oplus \vec{a}$.
Then by the definition of multimatrix multiplication (Def. \ref{mm_mult}),
\[ \forall  \bar{n_1} \in \Dim(\vec{n}), \bar{a} \in \Dim(\vec{a}) : \]
\[
 (\Ident^2(\vec{n}) \mmult{\shape{\vec{n}}} A)[\bar{n_1} \oplus \bar{a}]
 =
 \sum_{\forall \bar{n_2} \in \Dim(\vec{n})}
 \Ident^2(\vec{n})[\bar{n_1} \oplus \bar{n_2}]A[\bar{n_2} \oplus \bar{a}] 
\]
Then, by the definition of $\Ident^2(\vec{n})$ (Def. \ref{mm_ident}) it follows
that,
\begin{align*}
 (\Ident^2(\vec{n}) \mmult{\shape{\vec{n}}} A)[\bar{n_1} \oplus \bar{a}]
 &=
 \left(
  \sum_{\forall \bar{n_2} \ne \bar{n_1}}
  (0) A[\bar{n_2} \oplus \bar{a}]
 \right)
 +
 \left(
  \sum_{\forall \bar{n_2} = \bar{n_1}}
  (1) A[\bar{n_2} \oplus \bar{a}]
 \right) \\
 \\
 &= 0 + A[\bar{n_1} \oplus \bar{a}] \\
 &= A[\bar{n_1} \oplus \bar{a}]
\end{align*}
Therefore $\Ident^2(\vec{n}) \mmult{\shape{\vec{n}}} A = A$
\end{ppart}
\end{proof}

There is an obvious corollary to that which you might consider an
alternate definition of $\Ident^2$.

\begin{corollary}[Multiplicative Identity Implication]
\label{mm_ident_imp}
If there is a matrix $X$ of shape $\vec{x} \oplus \vec{x}$ such that
for all matricies, $A$, for which the operation is defined,

\[A \mmult{\shape{\vec{x}}} X = A \]

Then $X = \Ident^2(\vec{x})$.

Or if for all matricies, $B$, for which the operation is defined,

\[X \mmult{\shape{\vec{x}}} B = B \]

Then, again, $X = \Ident^2(\vec{x})$.
\end{corollary}
\begin{proof}
Let $X$ be a matrix of shape $\vec{x} \oplus \vec{x}$. 
\begin{ppart}
If for all multimatrixes, $A$, on which $A \mmult{\shape{\vec{x}}} X$ is defined, 
$A \mmult{\shape{\vec{x}}} X = A$ this implies that, in the case of $A = \Ident^2(\vec{x})$,

\begin{align*}
  A \mmult{\shape{\vec{x}}} X &= A \\
 \Ident^2(\vec{x}) \mmult{\shape{\vec{x}}} X &= \Ident^2(\vec{x}) \\
  X &= \Ident^2(\vec{x}) 
\end{align*}
\end{ppart}
\begin{ppart}
If for all multimatrixes, $B$, on which $X \mmult{\shape{\vec{x}}} B$ is defined, 
$X \mmult{\shape{\vec{x}}} B = B$ this implies that, in the case of $B = \Ident^2(\vec{x})$,

\begin{align*}
  X \mmult{\shape{\vec{x}}} B &= B \\
  X \mmult{\shape{\vec{x}}} \Ident^2(\vec{x}) &= \Ident^2(\vec{x}) \\
  X &= \Ident^2(\vec{x}) 
\end{align*}
\end{ppart}
\end{proof}

Multimatrix identities also have the nice property of being communicative when multiplied
across multiples of their base shape and completely engulfing a smaller multimatrix.

\begin{theorem}[Communicativity of Multiplicative Identities]
Let $X$ be a multimatrix of shape $k * \vec{x}$ where $k$ is some natural number.
Remember (from the identity definition) that the $*$ operator denote successive concatenations of a vector so that,
\[ k * \vec{x} = \bigoplus_{i=1}^k \vec{x} \]
Then for any $n \ge k$,
\[ X \mmult{k \shape{\vec{x}}} \Ident^n(\vec{x}) = \Ident^n(\vec{x}) \mmult{k \shape{\vec{x}}} X \]
\end{theorem}
\begin{proof}
Given that $\shape{X} = k * \vec{x}$ we know that
$\shape{X \mmult{k \shape{\vec{x}}} \Ident^n(\vec{x})} = (n-k) * \vec{x}$.
Let $\bar{x}_1, \bar{x}_2 \ldots \bar{x}_{n-k}$ each be index vectors in $\Dim(\vec{x})$,
for simplicity denoted as,
\[ \bar{x}_I = \bigoplus_{i=1}^{n-k} \bar{x}_i \]
We'll also need a similarly denoted $\bar{y}_1 \ldots \bar{y}_k$ for the free indicies of
the sum below and that shall also be represented as,
\[ \bar{y}_J = \bigoplus_{j=1}^{k} \bar{y}_j \]
Then we know that,
\[
  \left(X \mmult{k \shape{\vec{x}}} \Ident^n(\vec{x}) \right)[\bar{x}_I]
  = \sum_{\forall \bar{y}_J} X[\bar{y}_J] \Ident^n(\vec{x})[\bar{y}_J \oplus \bar{x}_I]
\]
Notice that $\Ident^n(\vec{x})[\bar{y}_J \oplus \bar{x}_I] = \Ident^n(\vec{x})[\bar{x}_I \oplus \bar{y}_J]$
since the sub index vectors will always either be equal or not equal, $1$ or $0$, their order doesnt matter.
So,
\begin{align*}
  \left(X \mmult{k \shape{\vec{x}}} \Ident^n(\vec{x}) \right)[\bar{x}_I]
  &= \sum_{\forall \bar{y}_J} X[\bar{y}_J] \Ident^n(\vec{x})[\bar{x}_I \oplus \bar{y}_J] \\
  &= \sum_{\forall \bar{y}_J} \Ident^n(\vec{x})[\bar{x}_I \oplus \bar{y}_J] X[\bar{y}_J] \\
  &= \left(\Ident^n(\vec{x}) \mmult{k \shape{\vec{x}}} X \right)[\bar{x}_I]
\end{align*}
\end{proof}

The square identity also pops up in the derivative of a multimatrix
with respect to itself.

\begin{theorem}[Self Derivative]
\label{self_derivative}
Let $X$ be some multimatrix. Then,
\[ \frac{dX}{dX} = \Ident^2(\shape{X}) \]
\end{theorem}
\begin{proof}
Let $X$ be a multimatrix of shape $\shape{X}=\vec{x}$.
By the derivative definition (Def. \ref{mm_derivative}),
\[
 \forall \bar{x_1} \in \Dim(\vec{x}),
         \bar{x_2} \in \Dim(\vec{x}):
\]
\[
 \frac{dX}{dX}[\bar{x_1} \oplus \bar{x_2}] = 
 \frac{\partial X[\bar{x_1}]}{\partial X[\bar{x_2}]}
\]
Since each element of $X$ is independent with respect to itself,
\begin{align*}
 \frac{dX}{dX}[\bar{x_1} \oplus \bar{x_2}]
 &= \left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{x_1} = \bar{x_2} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \Ident^2(\vec{x})[\bar{x_1} \oplus \bar{x_2}]
\end{align*}
Therefore $dX/dX = \Ident^2(\shape{X})$
\end{proof}

The cubic identity, strangely enough, appear in the derivative of elementwise
functions. Which is one of the reasons I needed to expand my definition of
identities beyond $\Ident^2(\vec{v})$.

\begin{theorem}[Elementwise Derivative]
\label{elementwise_derivative}
Let $M$ be an element wise function on $X$. That is, Let $M(X)$ be a function
such that,
\[ M(X) : \mathbb{R}^{\vec{x}} \to \mathbb{R}^{\vec{x}} \]
And which applies the same scalar function to each element of $X$, 
\[ M(X)[\bar{x}] = m(X[\bar{x}]) \]
Also, let $M'$ be the elementwise function on $X$ which applies the derivative
of $M$'s associated scalar function to each element of $X$. That is,
\[ M'(X)[\bar{x}] = m'(X[\bar{x}]) \]
Then,
\[ \frac{dM}{dX} = \Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} M'(X) \]
\end{theorem}
\begin{proof}
By the definition of a multimatrix derivative,
\[ \forall \bar{m}, \bar{x} \in \Dim(\shape{X}) : \]
\begin{align*}
 \frac{dM}{dX}[\bar{m} \oplus \bar{x}]
 &= \frac{\partial M[\bar{m}]}{\partial X[\bar{x}]} \\
 &= \frac{\partial m(X[\bar{m}])}{\partial X[\bar{x}]} \\
 &= \left\{
  \begin{array}{ll}
    m'(X[\bar{m}]) & \mbox{if } \bar{m} = \bar{x} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \delta_{\bar{m} \bar{x}} m'(X[\bar{m}])
\end{align*}
Since, by our definition of elementwise function, $\shape{M} = \shape{X}$, we can
say that, 
\begin{align*}
 (\Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} M'(X))[\bar{m} \oplus \bar{x}]
 &= \sum_{\forall \bar{y} \in \Dim(\shape{X})}
 \Ident^3(\shape{X})[\bar{m} \oplus \bar{x} \oplus \bar{y}] M'(X)[\bar{y}] \\
 &= \sum_{\forall \bar{y} \in \Dim(\shape{X})}
 \Ident^3(\shape{X})[\bar{m} \oplus \bar{x} \oplus \bar{y}] m'(X[\bar{y}]) \\
 &= \sum_{\forall \bar{y} = \bar{m} = \bar{x}}
 \Ident^3(\shape{X})[\bar{m} \oplus \bar{x} \oplus \bar{y}] m'(X[\bar{y}]) \\
 &= \left\{
  \begin{array}{ll}
    m'(X[\bar{m}]) & \mbox{if } \bar{m} = \bar{x} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \delta_{\bar{m} \bar{x}} m'(X[\bar{m}])
\end{align*}
Therefore $dM/dX = \Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} M'(X)$
\end{proof}

Another important fact about the identity multimatrix is that it can be
reduced by multiplication with a $1^{\vec{s}}$ matrix.

\begin{theorem}[Identity Reduction]
Let $\vec{x}$ be a valid multimatrix shape and $m$ and $n$ be natural numbers
such that $m < n$.
Remember (from the identity definition) that the $*$ operator denote successive
concatenations of a vector so that,
\[ m*\vec{x} = \bigoplus_{i=1}^{m} \vec{x} \]
Then,
\[1^{m*\vec{x}} \mmult{m \shape{\vec{x}}} \Ident^n(\vec{x}) = \Ident^{n-m}(\vec{x})\]
and,
\[\Ident^n(\vec{x}) \mmult{m \shape{\vec{x}}} 1^{m*\vec{x}} = \Ident^{n-m}(\vec{x})\]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $\bar{x}_1, \bar{x}_2 \ldots  \bar{x}_{n-m}$ each in $\Dim(\vec{x})$ and let
\[\bigoplus_{i = 1}^{n-m} \bar{x}_i = \bar{x}_I \]
In the sum below, let $\bar{y}_1, \bar{y}_2 \ldots \bar{y}_m$ each in $\Dim(\vec{x})$ be the
free indicies denoted,
\[\bigoplus_{j = 1}^m \bar{y}_j = \bar{y}_J \]
Then,
\begin{align*}
  1^{m*\vec{x}} \mmult{m \shape{\vec{x}}} \Ident^n(\vec{x}) [\bar{x}_I] &=
  \sum_{\forall \bar{y}_J} 1^{m*\vec{x}}[\bar{y}_J] \Ident^n(\vec{x})[\bar{y}_J \oplus \bar{x}_I] \\
  &=
  \sum_{\forall \bar{y}_J} (1) \Ident^n(\vec{x})[\bar{y}_J \oplus \bar{x}_I] \\
  &=
  \sum_{\forall \bar{y}_J} \Ident^n(\vec{x})[\bar{y}_J \oplus \bar{x}_I] \\
  &=
  \left(
    \sum_{\forall \bar{y}_J, \text{ each } \bar{y}_i \ne \bar{x}_1} \Ident^n(\vec{x})[\bar{y}_J \oplus \bar{x}_I]
  \right)
  + \Ident^n(\vec{x})\left[\left( \bigoplus_{j=1}^m \bar{x}_1 \right) \oplus \bar{x}_I\right] \\
  &= 0 + \Ident^n(\vec{x})\left[\left( \bigoplus_{j=1}^m \bar{x}_1 \right) \oplus \bar{x}_I\right] \\
  &= \Ident^{n-m}(\vec{x})[\bar{x}_I]
\end{align*}
\end{ppart}
\begin{ppart}
Let $\bar{x}_1, \bar{x}_2 \ldots \bar{x}_{n-m}$ each in $\Dim(\vec{x})$ and let
\[\bigoplus_{i = 1}^{n-m} \bar{x}_i = \bar{x}_I \]
In the sum below, let $\bar{y}_1, \bar{y}_2 \ldots \bar{y}_m$ each in $\Dim(\vec{x})$ be the
free indicies denoted,
\[\bigoplus_{j = 1}^m \bar{y}_j = \bar{y}_J \]
Then,
\begin{align*}
  \Ident^n(\vec{x}) \mmult{m \shape{\vec{x}}} 1^{m*\vec{x}} [\bar{x}_I] &=
  \sum_{\forall \bar{y}_J} \Ident^n(\vec{x})[\bar{x}_I \oplus \bar{y}_J] 1^{m*\vec{x}}[\bar{y}_J] \\
  &=
  \sum_{\forall \bar{y}_J} \Ident^n(\vec{x})[\bar{x}_I \oplus \bar{y}_J] (1) \\
  &=
  \sum_{\forall \bar{y}_J} \Ident^n(\vec{x})[\bar{x}_I \oplus \bar{y}_J] \\
  &=
  \left(
    \sum_{\forall \bar{y}_J, \text{ each } \bar{y}_i \ne \bar{x}_1} \Ident^n(\vec{x})[\bar{x}_I \oplus \bar{y}_J]
  \right)
  + \Ident^n(\vec{x})\left[\bar{x}_I \oplus \bigoplus_{j=1}^m \bar{x}_1 \right] \\
  &= 0 + \Ident^n(\vec{x})\left[\bar{x}_I \oplus \bigoplus_{j=1}^m \bar{x}_1 \right] \\
  &= \Ident^{n-m}(\vec{x})[\bar{x}_I]
\end{align*}
\end{ppart}
\end{proof}

Multimatrix identities also tend to cancel eachother out when multiplied by
multiples of their base shape lengths.

\begin{landscape}
\begin{theorem}[Identity Contraction]
For all $m, n, k \in \mathbb{N}$ (note $m$, $n$, and $k$ are non-zero) and $\vec{x}$ as a valid multimatrix shape,
where $m > k$ and $n > k$,
\[ \Ident^m(\vec{x}) \mmult{k\shape{\vec{x}}} \Ident^n(\vec{x}) = \Ident^{m+n-2k}(\vec{x}) \]
\end{theorem}
\begin{proof}
Consider that for some indicies vectors
$\bar{a}_1, \bar{a}_2 \ldots \bar{a}_{m-k}$
and $\bar{b}_1, \bar{b}_2 \ldots \bar{b}_{n-k}$ where each
$\bar{a_i} \in \Dim(\vec{x})$ and each $\bar{b_i} \in \Dim(\vec{x})$.
Now let,
\[ \bigoplus_{i=1}^{m-k} \bar{a}_i = \bar{a}_I \]
and,
\[ \bigoplus_{j=1}^{n-k} \bar{b}_j = \bar{b}_J \]
In the sums below we'll let $\bar{c}_1, \bar{c}_2 \ldots \bar{c}_k$ be free indicies
each in $\Dim(\vec{x})$ and denote,
\[ \bigoplus_{l = 1}^k \bar{c}_l = \bar{c}_K \]

Then,
\[
\left( \Ident^m(\vec{x}) \mmult{k\shape{\vec{x}}} \Ident^n(\vec{x}) \right)
[\bar{a}_I \oplus \bar{b}_J] = 
\sum_{\forall \bar{c}_K} \Ident^m(\vec{x}) [\bar{a}_I \oplus \bar{c}_K] \Ident^n(\vec{x}) [\bar{c}_K \oplus \bar{b}_J]
\]

Consider, for any $i$ if $\bar{a}_i \ne \bar{a}_1$ then
$\Ident^m(\vec{x}) [\bar{a}_I \oplus \bar{c}_K]$ will always be zero.
Similarly, if for any $j$, $\bar{b}_j \ne \bar{b}_1$ 
$\Ident^n(\vec{x}) [\bar{c}_K \oplus \bar{b}_J]$ will always be zero.

So, if we let, $\hat{a}_1 = (m-k) * \bar{a}_1$ and $\hat{b}_1 = (n-k) * \bar{b}_1$ then,
\begin{align*}
\left( \Ident^m(\vec{x}) \mmult{k\shape{\vec{x}}} \Ident^n(\vec{x}) \right) &
[\bar{a}_I \oplus \bar{b}_J] = \\
&\left\{
  \begin{array}{ll}
    \sum_{\forall \bar{c}_K}
      \Ident^m(\vec{x}) [\hat{a}_1 \oplus \bar{c}_K] 
      \Ident^n(\vec{x}) [\bar{c}_K \oplus \hat{b}_1]
    & \mbox{if } \forall i, j : \bar{a}_i = \bar{a}_1 \mbox{ and } \bar{b}_j = \bar{b}_1 \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\end{align*}

Now, for any $\bar{c}_i$ that is not equal to $\bar{a}_1$, 
$\Ident^m(\vec{x}) [\hat{a}_1 \oplus \bar{c}_K]$ will be zero.
So, we only need to consider the part of the sum where $\bar{c}_K = k * \bar{a}_1$
\begin{align*}
\left( \Ident^m(\vec{x}) \mmult{k\shape{\vec{x}}} \Ident^n(\vec{x}) \right) &
[\bar{a}_I \oplus \bar{b}_J] = \\
&\left\{
  \begin{array}{ll}
    \Ident^m(\vec{x}) [\hat{a}_1 \oplus (k * \bar{a}_1)] 
    \Ident^n(\vec{x}) [(k * \bar{a}_1) \oplus \hat{b}_1]
    & \mbox{if } \forall i, j : \bar{a}_i = \bar{a}_1 \mbox{ and } \bar{b}_j = \bar{b}_1 \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\end{align*}

But $\Ident^m(\vec{x}) [\hat{a}_1 \oplus (k * \bar{a}_1)]$ is just $1$, so,
\[
\left( \Ident^m(\vec{x}) \mmult{k\shape{\vec{x}}} \Ident^n(\vec{x}) \right)
[\bar{a}_I \oplus \bar{b}_J] =
\left\{
  \begin{array}{ll}
    \Ident^n(\vec{x}) [(k * \bar{a}_1) \oplus \hat{b}_1]
    & \mbox{if } \forall i, j : \bar{a}_i = \bar{a}_1 \mbox{ and } \bar{b}_j = \bar{b}_1 \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\]

And $\Ident^n(\vec{x}) [(k * \bar{a}_1) \oplus \hat{b}_1]$ is only $1$ when $\bar{a}_1 = \bar{b}_1$,
\[
\left( \Ident^m(\vec{x}) \mmult{k\shape{\vec{x}}} \Ident^n(\vec{x}) \right)
[\bar{a}_I \oplus \bar{b}_J] =
\left\{
  \begin{array}{ll}
    1 & \mbox{if } \forall i, j : \bar{a}_i = \bar{a}_1 \mbox{ and } \bar{b}_j = \bar{a}_1 \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\]

Which is exactly equal the result we were looking for,
\[
  \left( \Ident^m(\vec{x}) \mmult{k\shape{\vec{x}}} \Ident^n(\vec{x}) \right) [\bar{a}_I \oplus \bar{b}_J]
  = \Ident^{m+n-2k}(\vec{x}) [\bar{a}_I \oplus \bar{b}_J]
\]
\end{proof}
\end{landscape}


A final useful theorem (definition?) happens to involve 0 overlap multiplication.
You'll notice that I very carefully constructed my definition to allow for this
definition (theorem?). Does it still count as a theorem if it only works because
I tweeked the definition of multimatrix multiplication to make it fit? I feel
a bit like Tommy Wiseau writing my own sex scene into the script... twice... God,
what a tool.

\begin{theorem}[Zero Overlap Multiplication]
Anyways, if $\shape{A} = \vec{a}$ and $\shape{B} = \vec{b}$, for every $\bar{a} \in \Dim(\vec{a})$
and $\bar{b} \in \Dim(\vec{b}):$

\[ (A \mmult{0} B)[\bar{a} \oplus \bar{b}] = A[\bar{a}] B[\bar{b}] \]

\end{theorem}
\begin{proof}
\begin{align*}
	(A \mmult{0} B)[\bar{a} \oplus \bar{b}]
	&=
	\sum_{\forall \bar{v} \in \Dim(\left<\right>)}
	A[\bar{a} \oplus \bar{v}] B[\bar{v} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{v} \in \left\{\left<\right>\right\}}
	A[\bar{a} \oplus \bar{v}] B[\bar{v} \oplus \bar{b}] \\
	&=
	A[\bar{a} \oplus \left<\right>] B[\left<\right> \oplus \bar{b}] \\
	&=
	A[\bar{a}] B[\bar{b}] \\
\end{align*}
\end{proof}

\section{Multiplicative Inverses}

Given a multimatrix, $X$, you might ask, ``What is the multiplicative inverse of $X$?''
And I would have to tell you that your question is not well formed. Do you mean left multiplication
or right multiplication? Multiplication by what width? It's not clear which of these specifications
is even required: consider that multiplication is not communicative for regular 2d matricies but
those have simple multiplicative inverses (if an inverse exists). 

So let's clarify what we can.

\begin{definition}[Multiplicative Functions]
Let us declare the function $\RMult[m,X](M)$ to be the function defined 
by the tuple,
\[ \left<m,X\right> \in \{  (\{0\} \cup \mathbb{N}) \times \mathbb{R}^{\vec{m} \oplus \vec{x}}  :  \shape{\vec{m}} = m \} \]
such that,
\[ \RMult[m,X](M) : \mathbb{R}^{\vec{l} \oplus \vec{m}} \to \mathbb{R}^{\vec{l} \oplus \vec{x}} \]
\[ \RMult[m,X](M) = M \mmult{m} X \]

Let us also declare the function $\LMult[m,X](M)$ to be the function defined 
by the tuple,
\[ \left<m,X\right> \in \{  (\{0\} \cup \mathbb{N}) \times \mathbb{R}^{\vec{x} \oplus \vec{m}}  :  \shape{\vec{m}} = m \} \]
such that,
\[ \LMult[m,X](M) : \mathbb{R}^{\vec{m} \oplus \vec{r}} \to \mathbb{R}^{\vec{x} \oplus \vec{r}} \]
\[ \LMult[m,X](M) = X \mmult{m} M \]

The the set of all these functions be $\RMult$ and $\LMult$ respectively.
\end{definition}

That will let us formally define a multiplicative inverse:

\begin{definition}[Multiplicative Inverse]
For any $\RMult[m,X]$ and $\RMult[n,Y]$ if, for all valid inputs of $\RMult[m,X]$, $M$,
\[ \RMult[n,Y] \circ \RMult[m,X] (M) = M \]
Then $\RMult[n,Y]$ is an inverse of $\RMult[m,X]$. In other words,
\[ \RMult[n,Y] \in \RMult[m,X]^{-1} \]

Similarly, for any $\LMult[m,X]$ and $\LMult[n,Y]$ if, for all valid inputs of $\LMult[m,X]$, $M$,
\[ \LMult[n,Y] \circ \LMult[m,X] (M) = M \]
Then $\LMult[n,Y]$ is an inverse of $\LMult[m,X]$. In other words,
\[ \LMult[n,Y] \in \LMult[m,X]^{-1} \]
\end{definition}

Note that those inverses are defined as sets of operations, not individual operations. 
You'll see why in a bit but to clear up any early confusion you should note that multimatrix
multiplicative inverses aren't unique. For example, $\RMult[2,\Ident^2(\left<2,2\right>)]$ has
two multiplicative inverse I can think of off the top of my head;
$\RMult[2,\Ident^2(\left<2,2\right)]$ or
$\RMult[1,\Ident^2(\left<2\right>)]$.

\begin{theorem}[Relation of Inverse to Identity]
If $\RMult[n,Y]$ is an inverse of $\RMult[m,X]$ where $\shape{X} = \vec{m} \oplus \vec{x}$ and $\shape{\vec{m}} = m$
then $X \mmult{n} Y = \Ident^2(\vec{m})$.
And if $\LMult[n,Y]$ is an inverse of $\LMult[m,X]$ where $\shape{X} = \vec{x} \oplus \vec{m}$ and $\shape{\vec{m}} = m$
then $Y \mmult{n} X = \Ident^2(\vec{m})$.
\end{theorem}
\begin{proof}
\begin{ppart}
Let $X$ be a multimatrix such that $\shape{X} = \vec{m} \oplus \vec{x}$ and $\shape{\vec{m}} = m$.
If $\RMult[n,Y]$ is the inverse of $\RMult[m,X]$ then, for all $M$,
\begin{align*}
  \RMult[n,Y] \circ \RMult[m,X] (M) &= M \\
  (M \mmult{m} X) \mmult{n} Y &= M
\end{align*}

This is also true in the case where $M = \Ident^2(\vec{m})$. In which case,
\begin{align*}
  (\Ident^2(\vec{m}) \mmult{m} X) \mmult{n} Y &= \Ident^2(\vec{m}) \\
  X \mmult{n} Y &= \Ident^2(\vec{m}) \\
\end{align*}
\end{ppart}
\begin{ppart}
Let $X$ be a multimatrix such that $\shape{X} = \vec{x} \oplus \vec{m}$ and $\shape{\vec{m}} = m$.
If $\LMult[n,Y]$ is the inverse of $\LMult[m,X]$ then, for all $M$,
\begin{align*}
  \LMult[n,Y] \circ \LMult[m,X] (M) &= M \\
  Y \mmult{n} (X \mmult{m} M) &= M
\end{align*}

This is also true in the case where $M = \Ident^2(\vec{m})$. In which case,
\begin{align*}
  Y \mmult{n} (X \mmult{m} \Ident^2(\vec{m})) &= \Ident^2(\vec{m}) \\
  Y \mmult{n} X &= \Ident^2(\vec{m})
\end{align*}
\end{ppart}
\end{proof}

\begin{theorem}[Shape of Inverses]
If $\RMult[n,Y]$ is an inverse of $\RMult[m,X]$ where $\shape{X} = \vec{m} \oplus \vec{x}$ and
$\shape{\vec{m}} = m$ then $n \le \shape{\shape{X}}-m$.

Also, if $\LMult[n,Y]$ is an inverse of $\LMult[m,X]$ where $\shape{X} = \vec{x} \oplus \vec{m}$ and
$\shape{\vec{m}} = m$ then $n \le \shape{\shape{X}}-m$.
\end{theorem}
\begin{proof}
\begin{ppart}
Let $\RMult[n,Y]$ be an inverse of $\RMult[m,X]$ where $\shape{X} = \vec{m} \oplus \vec{x}$ and
$\shape{\vec{m}} = m$.
Let $M$ be some valid input of $\RMult[m,X]$.
In other words, $\shape{M} = \vec{l} \oplus \vec{m}$ for some $\vec{l}$.
So $\shape{M \mmult{m} X} = \vec{l} \oplus \vec{x}$.
Since $\RMult[n,Y]$ is a valid operation on $M \mmult{m} X$, $n \le \shape{\vec{l} \oplus \vec{x}}$.
And since there is no restriction on $\vec{l}$ it's only useful to say that $n \le \shape{\vec{x}}$.
Given that $\shape{\shape{X}} = \shape{\vec{x}} + m$ it follows that $n \le \shape{\shape{X}} - m$.
\end{ppart}
\begin{ppart}
Let $\LMult[n,Y]$ be an inverse of $\LMult[m,X]$ where $\shape{X} = \vec{x} \oplus \vec{m}$ and
$\shape{\vec{m}} = m$.
Let $M$ be some valid input of $\LMult[m,X]$.
In other words, $\shape{M} = \vec{m} \oplus \vec{r}$ for some $\vec{r}$.
So $\shape{X \mmult{m} M} = \vec{x} \oplus \vec{r}$.
Since $\RMult[n,Y]$ is a valid operation on $X \mmult{m} M$, $n \le \shape{\vec{x} \oplus \vec{r}}$.
And since there is no restriction on $\vec{r}$ it's only useful to say that $n \le \shape{\vec{x}}$.
Given that $\shape{\shape{X}} = \shape{\vec{x}} + m$ it follows that $n \le \shape{\shape{X}} - m$.
\end{ppart}
\end{proof}


\begin{theorem}[Relation of Identity to Inverse]
If $X \mmult{n} Y = \Ident^2(\vec{m})$, $\shape{\vec{m}} = m$, $\shape{X} = \vec{m} \oplus \vec{x}$,
and $n \le \shape{\shape{X}}-m$, that implies that $\RMult[n, Y]$ is an inverse of $\RMult[m, X]$.

Also,  if $Y \mmult{n} X = \Ident^2(\vec{m})$, $\shape{\vec{m}} = m$, $\shape{X} = \vec{x} \oplus \vec{m}$,
and $n \le \shape{\shape{X}}-m$, that implies that $\LMult[n, Y]$ is an inverse of $\LMult[m, X]$.
\end{theorem}
\begin{proof}
\begin{ppart}
Let $X \mmult{n} Y = \Ident^2(\vec{m})$, $\shape{\vec{m}} = m$, $\shape{X} = \vec{m} \oplus \vec{x}$, and $n \le \shape{\shape{X}}-m$.
We know that $n \le \shape{\vec{x}}$ since $\shape{\vec{x}} = \shape{\shape{X}}-m$.
Then we can split up $\vec{x}$ into $\vec{x_l} \oplus \vec{x_r}$ where $\shape{\vec{x_r}} = n$.
So $\shape{Y} = \vec{x_r} \oplus \vec{y}$ for some $\vec{y}$ since $X \mmult{n} Y$ is defined.
We can then find the shape $\shape{X \mmult{n} Y} = \vec{m} \oplus \vec{x_l} \oplus \vec{y}$.
Then for any $M$ such that $M \mmult{m} X$ is defined, i.e. any $M$ of shape $\vec{l} \oplus \vec{m}$ for some $\vec{l}$,
we can tell that $M \mmult{m} (X \mmult{n} Y)$ is also defined.
And since $n \le \shape{\shape{X}}-m$ that implies $n+m \le \shape{\shape{X}}$ so $M \mmult{m} (X \mmult{n} Y) = (M \mmult{m} X) \mmult{n} Y$
by Theorem \ref{mm_associativity}.
And if we just replace $X \mmult{n} Y$ with it's given value, then for all $M$ on which the $\RMult[m,X]$ is defined,
\begin{align*}
  (M \mmult{m} X) \mmult{n} Y &= M \mmult{m} (X \mmult{n} Y)  \\
  &= M \mmult{m} \Ident^2(\vec{m}) \\
  &= M \\
  \RMult[n,Y] \circ \RMult[m,X](M) &= M
\end{align*}
Therefore $\RMult[n,Y]$ is an inverse of $\RMult[m,X]$.
\end{ppart}
\begin{ppart}
Let $Y \mmult{n} X = \Ident^2(\vec{m})$, $\shape{\vec{m}} = m$, $\shape{X} = \vec{x} \oplus \vec{m}$, and $n \le \shape{\shape{X}}-m$.
We know that $n \le \shape{\vec{x}}$ since $\shape{\vec{x}} = \shape{\shape{X}}-m$.
Then we can split up $\vec{x}$ into $\vec{x_l} \oplus \vec{x_r}$ where $\shape{\vec{x_l}} = n$.
So $\shape{Y} = \vec{y} \oplus \vec{x_l}$ for some $\vec{y}$ since $Y \mmult{n} X$ is defined.
We can then find the shape $\shape{Y \mmult{n} X} = \vec{y} \oplus \vec{x_r} \oplus \vec{m}$.
Then for any $M$ such that $X \mmult{m} M$ is defined, i.e. any $M$ of shape $\vec{m} \oplus \vec{r}$ for some $\vec{r}$,
we can tell that $(Y \mmult{n} X) \mmult{m} M$ is also defined.
And since $n \le \shape{\shape{X}}-m$ that implies $n+m \le \shape{\shape{X}}$ so $(Y \mmult{n} X) \mmult{m} M = Y \mmult{n} (X \mmult{m} M)$
by Theorem \ref{mm_associativity}.
And if we just replace $Y \mmult{n} X$ with it's given value, then for all $M$ on which the $\LMult[m,X]$ is defined,
\begin{align*}
  Y \mmult{n} (X \mmult{m} M) &= (Y \mmult{n} X) \mmult{m} M  \\
  &= \Ident^2(\vec{m}) \mmult{m} M\\
  &= M \\
  \LMult[n,Y] \circ \LMult[m,X](M) &= M
\end{align*}
Therefore $\LMult[n,Y]$ is an inverse of $\LMult[m,X]$.
\end{ppart}
\end{proof}
 
This is all well and good, but I'm sure what you really want to know is how to find a multiplicative inverse.
It seams increadibly complicated until you realize that there is a way to relate any multimatrix multiplication
to a 2d matrix multiplication. We'll first need to define a special function:

\begin{definition}[Indexing Function]
Let $\vec{x}$ be some shape vector and $\bar{x} \in \Dim(\vec{x})$. Then the indexing function is one which transforms
that $\bar{x}$ into a unique integer (i.e. it is one-to-one):
\[
  \Idx[\vec{x}](\bar{x}) = \sum_{1 \le i \le \shape{\vec{x}}} \left( (\bar{x}[i]-1) \prod_{1 \le j < i} \vec{x}[j] \right)
\]
And then inverse function takes an integer and returns a vector in $\Dim(\vec{x})$:
\[ \shape{\Idx[\vec{x}]^{-1}(n)} = \shape{\vec{x}} \]
\[
  \Idx[\vec{x}]^{-1}(n)[i] =
    \left\lfloor
      \frac{\remainder\left(n, \prod_{1 \le j \le i} \vec{x}[j]\right)}
      {\prod_{1 \le k < i} \vec{x}[k]}
    \right\rfloor + 1
\]
Where $\Idx[\vec{x}]^{-1}(n)[i]$ is the $i$th value of the vector output of $\Idx[\vec{x}]^{-1}(n)$.
Also, $\remainder$ is the remainder function.
\end{definition}

\begin{landscape}
Let's prove those properties of the indexing function before moving on. If we prove that the inverse function
really is an inverse, then that implies the function is one-to-one. So let's just do that:

\begin{proof}
Let $\bar{x} \in \Dim(\vec{x})$ and let $k$ be any index of $\bar{x}$.

\begin{align*}
  \Idx[\vec{x}]^{-1}(\Idx[\vec{x}](\bar{x}))[k] &= 
  \left\lfloor
    \frac{\remainder\left(
      \sum_{1 \le i \le \shape{\vec{x}}} \left( (\bar{x}[i]-1) \prod_{1 \le j < i} \vec{x}[j] \right) 
      , \prod_{1 \le m \le k} \vec{x}[m]\right)}
    {\prod_{1 \le n < k} \vec{x}[n]}
  \right\rfloor + 1 \\
  &=
  \left\lfloor
    \frac{\remainder\left(
      \sum_{1 \le i \le k} \left( (\bar{x}[i]-1) \prod_{1 \le j < i} \vec{x}[j] \right) 
      , \prod_{1 \le m \le k} \vec{x}[m]\right)}
    {\prod_{1 \le n < k} \vec{x}[n]}
  \right\rfloor + 1 \\
  &=
  \left\lfloor
    \frac{
      \sum_{1 \le i \le k} \left( (\bar{x}[i]-1) \prod_{1 \le j < i} \vec{x}[j] \right) 
    }
    {\prod_{1 \le n < k} \vec{x}[n]}
  \right\rfloor + 1 \\
  &=
  \left\lfloor
    \frac{
      \sum_{1 \le i < k} \left( (\bar{x}[i]-1) \prod_{1 \le j < i} \vec{x}[j] \right) 
    }
    {\prod_{1 \le n < k} \vec{x}[n]}
    +
    (\bar{x}[k]-1)
  \right\rfloor + 1 \\
  &= \bar{x}[k]-1+1 \\
  &= \bar{x}[k]
\end{align*}

A couple of leaps of logic there rely on the fact that 
\[
  \sum_{1 \le i \le k} \left( (\bar{x}[i]-1) \prod_{1 \le j < i} \vec{x}[j] \right)
  < \prod_{1 \le m \le k} \vec{x}[m]
\]
Which is true because
\begin{align*}
  \sum_{1 \le i \le k} \left( (\bar{x}[i]-1) \prod_{1 \le j < i} \vec{x}[j] \right)
  &< \prod_{1 \le m \le k} \vec{x}[m] \\
  \sum_{1 \le i \le k} \left( \bar{x}[i]\prod_{1 \le j < i} \vec{x}[j] - \prod_{1 \le j < i} \vec{x}[j] \right)
  &< \prod_{1 \le m \le k} \vec{x}[m] \\
  \sum_{1 \le i \le k} \left( \bar{x}[i]\prod_{1 \le j < i} \vec{x}[j] \right)
  - \sum_{1 \le i \le k} \left( \prod_{1 \le j < i} \vec{x}[j] \right)
  &< \prod_{1 \le m \le k} \vec{x}[m] \\
  \sum_{1 \le i \le k} \left( \prod_{1 \le j < i+1} \vec{x}[j] \right)
  - \sum_{1 \le i \le k} \left( \prod_{1 \le j < i} \vec{x}[j] \right)
  &< \prod_{1 \le m \le k} \vec{x}[m] \\
  \left( \prod_{1 \le j < k+1} \vec{x}[j] \right)
  + \sum_{1 \le i < k} \left( \prod_{1 \le j < i+1} \vec{x}[j] \right)
  - \sum_{1 < i \le k} \left( \prod_{1 \le j < i} \vec{x}[j] \right)
  - 1
  &< \prod_{1 \le m \le k} \vec{x}[m] \\
  \left( \prod_{1 \le j \le k} \vec{x}[j] \right)-1
  + \sum_{1 < i \le k} \left( \prod_{1 \le j < i} \vec{x}[j] \right)
  - \sum_{1 < i \le k} \left( \prod_{1 \le j < i} \vec{x}[j] \right)
  &< \prod_{1 \le m \le k} \vec{x}[m] \\
  \left( \prod_{1 \le j \le k} \vec{x}[j] \right)-1
  &< \prod_{1 \le m \le k} \vec{x}[m]
\end{align*}
Note in the 4th step there that $\bar{x}[i] \le \vec{x}[i]$ so replacing it with $\vec{x}[i]$
will not increase the maximum value.

So, that concludes the proof. Since the inverse works for any index of $\bar{x}$ we know that the inverse works to restore the
whole of the vector. Thus, $\Idx[\vec{x}]^{-1}$ is indeed the inverse of $\Idx[\vec{x}]$.
\end{proof}

TODO:exposition 

\begin{theorem}[Index of Concatenated Vectors]
Let $\vec{a}$ and $\vec{b}$ be any two vectors of natural numbers. Then for all $\bar{a} \in \Dim(\vec{a})$
and $\bar{b} \in \Dim(\vec{b})$:
\[ \Idx[\vec{a} \oplus \vec{b}](\bar{a} \oplus \bar{b}) = \Idx[\vec{a}](\bar{a}) + \shape{\Dim(\vec{a})} \Idx[\vec{b}](\bar{b}) \]
\end{theorem}
\begin{proof}
Let $\vec{a}$ and $\vec{b}$ be any two vectors of natural numbers. Then for all $\bar{a} \in \Dim(\vec{a})$
and $\bar{b} \in \Dim(\vec{b})$:
\begin{align*}
  \Idx[\vec{a} \oplus \vec{b}](\bar{a} \oplus \bar{b})
  &= 
  \sum_{1 \le i \le \shape{\vec{a} \oplus \vec{b}}} \left( ((\bar{a} \oplus \bar{b})[i]-1) \prod_{1 \le j < i} (\vec{a} \oplus \vec{b})[j] \right) \\
  &= 
  \left( \sum_{1 \le i \le \shape{\vec{a}}} \left( ((\bar{a} \oplus \bar{b})[i]-1) \prod_{1 \le j < i} (\vec{a} \oplus \vec{b})[j] \right) \right) +
  \left( \sum_{\shape{\vec{a}}+1 \le i \le \shape{\vec{a}}+\shape{\vec{b}}} \left( ((\bar{a} \oplus \bar{b})[i]-1) \prod_{1 \le j < i} (\vec{a} \oplus \vec{b})[j] \right) \right)\\
  &= 
  \left( \sum_{1 \le i \le \shape{\vec{a}}} \left( (\bar{a}[i]-1) \prod_{1 \le j < i} \vec{a}[j] \right) \right) +
  \left( \sum_{\shape{\vec{a}}+1 \le i \le \shape{\vec{a}}+\shape{\vec{b}}} \left( (\bar{b}[i-\shape{\vec{a}}]-1) \prod_{1 \le j \le \shape{\vec{a}}} \vec{a}[j] \prod_{1 \le j < i-\shape{\vec{a}}} \vec{b}[j] \right) \right)\\
  &= 
  \left( \sum_{1 \le i \le \shape{\vec{a}}} \left( (\bar{a}[i]-1) \prod_{1 \le j < i} \vec{a}[j] \right) \right) +
  \left( \sum_{1 \le i \le \shape{\vec{b}}} \left( (\bar{b}[i]-1) \shape{\Dim(\vec{a})} \prod_{1 \le j < i} \vec{b}[j] \right) \right)\\
  &= 
  \Idx[\vec{a}](\bar{a}) +
  \shape{\Dim(\vec{a})} \sum_{1 \le i \le \shape{\vec{b}}} \left( (\bar{b}[i]-1) \prod_{1 \le j < i} \vec{b}[j] \right)\\
  &= 
  \Idx[\vec{a}](\bar{a}) +
  \shape{\Dim(\vec{a})} \Idx[\vec{b}](\bar{b})
\end{align*}
\end{proof}

Now you might be wondering what that indexing function has to do with inverting
multimatricies. I said before that it we would need to show how the multiplication
of two multimatricies can be converted into an equivelent multiplication of two
2d matricies. Here's how that's done,

\begin{theorem}[Reshaping Multimatrix Multiplication]
Let $A$ and $B$ be two multimatricies of shapes $\shape{A} = \vec{a} \oplus \vec{c}$ and
$\shape{B} = \vec{c} \oplus \vec{b}$, and let $C$ be the multimatrix resulting in the $\shape{\vec{c}}$
width multiplication of $A$ and $B$, that is,
\[ A \mmult{\shape{\vec{c}}} B = C \]
Now, let $\vec{x}$, $\vec{y}$, and $\vec{z}$ be any vectors such that
$\shape{\Dim(\vec{a})} = \shape{\Dim(\vec{x})}$,
$\shape{\Dim(\vec{b})} = \shape{\Dim(\vec{y})}$,
and 
$\shape{\Dim(\vec{c})} = \shape{\Dim(\vec{z})}$.
Then for multimatricies $X$, $Y$, and $Z$ defined such that,
\[\forall i : 0 \le i < \size(A) \]
\[ X[\Idx[\vec{x} \oplus \vec{z}]^{-1}(i)] = A[\Idx[\vec{a} \oplus \vec{c}]^{-1}(i)] \]

\[\forall j : 0 \le j < \size(B)\]
\[ Y[\Idx[\vec{z} \oplus \vec{y}]^{-1}(j)] = B[\Idx[\vec{c} \oplus \vec{b}]^{-1}(j)] \]

and,

\[\forall k : 0 \le k < \size(C)\]
\[ Z[\Idx[\vec{x} \oplus \vec{y}]^{-1}(k)] = C[\Idx[\vec{a} \oplus \vec{b}]^{-1}(k)] \]

it is true that,

\[ X \mmult{\shape{\vec{z}}} Y = Z \]
\end{theorem}
\begin{proof}
Let $A$ and $B$ be two multimatricies of shapes $\shape{A} = \vec{a} \oplus \vec{c}$ and
$\shape{B} = \vec{c} \oplus \vec{b}$, and let $C$ be the multimatrix resulting from,
\[ A \mmult{\shape{\vec{c}}} B = C \]

Now, let $\vec{x}$, $\vec{y}$, and $\vec{z}$ be any vectors such that
$\shape{\Dim(\vec{a})} = \shape{\Dim(\vec{x})}$,
$\shape{\Dim(\vec{b})} = \shape{\Dim(\vec{y})}$,
and 
$\shape{\Dim(\vec{c})} = \shape{\Dim(\vec{z})}$.
And let multimatricies $X$, $Y$, and $Z$ be defined such that,
\[\forall i : 0 \le i < \size(A) \]
\[ X[\Idx[\vec{x} \oplus \vec{z}]^{-1}(i)] = A[\Idx[\vec{a} \oplus \vec{c}]^{-1}(i)] \]

\[\forall j : 0 \le j < \size(B)\]
\[ Y[\Idx[\vec{z} \oplus \vec{y}]^{-1}(j)] = B[\Idx[\vec{c} \oplus \vec{b}]^{-1}(j)] \]

and,

\[\forall k : 0 \le k < \size(C)\]
\[ Z[\Idx[\vec{x} \oplus \vec{y}]^{-1}(k)] = C[\Idx[\vec{a} \oplus \vec{b}]^{-1}(k)] \]

Then for all $\bar{a} \in \Dim(\vec{a})$ and $\bar{b} \in \Dim(\vec{b})$,

\begin{align*}
  (A \mmult{\shape{\vec{c}}} B)[\bar{a} \oplus \bar{b}]
  &= C[\bar{a} \oplus \bar{b}] \\
  &=
  \sum_{\forall \bar{c} \in \Dim(\vec{c})} A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}] \\
  &=
  \sum_{0 \le i_c < \shape{\Dim(\vec{c})}} A[\bar{a} \oplus \Idx[\vec{c}]^{-1}(i_c)] B[\Idx[\vec{c}]^{-1}(i_c) \oplus \bar{b}]
\end{align*}

Now let $i_a = \Idx[\vec{a}](\bar{a})$ and $i_b = \Idx[\vec{b}](\bar{b})$ and,
\begin{align*}
  C[\Idx[\vec{a}  \oplus \vec{b}]^{-1}(i_a + \shape{\Dim(\vec{a})} i_b)]
  &=
  \sum_{0 \le i_c < \shape{\Dim(\vec{c})}} A[\Idx[\vec{a}]^{-1}(i_a) \oplus \Idx[\vec{c}]^{-1}(i_c)] B[\Idx[\vec{c}]^{-1}(i_c) \oplus \Idx[\vec{b}]^{-1}(i_b)] \\
  &=
  \sum_{0 \le i_c < \shape{\Dim(\vec{c})}} A[\Idx[\vec{a} \oplus \vec{c}]^{-1}(i_a + \shape{\Dim(\vec{a})}i_c)]
                                     B[\Idx[\vec{c} \oplus \vec{b}]^{-1}(i_c + \shape{\Dim(\vec{c})}i_b)] \\
  &=
  \sum_{0 \le i_c < \shape{\Dim(\vec{c})}} X[\Idx[\vec{x} \oplus \vec{z}]^{-1}(i_a + \shape{\Dim(\vec{a})}i_c)]
                                     Y[\Idx[\vec{z} \oplus \vec{y}]^{-1}(i_c + \shape{\Dim(\vec{c})}i_b)] \\
  Z[\Idx[\vec{x}  \oplus \vec{y}]^{-1}(i_a + \shape{\Dim(\vec{a})} i_b)]
  &=
  \sum_{0 \le i_c < \shape{\Dim(\vec{c})}} X[\Idx[\vec{x} \oplus \vec{z}]^{-1}(i_a + \shape{\Dim(\vec{a})}i_c)]
                                     Y[\Idx[\vec{z} \oplus \vec{y}]^{-1}(i_c + \shape{\Dim(\vec{c})}i_b)] \\
  Z[\Idx[\vec{x}  \oplus \vec{y}]^{-1}(i_a + \shape{\Dim(\vec{x})} i_b)]
  &=
  \sum_{0 \le i_c < \shape{\Dim(\vec{z})}} X[\Idx[\vec{x} \oplus \vec{z}]^{-1}(i_a + \shape{\Dim(\vec{x})}i_c)]
                                     Y[\Idx[\vec{z} \oplus \vec{y}]^{-1}(i_c + \shape{\Dim(\vec{z})}i_b)] \\
  Z[\Idx[\vec{x}]^{-1}(i_a) \oplus \Idx[\vec{y}]^{-1}(i_b)]
  &=
  \sum_{0 \le i_c < \shape{\Dim(\vec{z})}} X[\Idx[\vec{x}]^{-1}(i_a) \oplus \Idx[\vec{z}]^{-1}(i_c)]
                                     Y[\Idx[\vec{z}]^{-1}(i_c) \oplus \Idx[\vec{y}]^{-1}(i_b)]
\end{align*}
Then if you let $\bar{x} = \Idx[\vec{x}](i_a)$ and $\bar{y} = \Idx[\vec{y}](i_b)$,
\begin{align*}
  Z[\bar{x} \oplus \bar{y}]
  &=
  \sum_{0 \le i_c < \shape{\Dim(\vec{z})}} X[\bar{x} \oplus \Idx[\vec{z}]^{-1}(i_c)]
                                     Y[\Idx[\vec{z}]^{-1}(i_c) \oplus \bar{y}] \\
  &=
  \sum_{\forall \bar{z} \in \Dim(\vec{z})} X[\bar{x} \oplus \bar{z}]
                                     Y[\bar{z} \oplus \bar{y}] \\
  &= X \mmult{\shape{\vec{z}}} Y
\end{align*}
\end{proof}
\end{landscape}

Therefore for any $\RMult(m,X)$ you can potentially find an inverse with
multiplication width $n \le \shape{\shape{X}}-m$ by simply reshaping the problem into
one using standard two dimensional matricies wich already has thoroughly
explored methods for solving.
For example, to find an $\RMult(n,Y)$ which is the inverse of $\RMult(m,X)$
you'd reshape $X \mmult{n} Y = \Ident^2(\vec{m})$ into $A \mmult{1} B = \Ident^2(\left<\size(\Dim(\vec{m}))\right>)$
where $\shape{A} = \left<\size(\Dim(\vec{m} \oplus \vec{x_l})), \size(\Dim(\vec{x_r}))\right>$
and   $\shape{B} = \left<\size(\Dim(\vec{x_r})),  \size(\Dim(\vec{y}))\right>$.
Then solve for $B$ and return the problem to the original shape to get $Y$.

%todo: give example

\section{Hadamard Product}

What the hell is a `Hadamard product'? The Hadamard product \cite{wiki:hadamard}
of two matricies is how one would actually expect matrix multiplication to work.

In short, if two matricies, $A$ and $B$ have the same shape,
\[ \shape{A} = \shape{B} = \left<r,c\right> \]
Then their Hadamard product, $A \bullet B$, is the same shape and each element
is just the product of the corresponding elements of $A$ and $B$. That is,
\[ (A \bullet B)[r,c] = A[r,c] B[r,c] \]

This seems to extend quite naturally into the world of multimatricies.
But hold on - there's a catch. While the Hadamard product only logically applies
to regular matricies when their shapes are exactly equal, there are case where
the Hadamard product can easily be applied to differently shaped multimatricies,
they just need to have overlapping portions of their shape vectors, just as
with multimatrix multiplication.

\begin{definition}[Multimatrix Hadamard Product]
\label{multi_had_prod}
Let $A$ and $B$ be multimatricies such that $\shape{A} = \vec{a} \oplus \vec{c}$
and $\shape{B} = \vec{c} \oplus \vec{b}$. Also let $c = \shape{\vec{c}}$.
Then, the multimatrix Hadamard product, $A \dmult{c} B$, is a multimatrix
such that,
\[ \shape{A \dmult{c} B} = \vec{a} \oplus \vec{c} \oplus \vec{b} \]
And for any $\bar{a} \in \Dim(\vec{a})$, $\bar{b} \in \Dim(\vec{b})$,
and $\bar{c} \in \Dim(\vec{c})$,
\[
  (A \dmult{c} B)[\bar{a} \oplus \bar{c} \oplus \bar{b}]
  = A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}]
\]
\end{definition}

TODO: exposition

\begin{theorem}[Equivalent Zero Hadamard Product and Multiplication]
\[ A \dmult{0} B = A \mmult{0} B \]
\end{theorem}
\begin{proof}
Let $\shape{A} = \vec{a}$ and $\shape{B} = \vec{b}$ then,
\[ \shape{A \dmult{0} B} = \vec{a} \oplus \vec{b} = \shape{A \mmult{0} B} \]
And for all $\bar{a} \in \Dim(\vec{a})$ and $\bar{b} \in \Dim(\vec{b})$,
\[ (A \dmult{0} B)[\bar{a} \oplus \bar{b}] = A[\bar{a}] B[\bar{b}] = (A \mmult{0} B)[\bar{a} \oplus \bar{b}] \]
\end{proof}

TODO: exposition

\begin{theorem}[Hadamard Product Associativity]
Let $A$, $B$, and $C$ be multimatrixes such that $\shape{A} = \vec{a} \oplus \vec{m}$,
$\shape{B} = \vec{m} \oplus \vec{b} \oplus \vec{n}$ and $\shape{C} = \vec{n} \oplus \vec{c}$,
where $\shape{\vec{m}} = m$ and $\shape{\vec{n}} = n$.
Then $(A \dmult{m} B) \dmult{n} C = A \dmult{m} (B \dmult{n} C)$.
\end{theorem}
\begin{proof}
For all $\bar{a} \in \Dim(\vec{a})$, etc,
\begin{align*}
((A \dmult{m} B) \dmult{n} C)[\bar{a} \oplus \bar{m} \oplus \bar{b} \oplus \bar{n} \oplus \bar{c}]
  &= (A \dmult{m} B)[\bar{a} \oplus \bar{m} \oplus \bar{b} \oplus \bar{n}] C[\bar{n} \oplus \bar{c}] \\
  &= (A[\bar{a} \oplus \bar{m}] B[\bar{m} \oplus \bar{b} \oplus \bar{n}]) C[\bar{n} \oplus \bar{c}] \\
  &= A[\bar{a} \oplus \bar{m}] (B[\bar{m} \oplus \bar{b} \oplus \bar{n}] C[\bar{n} \oplus \bar{c}]) \\
  &= A[\bar{a} \oplus \bar{m}] (B \dmult{n} C)[\bar{m} \oplus \bar{b} \oplus \bar{n} \oplus \bar{c}] \\
  &= (A \dmult{m} (B \dmult{n} C))[\bar{a} \oplus \bar{m} \oplus \bar{b} \oplus \bar{n} \oplus \bar{c}]
\end{align*}
\end{proof}

TODO: exposition

\begin{theorem}[Equivalent Hadamard Product and Multiplication Chains]
When $X_1 \ldots X_{m-1}$ are all in $\mathbb{R}^{\vec{x}}$ and
$\shape{X_m} = \vec{x} \oplus \vec{r}$ for any $\vec{r}$,
\[
  (((\Ident^n(\vec{x})
  \mmult{\shape{\vec{x}}} X_1) \mmult{\shape{\vec{x}}} X_2) \mmult{\shape{\vec{x}}} X_3)
  \ldots X_m
  =
  \Ident^{n-m}(\vec{x})
  \dmult{\shape{\vec{x}}} X_1 \dmult{\shape{\vec{x}}} X_2 \dmult{\shape{\vec{x}}} X_3
  \ldots X_m
\]
And when $X_2 \ldots X_m$ are all in $\mathbb{R}^{\vec{x}}$ and
$\shape{X_1} = \vec{l} \oplus \vec{x}$ for any $\vec{l}$,
\[
  X_1 \mmult{\shape{\vec{x}}} (X_2 \mmult{\shape{\vec{x}}} (X_3
  \ldots (X_m \mmult{\shape{\vec{x}}} 
  \Ident^n(\vec{x}))))
  =
  X_1 \dmult{\shape{\vec{x}}} X_2 \dmult{\shape{\vec{x}}} X_3
  \ldots X_m \dmult{\shape{\vec{x}}} 
  \Ident^{n-m}(\vec{x})
\]
\end{theorem}
\begin{proof}
TODO 
\end{proof}

TODO: exposition

\begin{theorem}[Communicativity of Hadamard Products]
If $A$ and $B$ are multimatrices of the same shape, $\shape{A} = \shape{B} = \vec{x}$,
then $A \dmult{\shape{\vec{x}}} B = B \dmult{\shape{\vec{x}}} A$.
\end{theorem}
\begin{proof}
For any $\bar{x} \in \Dim(\vec{x})$,
\begin{align*}
(A \dmult{\shape{\vec{x}}} B)[\bar{x}]
  &= A[\bar{x}] B[\bar{x}] \\
  &= B[\bar{x}] A[\bar{x}] \\
  &= (B \dmult{\shape{\vec{x}}} A)[\bar{x}]
\end{align*}
\end{proof}

\section{Exercises}

\begin{exercise}
Let $A$, $B$, and $C$ be multimatricies such that $\shape{A} = \left<1,2,3\right>$,
$\shape{B} = \left<3,1,2\right>$ and $\shape{C} = \left<3,2,1\right>$. For each of these
multiplications determine if it is valid, and if so, what the resulting
shape would be:
\begin{enumerate}
\item $A \mmult{2} B$
\item $B \mmult{2} A$
\item $A \mmult{2} C$
\item $A \mmult{1} C$
\end{enumerate}
\end{exercise}

\begin{exercise}
In which of these cases does $A \mmult{m} (B \mmult{n} C) = (A \mmult{m} B) \mmult{n} C$?
\begin{enumerate}
\item When,
	\begin{itemize}
		\item $\shape{A} = \left<3, 7, 9\right>$
		\item $\shape{B} = \left<9, 5\right>$
		\item $\shape{C} = \left<5, 7, 3\right>$
		\item and $m = n = 1$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $\shape{A} = \left<6, 6, 6\right>$
		\item $\shape{B} = \left<6, 6, 6\right>$
		\item $\shape{C} = \left<6, 6, 6\right>$
		\item and $m = n = 2$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $\shape{A} = \left<3, 5, 3\right>$
		\item $\shape{B} = \left<5, 3, 11\right>$
		\item $\shape{C} = \left<11, 2, 7\right>$
		\item $m = 2$
		\item and $n = 1$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $\shape{A} = \left<3, 5, 3\right>$
		\item $\shape{B} = \left<5, 3, 3, 5\right>$
		\item $\shape{C} = \left<3, 3, 5, 2\right>$
		\item $m = 2$
		\item and $n = 3$
	\end{itemize}
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $\shape{X} = \vec{x}$. Simplify the following where possible:
\begin{enumerate}
\item $\Ident^5(\vec{x}) \mmult{3 \shape{\vec{x}}} \Ident^3(\vec{x})$
\item $\Ident^2(\vec{x}) \mmult{\shape{\vec{x}}} X$
\item $\Ident^3(\vec{x}) \mmult{2\shape{\vec{x}}} X$
\item $\Ident^2(\vec{x}) \mmult{\shape{\vec{x}}} \left(X \mmult{\shape{\vec{x}}} \Ident^3(\shape{\vec{x}})
				\right) \mmult{2\shape{\vec{x}}} \Ident^3(\shape{\vec{x}})$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $M$ be some multimatrix such that $\shape{M} = \left<5,2,3\right>$. Then what is
$dM/dM$?
\end{exercise}

\begin{exercise}
Let $Y = \tan(X)$ be the elementwise application of the scalar tangent function to
the elements of $X$, that is, for all $\bar{v} \in \Dim(\shape{X})$
\[ Y[\bar{v}] = \tan(X[\bar{v}]) \]
What is $dY/dX$?
\end{exercise}

\chapter{Other Operations With Multimatricies}

\begin{displayquote}
``You can't just start a section with a subsection heading. At least put a quote
in there or something,'' - Dr. William Tepfenhart, my software architecture professor.
\end{displayquote}

\section{Scalars}

One of those obvious operations we'll need to do is multiply
a multimatrix by a constant. I guess that means we need another definition,

\begin{definition}[Multimatrix Scalar Multiplication]
Let $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$. If
\[ sA = As = B \]
Then
\[ \forall \bar{x} \in \Dim(\vec{x}):
   B[\bar{x}] = (s)(A[\bar{x}]) \]
In other words, multiplying $A$ by a scalar $s$ simply multiplies each of
it's elements by that scalar.
\end{definition}

But actually, you'll find that scalars can be modeled equally well by multricies
with the shape $\left<\right>$. It works with multiplication and derivatives. Look:

\begin{theorem}[Empty Shape Multimatrix and Scalar Multiplication Equivalence]
\label{s_mm_mult_equiv}
For any multiplication operation defined between multimatricies and scalars,
the scalar may be replaced by an equivalent empty-shaped multimatrix whose
single value is that of the scalar.

In other words, let $s \in \mathbb{R}$ and $S \in \mathbb{R}^{\left<\right>}$ such that
$S[\left<\right>] = s$. Then, for any multimatrix $A$, 
\[ sA = As = S \mmult{0} A = A \mmult{0} S \]
\end{theorem}
\begin{proof}
Let $\shape{A} = \vec{a}$. Then for all $\bar{a} \in \Dim(\vec{a})$,
\begin{align*}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= A[\bar{a}](s) \\
	&= (As)[\bar{a}]
\end{align*}
So $sA = As$. Also,
\begin{align*}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= S[\left<\right>]A[\bar{a}] \\
	&= \sum_{\forall \bar{c} \in \Dim(\left<\right>)}
		S[\left<\right> \oplus \bar{c}]A[\bar{c} \oplus \bar{a}] \\
	&= \sum_{\forall \bar{c} \in \{\left<\right>\}}
		S[\left<\right> \oplus \bar{c}]A[\bar{c} \oplus \bar{a}] \\
	&= S[\left<\right> \oplus \left<\right>]A[\left<\right> \oplus \bar{a}] \\
	&= S[\left<\right>]A[\bar{a}] \\
	&= (S \mmult{0} A)[\left<\right> \oplus \bar{a}] \\
	&= (S \mmult{0} A)[\bar{a}] \\
\end{align*}
So, $sA = S \mmult{0} A$.
Finally, by similiar reasoning,
\begin{align*}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= A[\bar{a}](s) \\
	&= A[\bar{a}]S[\left<\right>] \\
	&= \sum_{\forall \bar{c} \in \Dim(\left<\right>)}
		A[\bar{a} \oplus \bar{c}] S[\bar{c} \oplus \left<\right>] \\
	&= \sum_{\forall \bar{c} \in \{\left<\right>\}}
		A[\bar{a} \oplus \bar{c}] S[\bar{c} \oplus \left<\right>] \\
	&= A[\bar{a} \oplus \left<\right>] S[\left<\right> \oplus \left<\right>] \\
	&= A[\bar{a}]S[\left<\right>] \\
	&= (A \mmult{0} S)[\bar{a} \oplus \left<\right>] \\
	&= (A \mmult{0} S)[\bar{a}]
\end{align*}
So $sA = A \mmult{0} S$. Therefore we've shown that
$sA = As = S \mmult{0} A = A \mmult{0} S$
\end{proof}

It wasn't until I noticed this that I began to suspect that the same might be true
of the derivative rules.
A quick check confirmed that the dimensionality worked out and it seems intuitive
that we would always be able to treat scalars
as empty-shape multimatricies but let's canonize it.

\begin{theorem}[Empty Shape Multimatrix and Scalar Domain Derivative Equivalence]
\label{s_mm_domain_equiv}
That really rolls off the tongue.

Let $F(x)$ be a function $F : \mathbb{R} \to \mathbb{R}^{\vec{f}}$. And let
$G(X)$ be an equivalent function,
$G : \mathbb{R}^{\left<\right>} \to \mathbb{R}^{\vec{f}}$
such that for all $X \in \mathbb{R}^{\left<\right>}$ it is true that
$G(X) = F(X[\left<\right>])$
Then,
\[ \frac{dF(x)}{dx} = \frac{dG(X)}{dX} \]
\end{theorem}
\begin{argument}
Let $\bar{f} \in \Dim(\vec{f})$ then for $F(x)[\vec{f}] = G(X)[\vec{f}]$.
Since $\partial F(x)[\vec{f}] / \partial x$ is defined as the rate of
change of $F(x)[\vec{f}]$ with respect to $x$, it stands to reason that,
$\partial F(X[\left<\right>])[\vec{f}] / \partial X[\left<\right>]$ is
equivalent by definition. By are given $G(X) = F(X[\left<\right>])$ we
can then also say that,
\[
\frac{ \partial F(X[\left<\right>])[\vec{f}] }{ \partial X[\left<\right>] }
=
\frac{ \partial G(X)[\vec{f}] }{ \partial X[\left<\right>] }
\]
Therefore, by our definitions of derivatives,
\[
\frac{ d F(x) }{ d x }
=
\frac{ d G(X) }{ d X }
\]
\end{argument}

\begin{theorem}[Empty Shape Multimatrix and Scalar Range Derivative Equivalence]
\label{s_mm_range_equiv} Or `E.S.M.a.S.R.D.E', for short.

Let $f(X)$ be a function $f : \mathbb{R}^{\vec{x}} \to \mathbb{R}$. And let
$F(X)$ be an equivalent function,
$F : \mathbb{R}^{\vec{x}} \to \mathbb{R}^{\left<\right>}$
such that for all $X \in \mathbb{R}^{\vec{x}}$ it is true that
$F(X)[\left<\right>] = f(X)$
Then,
\[ \frac{df(X)}{dX} = \frac{dF(X)}{dX} \]
\end{theorem}
\begin{argument}
Let $\bar{x} \in \Dim(\vec{x})$. We note that as $\partial f(X) / \partial X[\bar{x}]$
is defined as the rate of change of $f(X)$ as $X[\bar{x}]$ changes, and each value of
$f(X)$ is equal to $F(X)[\left<\right>]$, we can reason that the rate of change of
$F(X)[\left<\right>]$ as $X[\bar{x}]$ changes is equal to the rate of change of $f(X)$
as $X[\bar{x}]$ changes. That is,
\[
	\frac{\partial f(X)}{\partial X[\bar{x}]}
		=
	\frac{\partial F(X)[\left<\right>]}{\partial X[\bar{x}]}
\]
And so by our definition of derivatives,
\[ \frac{df(X)}{dX} = \frac{dF(X)}{dX} \]
\end{argument}

\section{Addition}

Ah addition. What can I say about multimatrix addition that hasn't been
said so many times before. Well... anything, actually. I don't think anyone's
written about multimatrix addition before. Probably should define it first
though.

\begin{definition}[Multimatrix Addition]
Let $A, B, C \in \mathbb{R}^{\vec{x}}$. If
\[ A + B = C \]
Then
\[ \forall \bar{x} \in \Dim(\vec{x}):
   C[\bar{x}] = A[\bar{x}] + B[\bar{x}] \]
In other words, adding two multimatricies (which must be of the same shape) results
in a multimatrix of the same shape with elements which are the sum of the corresponding
elements of the added multimatricies.
\end{definition}

Multimatrix subtraction is defined as the inverse of multimatrix addition.

\begin{definition}[Multimatrix Subtraction]
Let $A, B \in \mathbb{R}^{\vec{x}}$. Then,
\[ (A + B) - B = A \]
\end{definition}

The following should be obvious, condesendingly so,

\begin{theorem}[Equivalent Addition for Subtraction]
\label{eq_add_sub_thm}
For any multimatricies $A$ and $B$ where $\shape{A} = \shape{B}$,
\[ A - B = A + (-1)B \]
\end{theorem}
\begin{proof}
Left as an exercise for the reader (Exercise \ref{eq_add_sub_ex}).
\end{proof}

As with scalar, vector, or regular matrix addition, multimatrix addition is
communicative. 

\begin{theorem}[Communicative Property of Multimatrix Addition]
Let $A, B \in \mathbb{R}^{\vec{x}}$. Then,
\[ A + B = B + A \]
\end{theorem}
\begin{proof}
Let $C = A + B$ then,
\[ \forall \bar{x} \in \Dim(\vec{x}) : C[\bar{x}] = A[\bar{x}] + B[\bar{x}] \]
By standard scalar addition rules that also means that,
$C[\bar{x}] = B[\bar{x}] + A[\bar{x}]$. So $C = B + A = A + B$.
\end{proof}

It's similarly trivial to show that multimatrix addition is associative.

\begin{theorem}[Associative Property of Multimatrix Addition]
Let $A, B, C \in \mathbb{R}^{\vec{x}}$. Then,
\[ (A + B) + C = A + (B + C) \]
\end{theorem}
\begin{proof}
For all $\bar{x} \in \Dim(\vec{x})$,
\begin{align*}
\left( (A+B)+C \right)[\bar{x}]
&= (A[\bar{x}]+B[\bar{x}])+C[\bar{x}] \\
&= A[\bar{x}]+(B[\bar{x}]+C[\bar{x}]) \\
&= \left( A+(B+C) \right)[\bar{x}]
\end{align*}
\end{proof}

A little less obvious is that multimatrix multiplication can be
distributed over it's addition.

\begin{landscape}
\begin{theorem}[Multiplication Distributive Property]
If $\shape{A} = \vec{a} \oplus \vec{n}$ and $\shape{B}=\shape{C}=\vec{n} \oplus \vec{r}$ then,
\[ A \mmult{n} (B + C) = A \mmult{n} B + A \mmult{n} C \]
Similarly, if $\shape{C} = \vec{n} \oplus \vec{c}$ and $\shape{A}=\shape{B}=\vec{l} \oplus \vec{n}$ then,
\[ (A + B) \mmult{n} C = A \mmult{n} C + B \mmult{n} C \]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $\shape{A} = \vec{a} \oplus \vec{n}$ and $\shape{B}=\shape{C}=\vec{n} \oplus \vec{r}$.
So, $\forall \bar{a} \in \Dim(\vec{a}), \bar{r} \in \Dim(\vec{r}) :$
\begin{align*}
 \left( A \mmult{n} (B + C) \right)[\bar{a} \oplus \bar{r}]
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] (B + C)[\bar{n} \oplus \bar{r}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] (B[\bar{n} \oplus \bar{r}] + C[\bar{n} \oplus \bar{r}]) \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] B[\bar{n} \oplus \bar{r}]
  + A[\bar{a} \oplus \bar{n}]C[\bar{n} \oplus \bar{r}] \\
 &= \left( \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] B[\bar{n} \oplus \bar{r}] \right)
    +
    \left( \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}]C[\bar{n} \oplus \bar{r}] \right) \\
 &= \left( A \mmult{n} B \right)[\bar{a} \oplus \bar{r}] +
     \left( A \mmult{n} C \right)[\bar{a} \oplus \bar{r}]
\end{align*}
So $A \mmult{n} (B + C) = A \mmult{n} B + A \mmult{n} C$
\end{ppart}
\begin{ppart}
Let $\shape{C} = \vec{n} \oplus \vec{c}$ and $\shape{A}=\shape{B}=\vec{l} \oplus \vec{n}$.
So, $\forall \bar{c} \in \Dim(\vec{c}), \bar{l} \in \Dim(\vec{l}) :$
\begin{align*}
 \left( (A + B) \mmult{n} C \right)[\bar{l} \oplus \bar{c}]
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    (A+B)[\bar{l} \oplus \bar{n}] C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    (A[\bar{l} \oplus \bar{n}]+B[\bar{l} \oplus \bar{n}])C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]
    +B[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}] \\
 &= \left(\sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]\right)
    +
    \left(\sum_{\forall \bar{n} \in \Dim(\vec{n})}
    B[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]\right) \\
 &= \left(A \mmult{n} C\right)[\bar{l} \oplus \bar{c}]
    +\left(B \mmult{n} C\right)[\bar{l} \oplus \bar{c}]
\end{align*}
So $(A+B) \mmult{n} C = A \mmult{n} C + B \mmult{n} C$ 
\end{ppart}
\end{proof}
\end{landscape}

But it should be obvious that that the same applies to scalar 
multiplication distributing over multimatrix addition, since
scalar multiplication is equivalent to multiplication by an
empty shaped multimatrix.

\begin{corollary}[Scalar on Multimatrix Distributive Property]
If $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$ then,
\[ s(A + B) = sA + sB \]
\end{corollary}
\begin{proof}
Let $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$.
Then for all $\bar{x} \in \Dim(\vec{x})$
\begin{align*}
(s(A+B))[\bar{x}]
&= (s)((A+B)[\bar{x}]) \\
&= (s)(A[\bar{x}] + B[\bar{x}]) \\
&= (s)(A[\bar{x}]) + s(B[\bar{x}])
\end{align*}
Therefore $s(A+B) = sA + sB$
\end{proof}

\section{Transposition}

Multimatrix transposition clearly requires an additional parameter defining where
the cut happens in the shape vector.

\begin{definition}[Multimatrix Transposition]
\label{tran_def}
Let $\shape{X} = \vec{l} \oplus \vec{r}$ where $\shape{\vec{l}} = l$ and $\shape{\vec{r}} = r$
\[ \shape{\Tran(X, l)} = \vec{r} \oplus \vec{l} \]
And,
\[ \forall \bar{l} \in \Dim(\vec{l}), \bar{r} \in \Dim(\vec{r}) : \]
\[ \Tran(X, l)[\bar{r} \oplus \bar{l}] = X[\bar{l} \oplus \bar{r}] \]
We call $l$ the index of transpose.
\end{definition}

Here's an example - get out your crayons and follow along.

\begin{example}
Let's construct some multimatrix, $M$, with a shape:
\[ \shape{M} = \left<3,2,7,5,9\right> \]
Then lets take the first transpose of that
multimatrix:
\[ \shape{\Tran(M, 1)} = \left<2,7,5,9,3\right> \]
And... how about the third:
\[ \shape{\Tran(M, 3)} = \left<5,9,3,2,7\right> \]
You can see, it's as though I cut off the front of the shape vector and transfered
it to the end of the vector.

You do the same thing with indicies. So,
\[ M[1,2,3,4,5] = \Tran(M, 3)[4,5,1,2,3] \]
\end{example}

However, you'll see cases in later chapters of multimatrix transposes where
the index of transpose is larger than the size of the shape matrix. So we need
to make sense of that operation as well. Fortunately, there is a logical way
to extend the definition. Consider that this transposition can be reasonably
shown to be equivalent to moving the first index of the shape matrix
to the end of the shape matrix over and over again, this can be done any number
of times, therefore doing so for more than the size of the shape matrix still works,
it just requires a bit of modular arithmetic.

\begin{definition}[Multimatrix Transpose Extension]
\label{tran_ext}
For any multimatrix, the following transposes are equivalent,
\[ \Tran(X, n) = \Tran(X, \remainder(n, \shape{\shape{X}})) \]
Where $\remainder(n, \shape{\shape{X}})$ is the remainder of $n$ divided by $\shape{\shape{X}}$.
That is, a value $0 \le \remainder(n, \shape{\shape{X}}) < \shape{\shape{X}}$ such that there exists an
integer $k$ where, $n = k\shape{\shape{X}} + \remainder(n, \shape{\shape{X}})$.
\end{definition}

This extension is consisent with the prevous definition because, for any
index of transpose, $l$, that would be covered by the original definition,
$0 \le l \le \shape{\shape{X}}$, you can see that, in all but one case $\remainder(l, \shape{\shape{X}}) = l$.
The exception, $l = \shape{\shape{X}}$ is covered by showing that
$\remainder(\shape{\shape{X}}, \shape{\shape{X}}) = 0$ and $\Tran(X, 0) = \Tran(X, \shape{\shape{X}})$. 

Thus, we (by which I mean \textit{me} since \textit{you} certainly haven't done any
of the work) have shown a natural equivalent class \cite{book:abstract} of transpose
operations. On multimatricies with a shape of some length $n$ there are $n$ unique
transpose; $\Tran(X, i)$ for all $0 \le i < n$. All other transposes are equivalent
to one of these base transposes by our extended definition.

To make use of this equivalence, we need to first show how these classes are
isomorphic \cite{book:abstract} to a more manageable group.

\begin{theorem}[Isomorphism of Transpostion and Modular Integers]
\label{tran_int_iso}
Let $\mathbb{T}_n$ be the set of possible transpositions on multimatricies with
shapes of length $n$. Let $\circ$ be the operation of composition. Then
$(\mathbb{T}_n, \circ)$ is a group which is isomorphic to the group of
integers mod $n$ under addition. That is,
\[ (\mathbb{T}_n, \circ) \cong (\mathbb{Z}_n, +) \]
\end{theorem}
\begin{proof}
I'll need to tackle this proof in two parts. First I'll need to show that
that set of transpositions is a group under composition, then I'll need to
provide a one-to-one and onto map that preseves the respective group operations
between transpositions and integers mod $n$.

\begin{ppart}
To begin, I'll show that the set of all transpositions on multimatricies 
with shapes of length $n$ form a group under composition.

Let's look at the composition of some two transposes on a multimatrix.
So, let $A$ be a multimatrix with shape of length $n$. Now let $x$ and $y$
be indicies of transpositions. Then what is $\Tran(\Tran(A, x), y)$?
To apply the the first transpose we need to break $\shape{A}$ into two vectors,
so let $\shape{A} = \vec{l} \oplus \vec{r}$ where $\shape{\vec{l}} = x$.

Then,
\[ \shape{\Tran(A, x)} = \vec{r} \oplus \vec{l} \]
And for all
$\bar{l} \in \Dim(\vec{l})$ and $\bar{r} \in \Dim(\vec{r})$,
\[ \Tran(A, x)[\bar{r} \oplus \bar{l}] = A[\bar{l} \oplus \bar{r}] \]

Now to apply a transposition with index of transposition $y$ to $\Tran(A,x)$
we need to break up $\shape{\Tran(A,x)}$ into two vector, such as
$\shape{\Tran(A,x)} = \vec{l}' \oplus \vec{r}'$.

Since we've already shown that $\shape{\Tran(A,x)} = \vec{r} \oplus \vec{l}$
we end up with two conceptual splits of that shape vector that need to
be reconciled.

\begin{case} $\shape{\vec{l}'} \le \shape{\vec{r}}$

Then we can divide $\shape{\Tran(A,x)}$ into three vectors, $\vec{a}$, $\vec{b}$, and $\vec{c}$
such that
\[ \shape{\Tran(A,x)} = \vec{b} \oplus \vec{c} \oplus \vec{a} \]
\[ \vec{l}' = \vec{b} \]
\[ \vec{r}' = \vec{c} \oplus \vec{a} \]
\[ \vec{r} = \vec{b} \oplus \vec{c} \]
\[ \vec{l} = \vec{a} \]
It helps to think of the $\oplus$ operation as a cut in two separate places in the same
vector because of the two conceptual splits from the two transpositions to see why this
logic is valid.

So we can then say,
\[ \shape{A} = \vec{a} \oplus \vec{b} \oplus \vec{c} \]

And since,
\[ \shape{\Tran(\Tran(A, x), y)} = \vec{r}' \oplus \vec{l}' \]
It follows that,
\[ \shape{\Tran(\Tran(A, x), y)} = \vec{c} \oplus \vec{a} \oplus \vec{b} \]

Now, let $\bar{a} \in \Dim(\vec{a})$, $\bar{b} \in \Dim(\vec{b})$, and
$\bar{c} \in \Dim(\vec{c})$. Notice that because of Theorem 
\ref{dim_set_ind_concat_thm}, $\bar{c} \oplus \bar{a} \in \Dim(\vec{r}')$ and
$\bar{b} \oplus \bar{c} \in \Dim(\vec{r})$.

Continuing our definition of transposition,
\begin{align*}
  \Tran(\Tran(A, x), y)[\bar{c} \oplus \bar{a} \oplus \bar{b}]
  &=
  \Tran(A, x)[\bar{b} \oplus \bar{c} \oplus \bar{a}] \\
  &=
  A[\bar{a} \oplus \bar{b} \oplus \bar{c}]
\end{align*}

See now that this composit operation is also a transposition, $\Tran(A, z)$ where,
$z = \shape{\vec{a} \oplus \vec{b}}$.
So by definition,
\begin{align*}
  \shape{\Tran(A, z)}
  &=
  \vec{c} \oplus \vec{b} \oplus \vec{a} \\
  &=
  \shape{\Tran(\Tran(A, x), y)}  
\end{align*}
And,
\begin{align*}
  \Tran(A, z)[\bar{c} \oplus \bar{a} \oplus \bar{b}]
  &=
  A[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
  &=
  \Tran(\Tran(A, x), y)[\bar{c} \oplus \bar{a} \oplus \bar{b}]
\end{align*}
\end{case}

\begin{case} $\shape{\vec{l}'} > \shape{\vec{r}}$
Then we can divide $\shape{\Tran(A,x)}$ into three vectors, $\vec{a}$, $\vec{b}$, and $\vec{c}$
such that
\[ \shape{\Tran(A,x)} = \vec{c} \oplus \vec{a} \oplus \vec{b} \]
\[ \vec{l}' = \vec{c} \oplus \vec{a} \]
\[ \vec{r}' = \vec{b} \]
\[ \vec{r} = \vec{c} \]
\[ \vec{l} = \vec{a} \oplus \vec{b} \]

So we can then say,
\[ \shape{A} = \vec{a} \oplus \vec{b} \oplus \vec{c} \]

And since,
\[ \shape{\Tran(\Tran(A, x), y)} = \vec{r}' \oplus \vec{l}' \]
It follows that,
\[ \shape{\Tran(\Tran(A, x), y)} = \vec{b} \oplus \vec{c} \oplus \vec{a} \]

Now, let $\bar{a} \in \Dim(\vec{a})$, $\bar{b} \in \Dim(\vec{b})$, and
$\bar{c} \in \Dim(\vec{c})$. Notice that because of Theorem 
\ref{dim_set_ind_concat_thm}, $\bar{c} \oplus \bar{a} \in \Dim(\vec{l}')$ and
$\bar{a} \oplus \bar{b} \in \Dim(\vec{l})$.

Continuing our definition of transposition,
\begin{align*}
  \Tran(\Tran(A, x), y)[\bar{b} \oplus \bar{c} \oplus \bar{a}]
  &=
  \Tran(A, x)[\bar{c} \oplus \bar{a} \oplus \bar{b}] \\
  &=
  A[\bar{a} \oplus \bar{b} \oplus \bar{c}]
\end{align*}

See now that this composit operation is also a transposition, $\Tran(A, z)$ where,
$z = \shape{\vec{a}}$.
So by definition,
\begin{align*}
  \shape{\Tran(A, z)}
  &=
  \vec{b} \oplus \vec{c} \oplus \vec{a} \\
  &=
  \shape{\Tran(\Tran(A, x), y)}  
\end{align*}
And,
\begin{align*}
  \Tran(A, z)[\bar{b} \oplus \bar{c} \oplus \bar{a}]
  &=
  A[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
  &=
  \Tran(\Tran(A, x), y)[\bar{b} \oplus \bar{c} \oplus \bar{a}]
\end{align*}
\end{case}

So I've shown that in both cases, the composit function of two transposes is
itself a transpose. So transposes of multimatrcies of shape of length $n$ are
a closed group under composition.
\end{ppart}

\begin{ppart}
In this last part, I'll define a bijective map
$\phi: \mathbb{T}_n \to \mathbb{Z}_n$ for which
\[ \phi(\Tran_y \circ \Tran_x) = \phi(\Tran(y)) + \phi(\Tran(x)) \]
Which is the definition of an isomorphism.

Note that I'm letting $\Tran_i$ be a shorthand for ``a transposition
with index of transposition $i$ that is in $\mathbb{T}_n$''.

So let's just define $\phi$.
\[ \phi(\Tran_i) = i \]
Done. I'm a genious.

Ok, the hard part is showing that it preserves the group operations.
Let's go back to our example in the first part of this proof. Let $A$ be
any multimatrix with shape length $n$. Then let there be two transpositons
$\Tran_x$ and $\Tran_y$ which we'll apply in series. First remember that
to define $\Tran_x$ we needed to divide $\shape{A}$ into $\shape{A} = \vec{l} \oplus \vec{r}$
where $\shape{\vec{l}} = x$. Then we'd say that
\[ \shape{\Tran(A, x)} = \vec{r} \oplus \vec{l} \]
Then to further apply $\Tran(\Tran(A, x), y)$ we need to divide $\shape{\Tran(A,x)}$
into,
\[ \shape{\Tran(A, x)} = \vec{l}' \oplus \vec{r}' \]
Where $\shape{\vec{l}'} = y$.
That leads to two cases,

\setcounter{case}{0}
\begin{case} $\shape{\vec{l}'} \le \shape{\vec{r}}$

And in this case we showed that we needed to split up $\shape{A}$ into,
\[ \shape{A} = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
\[ \vec{l}' = \vec{b} \]
\[ \vec{r}' = \vec{c} \oplus \vec{a} \]
\[ \vec{r} = \vec{b} \oplus \vec{c} \]
\[ \vec{l} = \vec{a} \]

So that,
\[ \shape{\Tran(A, x)} = \vec{b} \oplus \vec{c} \oplus \vec{a} \]
\[ \shape{\Tran(\Tran(A, x), y)} = \vec{c} \oplus \vec{a} \oplus \vec{b} \]
And note that,
\[ x = \shape{\vec{a}} \]
\[ y = \shape{\vec{b}} \]

And we showed that the composit $\Tran_z = \Tran_y \circ \Tran_x$ was such
that,
\[ z = \shape{\vec{a} \oplus \vec{b}} \]

Consider that $\phi(\Tran_z) = z$, and so,
\begin{align*}
  z
  &= \shape{\vec{a} \oplus \vec{b}} \\
  &= \shape{\vec{a}} + \shape{\vec{b}} \\
  &= x + y \\
  &= y + x \\
  z &\equiv y + x \mod n \\
  \phi(\Tran_z) &= \phi(\Tran_y) + \phi(\Tran_x) \\
  \phi(\Tran_y \circ \Tran_x) &= \phi(\Tran_y) + \phi(\Tran_x)
\end{align*}
So in this case the isomorphism works.
\end{case}

\begin{case} $\shape{\vec{l}'} > \shape{\vec{r}}$

And in this case we showed that we needed to split up $\shape{A}$ into,
\[ \shape{A} = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
\[ \vec{l}' = \vec{c} \oplus \vec{a} \]
\[ \vec{r}' = \vec{b} \]
\[ \vec{r} = \vec{c} \]
\[ \vec{l} = \vec{a} \oplus \vec{b} \]

So that,
\[ \shape{\Tran(A, x)} = \vec{c} \oplus \vec{a} \oplus \vec{b} \]
\[ \shape{\Tran(\Tran(A, x), y)} = \vec{b} \oplus \vec{c} \oplus \vec{a} \]
And note that,
\[ x = \shape{\vec{a} \oplus \vec{b}} \]
\[ y = \shape{\vec{c} \oplus \vec{a}} \]

And we showed that the composit $\Tran_z = \Tran_y \circ \Tran_x$ was such
that,
\[ z = \shape{\vec{a}} \]

Consider that $\phi(\Tran_z) = z$, and so,
\begin{align*}
  z
  &= \shape{\vec{a}} \\
  &\equiv \shape{\vec{a}} + n \mod n \\
  &\equiv \shape{\vec{a}} + \shape{\vec{a} \oplus \vec{b} \oplus \vec{c}} \mod n \\
  &\equiv \shape{\vec{a} \oplus \vec{a} \oplus \vec{b} \oplus \vec{c}} \mod n \\
  &\equiv \shape{\vec{c} \oplus \vec{a} \oplus \vec{a} \oplus \vec{b}} \mod n \\
  &\equiv \shape{\vec{c} \oplus \vec{a}} + \shape{\vec{a} \oplus \vec{b}} \mod n \\
  &\equiv y + x \mod n \\
  \phi(\Tran_z) &= \phi(\Tran_y) + \phi(\Tran_x) \\
  \phi(\Tran_y \circ \Tran_x) &= \phi(\Tran_y) + \phi(\Tran_x)
\end{align*}
So also in this case the isomorphism works.
\end{case}

So, in both cases $\phi$ is a valid isomorphism between $(\mathbb{T}_n, \circ)$
and $(\mathbb{Z}_n, +)$.
\end{ppart}
\end{proof}

Well what does that mean? I'm so glad you asked... weirdo who doesn't know what
an isomorphism is but is several chapters into a hobby math book. It means that...

\begin{corollary}[Stacked Transpose]
For any multimatrix $A$ whose shape is $n$ long (an $n$ dimensional multimatrix
in other words), it is always true that
\[ \Tran(\Tran(A, x),y) = \Tran(A, \remainder(x+y,n)) \]
\end{corollary}

\begin{example}
Let's take a look at a concrete example. Say we have a
multimatrix $A$ who's shape is $\left<11, 3, 7, 5\right>$. 
Then see that,
\[ \shape{\Tran(A, 2)} = \left<7, 5, 11, 3\right> \]
And applying another transpose,
\[ \shape{\Tran(\Tran(A, 2), 3)} = \left<3, 7, 5, 11\right> \]
Notice that our corollary says that this double transpose should be
equal to the transpose $\Tran(A, \remainder(2+3, 4))$ which would
be $\Tran(A, 1)$ and indeed it is:
\begin{align*}
  \shape{\Tran(A, 1)}
  &= \left<3, 7, 5, 11\right> \\
  &= \shape{\Tran(\Tran(A, 2), 3)}
\end{align*}
\end{example}

Another useful corollary from Theorem \ref{tran_int_iso} shows you how to
undo transposition operations with other transposition operations.

\begin{corollary}[Inverse Transpose]
Let $A$ be a multimatrix such that $\shape{\shape{A}} = n$. And let
$x$ be some integer. Then, the inverse of $\Tran_x$ is
$\Tran_{-x}$. That is,
\[ \Tran(\Tran(A, x), -x) = X \]
\end{corollary}

\begin{example}
Say you have a multimatrix, $A$, of shape $\left<2,7,1,8,3\right>$.
Let's apply a transpose to it, $\Tran_3$, and we'll get the shape,
\[ \shape{\Tran(A, 3)} = \left<8,3,2,7,1\right> \]
Now, to get back to $A$ we can apply another transpose $\Tran_{-3}$ according
to the above corollary.
Our extended definition of transposition (Def. \ref{tran_ext})
tells us that this is equal to $\Tran_{\remainder(-3, 5)} = \Tran_{2}$.
So the inverse of $\Tran_3$ is $\Tran_2$, which we can see here:
\begin{align*}
  \shape{\Tran(\Tran(A, 3), 2)}
  &= \left<2,7,1,8,3\right> \\
  &= \shape{A}
\end{align*}
\end{example}

Now that you understand the basics of transposition, let's take a look
at some theorems involving it.

%TODO: I think this is even more general.
% I think that if k = \shape{\vec{a}} mod \shape{\vec{a} \oplus \vec{b}} then
% \Tran(\Ident^n(\vec{a} \oplus \vec{b}), k) = \Ident^n(\vec{b} \oplus \vec{a})
\begin{theorem}[Identity Transpose]
Let $n$ be a positive integer and let $\vec{x}$ be a positive length
shape vector. Then for any integer $k$,
\[ \Tran(\Ident^n(\vec{x}), k\shape{\vec{x}}) = \Ident^n(\vec{x}) \]
\end{theorem}
\begin{proof}
Consider that for some ordered set of indicies
$\{\bar{x}_1, \bar{x}_2, \ldots \bar{x}_n\} = X$ such that for each
$\bar{x}_i$ it is true that $\bar{x}_i \in \Dim(\vec{x})$.
Let $X'$ be some ordered permutation of $X$, then we can reason that,
\begin{align*}
 \Ident^n(\vec{x})\left[\bigoplus X'\right]
 &= 
	\left\{
  \begin{array}{ll}
    1 & \mbox{if all } \bar{x}_i \in X' \mbox{ are equal.}\\
    0 & \mbox{otherwise}
  \end{array}
	\right.\\
 &= 
	\left\{
  \begin{array}{ll}
    1 & \mbox{if all } \bar{x}_i \in X \mbox{ are equal.}\\
    0 & \mbox{otherwise}
  \end{array}
	\right.\\
 &=\Ident^n(\vec{n})\left[\bigoplus X\right]
\end{align*}
Since for the permutation
$X' = \left\{\bar{x}_{k+1}, \bar{x}_{k+2} \ldots \bar{x}_{n},
\bar{x}_1, \bar{x}_2 \ldots \bar{x}_k\right\}$
\begin{align*}
  \Tran(\Ident^n(\vec{x}), k\shape{\vec{x}})\left[\bigoplus X'\right]
	&=
  \Ident^n(\vec{x})\left[\bigoplus X\right]\\
	&=
  \Ident^n(\vec{x})\left[\bigoplus X'\right]
\end{align*}
Therefore
$\Tran(\Ident^n(\vec{x}), k\shape{\vec{x}}) = \Ident^n(\vec{x})$
\end{proof}

The above theorem is useful whenever you come accross a transpose of a
multimatrix identity.

Another useful theorem for whenever you need to change the order of
multimatrix multiplications is this one,

\begin{theorem}[Transposition of Multimatrix Multiplication]
\label{mm_tran_mult}
Let $\shape{A} = \vec{a} \oplus \vec{c}$ and $\shape{B} = \vec{c} \oplus \vec{b}$ then
\[
 \Tran(A \mmult{\shape{\vec{c}}} B, \shape{\vec{a}})
 =  \Tran(B, \shape{\vec{c}}) \mmult{\shape{\vec{c}}} \Tran(A, \shape{\vec{a}})
\]
\end{theorem}
\begin{proof}
Let $\bar{a} \in \Dim(\vec{a})$ and $\bar{b} \in \Dim(\vec{b})$.
Then,
\begin{align*}
	\Tran(A \mmult{\shape{\vec{c}}} B, \shape{\vec{a}})[\bar{b} \oplus \bar{a}]
	&=
	\left(A \mmult{\shape{\vec{c}}} B \right)[\bar{a} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	B[\bar{c} \oplus \bar{b}] A[\bar{a} \oplus \bar{c}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	\Tran(B, \shape{\vec{c}})[\bar{b} \oplus \bar{c}]
	\Tran(A, \shape{\vec{a}})[\bar{c} \oplus \bar{a}] \\
	&=
	\left(\Tran(B, \shape{\vec{c}}) \mmult{\shape{\vec{c}}} \Tran(A, \shape{\vec{a}}) \right)
	[\bar{b} \oplus \bar{a}]
\end{align*}
\end{proof}

TODO: exposition here

\begin{theorem}[Communicativity of Scalar Multiplication and Transposition]
Let $s$ be a scalar, $x$ be in $\mathbb{N} \cup \{0\}$, and $A$ be
a multimatrix on which $\Tran(A, x)$ is defined. Then,
\[ s\Tran(A, x) = \Tran(sA, x) \]
\end{theorem}
\begin{proof}
Let $\shape{A} = \vec{x} \oplus \vec{a}$ where $\shape{\vec{x}} = x$. Then
let $\bar{x} \in \Dim(\vec{x})$ and $\bar{a} \in \Dim(\vec{a})$. So,
\begin{align*}
  (s\Tran(A, x))[\bar{a} \oplus \bar{x}]
  &= s \Tran(A, x)[\bar{a} \oplus \bar{x}] \\
  &= s A[\bar{x} \oplus \bar{a}] \\
  &= (s A)[\bar{x} \oplus \bar{a}] \\
  &= \Tran(sA, x)[\bar{a} \oplus \bar{x}]
\end{align*}
Therefore $s\Tran(A, x) = \Tran(sA, x)$.
\end{proof}

TODO: exposition here

\begin{theorem}[Communicativity of Addition and Transposition]
\label{com_add_tran_thm}
Let $x$ be in $\mathbb{N} \cup \{0\}$ and let $A$ and $B$ be
multimatricies such that $\shape{A} = \shape{B}$ and $\Tran(A, x)$ is defined.
Then,
\[ \Tran(A, x) + \Tran(B, x) = \Tran(A+B, x) \]
\end{theorem}
\begin{proof}
Left as an exercise for the reader (Exercise \ref{com_add_tran_ex}).
\end{proof}

\section{Exercises}

\begin{exercise}
\label{eq_add_sub_ex}
Prove Theorem \ref{eq_add_sub_thm} which is that,
\[ A - B = A + (-1)B \]
\end{exercise}

\begin{exercise}
\label{com_add_tran_ex}
Prove Theorem \ref{com_add_tran_thm} which is that,
\[ \Tran(A, x) + \Tran(B, x) = \Tran(A+B, x) \]
\end{exercise}

\begin{exercise}
Let $A$ be a multimatrix such that $\shape{A} = <1,2,3,4,5>$,
Find the shape of the following trasnposes:
\begin{enumerate}
\item $\shape{\Tran(A, 3)} = ?$
\item $\shape{\Tran(A, 4)} = ?$
\item $\shape{\Tran(A, 5)} = ?$
\item $\shape{\Tran(A, 0)} = ?$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $M$ be any multimatrix.
Find the value, in terms of $M$, of the following equations.
For example $\Tran(M, 1)[2,3,4,5,1] = M[1,2,3,4,5]$
\begin{enumerate}
\item $\Tran(M, 3)[1,2,3,4] = ?$
\item $\Tran(M, 0)[6,5,3] = ?$
\item $\Tran(M, 2)[1,1,1] = ?$
\end{enumerate}
\end{exercise}

\chapter{Generic Derivative Rules}

\begin{displayquote}
``It goes without saying that all writers owe a debt to their teachers... who
naturally bear all responsibility for any errors contained herein...''
- Paul Rentel, 'Manifolds, Tensors, and Forms'
\end{displayquote}

\begin{theorem}[Transpose Derivative]
\label{tran_derivative}
$\shape{X} = \vec{l} \oplus \vec{r}$
\[
 \frac{d\Tran(X, \shape{\vec{l}})}{dX} =
 \Tran(\Ident^2(\vec{r}) \mmult{0} \Ident^2(\vec{l}), \shape{\vec{r}})
\]
\end{theorem}
\begin{proof}
\begin{align*}
 \frac{d\Tran(X, \shape{\vec{l}})}{dX}
 [\bar{r_1} \oplus \bar{l_1} \oplus \bar{l_2} \oplus \bar{r_2}]
 &= \delta_{\bar{r_1}\bar{r_2}} \delta_{\bar{l_1}\bar{l_2}} \\
 &= \delta_{\bar{r_2}\bar{r_1}} \delta_{\bar{l_1}\bar{l_2}} \\
 &=
  \Ident^2(\shape{\vec{r}})[\bar{r_2}\oplus\bar{r_1}]
  \Ident^2(\shape{\vec{l}})[\bar{l_1}\oplus\bar{l_2}] \\
 &=
   \left(
    \Ident^2(\shape{\vec{r}})
    \mmult{0}
    \Ident^2(\shape{\vec{l}})
  \right)
  [\bar{r_2}\oplus\bar{r_1}\oplus\bar{l_1}\oplus\bar{l_2}] \\
 &=
  \Tran(
    \Ident^2(\shape{\vec{r}})
    \mmult{0}
    \Ident^2(\shape{\vec{l}})
  , \shape{\vec{r}}) 
 [\bar{r_1} \oplus \bar{l_1} \oplus \bar{l_2} \oplus \bar{r_2}]
\end{align*}
\end{proof}

\begin{theorem}[Right Multiplicative Derivative]
\label{right_mult_derivative}
Let $A$ and $X$ be multimatricies such that $A \mmult{n} X$ is
defined. Then,
\[ \frac{d(A \mmult{n} X)}{dX} = A \mmult{n} \Ident^2(\shape{X}) \]
\end{theorem}
\begin{proof}
Let $\shape{A} = \vec{a} \oplus \vec{n}$ and $\shape{X} = \vec{n} \oplus \vec{x}$
where $\shape{\vec{n}} = n$ which is equivalent to saying $A \mmult{n} X$ is defined.
Also let $\bar{a} \in \Dim(\vec{a})$, $\bar{n} \in \Dim(\vec{n})$,
$\bar{x}_1 \in \Dim(\vec{x})$ and $\bar{x}_2 \in \Dim(\vec{x})$.
Then, starting with the definition of the derivative,
\begin{align*}
  \frac{d(A\mmult{n}X)}{dX}[\bar{a} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2]
  &=
  \frac{
    \partial (A \mmult{n} X)[\bar{a} \oplus \bar{x}_1]
  }{
    \partial X[\bar{n} \oplus \bar{x}_2]
  } \\
  &=
  \frac{
    \partial \left(
      \sum_{\forall \bar{m} \in \Dim(\vec{n})}
        A[\bar{a} \oplus \bar{m}]
        X[\bar{m} \oplus \bar{x}_1]
    \right)
  }{
    \partial X[\bar{n} \oplus \bar{x}_2]
  } \\
  &=
  \sum_{\forall \bar{m} \in \Dim(\vec{n})}
    \frac{
      \partial 
      A[\bar{a} \oplus \bar{m}]
      X[\bar{m} \oplus \bar{x}_1]
    }{
      \partial X[\bar{n} \oplus \bar{x}_2]
    } \\
  &=
  \left(
    \sum_{\forall \bar{m} \ne \bar{n}} 0
  \right)
  +
  \left(
    \sum_{\forall \bar{m} = \bar{n}}
    \frac{
      \partial 
      A[\bar{a} \oplus \bar{m}]
      X[\bar{m} \oplus \bar{x}_1]
    }{
      \partial X[\bar{n} \oplus \bar{x}_2]
    }
  \right) \\
  &=
  \frac{
    \partial 
    A[\bar{a} \oplus \bar{n}]
    X[\bar{n} \oplus \bar{x}_1]
  }{
    \partial X[\bar{n} \oplus \bar{x}_2]
  } \\
  &= A[\bar{a} \oplus \bar{n}] \delta_{\bar{x}_1 \bar{x}_2}
\end{align*}
Then, note that,
\begin{align*}
  A \mmult{n} \Ident^2(\shape{X})[\bar{a} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2]
  &=
  \sum_{\forall \bar{m} \in \Dim(\vec{n})}
  A[\bar{a} \oplus \bar{m}]
  \Ident^2(\shape{X})[\bar{m} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2] \\
  &=
  \sum_{\forall \bar{m} \in \Dim(\vec{n})}
  A[\bar{a} \oplus \bar{m}]
  \delta_{\bar{m} \bar{n}}
  \delta_{\bar{x}_1 \bar{x}_2} \\
  &=
  \left(
    \sum_{\forall \bar{m} \ne \bar{n}} 0
  \right)
  +
  \left(
    \sum_{\forall \bar{m} = \bar{n}}
    A[\bar{a} \oplus \bar{m}]
    \delta_{\bar{m} \bar{n}}
    \delta_{\bar{x}_1 \bar{x}_2}
  \right) \\
  &=
  A[\bar{a} \oplus \bar{n}]
  \delta_{\bar{n} \bar{n}}
  \delta_{\bar{x}_1 \bar{x}_2} \\
  &=
  A[\bar{a} \oplus \bar{n}]
  \delta_{\bar{x}_1 \bar{x}_2}
\end{align*}
So, 
\[
  \frac{d(A\mmult{n}X)}{dX}[\bar{a} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2]
  =
  A \mmult{n} \Ident^2(\shape{X})[\bar{a} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2]
\]
For all $\bar{a}$, $\bar{x}_1$, $\bar{n}$ and $\bar{x}_2$. Therefore,
\[
  \frac{d(A\mmult{n}X)}{dX}
  =
  A \mmult{n} \Ident^2(\shape{X})
\]
\end{proof}

\begin{landscape}
\begin{theorem}[Left Multiplicative Derivative]
\label{left_mult_derivative}
If $\shape{X} = \vec{x} \oplus \vec{n}$ and $\shape{A} = \vec{n} \oplus \vec{a}$
and $n = \shape{\vec{n}}$ and $a = \shape{\vec{a}}$ then,
\[ \frac{d(X \mmult{n} A)}{dX} = \Ident^2(\shape{X \mmult{n} A}) \mmult{a} \Tran(A, n) \]
\end{theorem}
\begin{proof}
Let $\shape{X} = \vec{x} \oplus \vec{n}$ and $\shape{A} = \vec{n} \oplus \vec{a}$. Then,
for all
$\bar{x}_1, \bar{x}_2 \in \Dim(\vec{x}), \bar{a}_1 \in \Dim(\vec{a}),
\bar{n}_1 \in \Dim(\vec{n}):$
\begin{align*}
	\frac{d(X \mmult{n} A)}{dX}
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	&= 
	\frac{
		\partial (X \mmult{n} A) [\bar{x}_1 \oplus \bar{a}_1]
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\frac{
		\partial \left(
			\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
			X[\bar{x}_1 \oplus \bar{n}_2] A[\bar{n}_2 \oplus \bar{a}_1]
		\right)
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
	\frac{
		\partial \left(
			X[\bar{x}_1 \oplus \bar{n}_2] A[\bar{n}_2 \oplus \bar{a}_1]
		\right)
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
	\delta_{\bar{x}_1\bar{x}_2}
	\delta_{\bar{n}_1\bar{n}_2}
	A[\bar{n}_2 \oplus \bar{a}_1] \\
	&= 
	\delta_{\bar{x}_1\bar{x}_2}
	A[\bar{n}_1 \oplus \bar{a}_1]
\end{align*}
We can also show that,
\begin{align*}
	\left(\Ident^2(\shape{X \mmult{n} A}) \mmult{a} \Tran(A, n)\right)
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a}_1)}
	\Ident^2(\shape{X \mmult{n} A})[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{a}_2]
	\Tran(A, n)[\bar{a}_2 \oplus \bar{n}_1] \\
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a})}
	\delta_{\bar{x}_1 \bar{x}_2} \delta_{\bar{a}_1 \bar{a}_2}
	\Tran(A, n)[\bar{a}_2 \oplus \bar{n}_1] \\
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a})}
	\delta_{\bar{x}_1 \bar{x}_2} \delta_{\bar{a}_1 \bar{a}_2}
	A[\bar{n}_1 \oplus \bar{a}_2] \\
	&=
	\delta_{\bar{x}_1 \bar{x}_2}
	A[\bar{n}_1 \oplus \bar{a}_1]
\end{align*}
Then since,
\[
	\frac{d(X \mmult{n} A)}{dX}
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	=
	\left(\Ident^2(\shape{X \mmult{n} A}) \mmult{a} \Tran(A, n)\right)
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
\]
we have shown that,
\[
	\frac{d(X \mmult{n} A)}{dX}
	=
	\Ident^2(\shape{X \mmult{n} A}) \mmult{a} \Tran(A, n)
\]
\end{proof}
\end{landscape}


\begin{theorem}[Addition Rule]
\label{addition_drule}
\[ \frac{d(F(X) + G(X))}{dX} = \frac{dF(X)}{dX} + \frac{dG(X)}{dX} \]
\end{theorem}
\begin{proof}
Let $\shape{F(X)} = \shape{G(X)} = \vec{f}$ and $\shape{X} = \vec{x}$. Then,
$\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):$
\begin{align*}
	\frac{d(F(X) + G(X))}{dX}[\bar{f} \oplus \bar{x}]
	&=
	\frac{\partial (F(X) + G(X))[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial (F(X)[\bar{f}] + G(X)[\bar{f}])}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial F(X)[\bar{f}]}{\partial X[\bar{x}]} +
	\frac{\partial G(X)[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{dF(X)}{dX}[\bar{f} \oplus \bar{x}]+
	\frac{dG(X)}{dX}[\bar{f} \oplus \bar{x}]
\end{align*}
So $d(F(X) + G(X))/dX = dF(X)/dX + dG(X)/dX$
\end{proof}

A rule for derivatives involving scalar multiplication also seems generally
useful. Of course, by the Theorem \ref{s_mm_mult_equiv}, this is just a very specific
case of Theorem \ref{right_mult_derivative} but the proof is simple and worth
the additional ink, I think,

\begin{theorem}[Scalar Multiplication Rule]
\label{scalar_mult_drule}
\[ \frac{d(sF(X))}{dX} = s\frac{dF(X)}{dX} \]
\end{theorem}
\begin{proof}
Let $\shape{F(X)} = \vec{f}$ and $\shape{X} = \vec{x}$. Then
$\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):$
\begin{align*}
	\frac{d(sF(X))}{dX}[\bar{f} \oplus \bar{x}]
	&=
	\frac{\partial (sF(X))[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial (s)(F(X)[\bar{f}])}{\partial X[\bar{x}]} \\
	&=
	s\frac{\partial F(X)[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	s\frac{dF(X)}{dX}[\bar{f} \oplus \bar{x}]
\end{align*}
Therefore $d(sF(X))/dX = s(dF(X)/dX)$
\end{proof}

\begin{landscape}
\begin{theorem}[Multiplication Rule]
\label{multiplication_rule}
Let $\shape{F} = \vec{f} \oplus \vec{c}$, $\shape{G} = \vec{c} \oplus \vec{g}$,
and $\shape{X} = \vec{x}$. Then,
\begin{align*}
 \frac{d\left(F(X) \mmult{\shape{\vec{c}}} G(X)\right)}{dX} =
 F(X) \mmult{\shape{\vec{c}}} \frac{dG(X)}{dX} +
 \Tran\left(
   \Tran(G(X), \shape{\vec{c}})
     \mmult{\shape{\vec{c}}}
   \Tran\left(\frac{dF(X)}{dX}, \shape{\vec{f}}\right),
   \shape{\vec{g} \oplus \vec{x}}
 \right)
\end{align*}
\end{theorem}
\begin{proof}
\[
 \forall
  \bar{f} \in \Dim(\vec{f}),
  \bar{g} \in \Dim(\vec{g}),
  \bar{x} \in \Dim(\vec{x})
 :
\]
\begin{align*}
 \frac{d\left(F(X) \mmult{\shape{\vec{c}}} G(X)\right)}{dX}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
 &= \frac{
       \partial \left(F(X) \mmult{\shape{\vec{c}}} G(X)\right)[\bar{f} \oplus \bar{g}]
    }{
       \partial X[\bar{x}]
    } \\
 &= \frac{
       \partial \left(
        \sum_{\forall \bar{c} \in \Dim(\vec{c})}
         F(X)[\bar{f} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{g}]
      \right)
    }{
       \partial X[\bar{x}]
    } \\
 &= \sum_{\forall \bar{c} \in \Dim(\vec{c})}
    \frac{
      \partial F(X)[\bar{f} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{g}]
    }{
      \partial X[\bar{x}]
    }\\
 &= \sum_{\forall \bar{c} \in \Dim(\vec{c})}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]}
    +
    \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
   \left(
    \sum_{\forall \bar{c}}
      F(X)[\bar{f} \oplus \bar{c}]
      \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]}
   \right)
   +
   \left(
    \sum_{\forall \bar{c}}
      \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
      G(X)[\bar{c} \oplus \bar{g}]
   \right) \\
 &=
    l + r
\end{align*}
Which each become,
\begin{align*}
 l
 &=
  \sum_{\forall \bar{c}}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]} \\
 &=
  \sum_{\forall \bar{c}}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{dG(X)}{dX}[\bar{c} \oplus \bar{g} \oplus \bar{x}] \\
 &=
   \left( F(X) \mmult{\shape{\vec{c}}} \frac{dG(X)}{dX} \right)
   [\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{align*}
\begin{align*}
 r
 &=
  \sum_{\forall \bar{c}}
    \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
  \sum_{\forall \bar{c}}
    \frac{dF(X)}{dX}[\bar{f} \oplus \bar{c} \oplus \bar{x}]
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
  \sum_{\forall \bar{c}}
    \Tran\left(\frac{dF(X)}{dX}, \shape{\vec{f}}\right)
      [\bar{c} \oplus \bar{x} \oplus \bar{f}]
    \Tran(G(X), \shape{\vec{c}})[\bar{g} \oplus \bar{c}] \\
 &=
  \sum_{\forall \bar{c}}
    \Tran(G(X), \shape{\vec{c}})[\bar{g} \oplus \bar{c}]
    \Tran\left(\frac{dF(X)}{dX}, \shape{\vec{f}}\right)
      [\bar{c} \oplus \bar{x} \oplus \bar{f}] \\
 &=
  \left(
    \Tran(G(X), \shape{\vec{c}}) \mmult{\shape{\vec{c}}}
    \Tran\left(\frac{dF(X)}{dX}, \shape{\vec{f}}\right)
  \right)[\bar{g} \oplus \bar{x} \oplus \bar{f}] \\
 &= 
  \Tran\left(
    \Tran(G(X), \shape{\vec{c}}) \mmult{\shape{\vec{c}}}
    \Tran\left(\frac{dF(X)}{dX}, \shape{\vec{f}}\right)
    , \shape{\vec{g} \oplus \vec{x}}
  \right)[\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{align*}
Therefore 
\begin{alignat*}{3}
 \frac{d\left(F(X) \mmult{\shape{\vec{c}}} G(X)\right)}{dX}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
  &=&&
  \left( F(X) \mmult{\shape{\vec{c}}} \frac{dG(X)}{dX} \right)
  [\bar{f} \oplus \bar{g} \oplus \bar{x}] \\
  &&&+
  \Tran\left(
    \Tran(G(X), \shape{\vec{c}}) \mmult{\shape{\vec{c}}}
    \Tran\left(\frac{dF(X)}{dX}, \shape{\vec{f}}\right)
    , \shape{\vec{g} \oplus \vec{x}}
  \right)[\bar{f} \oplus \bar{g} \oplus \bar{x}] \\
  &=&&
  \left\{
    \begin{array}{l}
      \left( F(X) \mmult{\shape{\vec{c}}} \frac{dG(X)}{dX} \right) \\
      +
      \Tran\left(
        \Tran(G(X), \shape{\vec{c}}) \mmult{\shape{\vec{c}}}
        \Tran\left(\frac{dF(X)}{dX}, \shape{\vec{f}}\right)
        , \shape{\vec{g} \oplus \vec{x}}
      \right)
    \end{array}
  \right\}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{alignat*}
\end{proof}
\end{landscape}

\begin{theorem}[Hadamard Multiplication Rule]
Let $F(X)$ and $G(X)$ be functions of $X$ where
$\shape{F(X)} = \vec{a} \oplus \vec{c}$ and $\shape{G(X)} = \vec{c} \oplus \vec{b}$
then,
\[
  \frac{d(F(X) \dmult{\shape{\vec{c}}} G(X))}{dX} =
    F(X) \dmult{\shape{\vec{c}}} \frac{dG}{dX} +
    \Tran\left(\Tran\left(\frac{dF}{dX}, \shape{\shape{F}}\right) \dmult{\shape{\vec{c}}} G(X), \shape{\shape{X}}\right)
\]
\end{theorem}
\begin{landscape}
\begin{proof}
For $\vec{x} = \shape{X}$ and for $\bar{a} \in \Dim(\vec{a})$ etc,
\begin{align*}
\frac{d(F(X) \dmult{\shape{\vec{c}}} G(X))}{dX}[\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}]
  &= \frac{\partial (F(X) \dmult{\shape{\vec{c}}} G(X))[\bar{a} \oplus \bar{c} \oplus \bar{b}]}{\partial X[\bar{x}]} \\
  &= \frac{\partial (F(X)[\bar{a} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{a}])}{\partial X[\bar{x}]} \\
  &= \frac{\partial F(X)[\bar{a} \oplus \bar{c}]}{\partial X[\bar{x}]} G(X)[\bar{c} \oplus \bar{b}]
    + F(X)[\bar{a} \oplus \bar{c}] \frac{\partial G(X)[\bar{c} \oplus \bar{b}]}{\partial X[\bar{x}]} \\
  &= \frac{dF}{dX}[\bar{a} \oplus \bar{c} \oplus \bar{x}] G(X)[\bar{c} \oplus \bar{b}]
    + F(X)[\bar{a} \oplus \bar{c}] \frac{dG}{dX}[\bar{c} \oplus \bar{b} \oplus \bar{x}] \\
  &= \frac{dF}{dX}[\bar{a} \oplus \bar{c} \oplus \bar{x}] G(X)[\bar{c} \oplus \bar{b}]
    + \left(F(X) \dmult{\shape{\vec{c}}} \frac{dG}{dX}\right)[\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}] \\
  &= \Tran\left(\frac{dF}{dX}, \shape{\shape{F}}\right)[\bar{x} \oplus \bar{a} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{b}]
    + \left(F(X) \dmult{\shape{\vec{c}}} \frac{dG}{dX}\right)[\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}] \\
  &= \left(\Tran\left(\frac{dF}{dX}, \shape{\shape{F}}\right) \dmult{\shape{\vec{c}}} G(X)\right) [\bar{x} \oplus \bar{a} \oplus \bar{c} \oplus \bar{b}]
    + \left(F(X) \dmult{\shape{\vec{c}}} \frac{dG}{dX}\right)[\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}] \\
  &= \Tran\left(\Tran\left(\frac{dF}{dX}, \shape{\shape{F}}\right) \dmult{\shape{\vec{c}}} G(X), \shape{\shape{X}}\right) [\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}]
    + \left(F(X) \dmult{\shape{\vec{c}}} \frac{dG}{dX}\right)[\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}] \\
  &= \left(F(X) \dmult{\shape{\vec{c}}} \frac{dG}{dX}\right)[\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}]
    + \Tran\left(\Tran\left(\frac{dF}{dX}, \shape{\shape{F}}\right) \dmult{\shape{\vec{c}}} G(X), \shape{\shape{X}}\right) [\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}] \\
  &= \left\{ F(X) \dmult{\shape{\vec{c}}} \frac{dG}{dX}
    + \Tran\left(\Tran\left(\frac{dF}{dX}, \shape{\shape{F}}\right) \dmult{\shape{\vec{c}}} G(X), \shape{\shape{X}}\right) \right\}[\bar{a} \oplus \bar{c} \oplus \bar{b} \oplus \bar{x}]
\end{align*}
\end{proof}
\end{landscape}

\begin{theorem}[Chain Rule]
\label{mmm_chain_rule}
Let $F(G)$ be a function $\mathbb{R}^{\vec{g}} \rightarrow \mathbb{R}^{\vec{f}}$
and $G(X)$ be a function $\mathbb{R}^{\vec{x}} \rightarrow \mathbb{R}^{\vec{g}}$.
Then,
\[
\frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{\shape{\vec{g}}} \frac{dG}{dX}
\]
\end{theorem}
\begin{proof}
By the definition of a derivative (\ref{mm_derivative}),
\[
\forall \bar{f} \in \Dim(\vec{f}), \bar{g} \in \Dim(\vec{g}):
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
= \frac{\partial F(G)[\bar{f}]}{\partial G[\bar{g}]}
\]
\[
\forall \bar{g} \in \Dim(\vec{g}), \bar{x} \in \Dim(\vec{x}):
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
= \frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
By the scalar total derivative rule \cite{wiki:totalderiv},
\[
\frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{\partial F(G)[\bar{f}]}{\partial G[\bar{g}]}
\frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
Substituting in our above definitions this yields,
\[
\frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
And again, by the definition of a derivative (\ref{mm_derivative}),
\[
\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):
\frac{dF(G(X))}{dX}[\bar{f} \oplus \bar{x}]
= \frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
\]
Therefore,
\[
\frac{dF(G(X))}{dX}[\bar{f} \oplus \bar{x}]
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
Which, taking a look at the definition of multidimensional matrix multiplication
(\ref{mm_mult}), tells us that,
\[
\frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{\shape{\vec{g}}} \frac{dG}{dX}
\]
\end{proof}

\chapter{Machine Learning Specific Derivatives}

\begin{displayquote}
``I'm not saying Machine Learning is a portal to a demon universe, I'm just saying
that some doors are best left unopened.'' - James Mickens, AKA `Galactic Viceroy of Research Excellence', USENIX Security 2018
\end{displayquote}

\begin{definition}[Sum]
Let $\Sum(X)$ be the function $\Sum : \mathbb{R}^{\vec{x}} \to \mathbb{R}$
such that \[ \Sum(X) = \sum_{\forall \bar{x} \in \Dim(\vec{x})} X[\bar{x}] \]
\end{definition}

\begin{theorem}[Derivative of Sum]
Let $X$ be any multimatrix, then,
\[\frac{d\Sum(X)}{dX} = 1^{\shape{X}}\]
\end{theorem}
\begin{proof}
Let $\shape{X} = \vec{x}$ and $\bar{x} = \Dim(\vec{x})$. Then,
\begin{align*}
  \frac{d\Sum(X)}{dX}[\bar{x}]
  &= \frac{\partial \Sum(X)}{\partial X[\bar{x}]} \\
  &= \frac{
    \partial \left(
      \sum_{\forall \bar{y} \in \Dim(\vec{x})} X[\bar{y}]
    \right)
  }{
    \partial X[\bar{x}]
  } \\
  &= \sum_{\forall \bar{y} \in \Dim(\vec{x})}
  \frac{
    \partial X[\bar{y}]
  }{
    \partial X[\bar{x}]
  } \\
  &= \left(
    \sum_{\forall \bar{y} \ne \bar{x}} 0
  \right)
  +
  \left(
    \sum_{\forall \bar{y} = \bar{x}}
    \frac{\partial X[\bar{y}]}{\partial X[\bar{x}]}
  \right) \\
  &= \frac{\partial X[\bar{x}]}{\partial X[\bar{x}]} \\
  &= 1 \\
  &= 1^{\vec{x}}[\bar{x}]
\end{align*}
So,
\[\frac{d\Sum(X)}{dX} = 1^{\shape{X}}\]
\end{proof}

\begin{definition}[Sum of Squares]
Let $\SoS(X)$ be the function $\SoS : \mathbb{R}^{\vec{x}} \to \mathbb{R}$
such that 
\[ \SoS(X) = \sum_{\forall \bar{x} \in \Dim(\vec{x})} X[\bar{x}]^2 \]
\end{definition}

\begin{theorem}[Derivative of Sum of Squares]
Let $X$ be any multimatrix then,
\[ \frac{d \SoS(X)}{dX} = 2X \]
\end{theorem}
\begin{proof}
Let $\shape{X} = \vec{x}$. Notice that $\SoS(X) = X \mmult{\shape{\vec{x}}} X$ since,
$\shape{X \mmult{\shape{\vec{x}}} X} = \left<\right>$ and,
\[
  \left(X \mmult{\shape{\vec{x}}} X\right)[\left<\right> \oplus \left<\right>]
  =
  \sum_{\forall \bar{x} \in \Dim(\vec{x})}
    X[\left<\right> \oplus \bar{x}]
    X[\bar{x} \oplus \left<\right>]
\]
Then, by the multiplication rule (Thm. \ref{multiplication_rule}),
\begin{align*}
  \frac{d\left(X \mmult{\shape{\vec{x}}} X\right)}{dX}
  &=
  X \mmult{\shape{\vec{x}}} \frac{dX}{dX} +
  \Tran\left(
    \Tran(X, \shape{\vec{x}})
      \mmult{\shape{\vec{x}}}
    \Tran\left(\frac{dX}{dX}, \shape{\vec{x}}\right),
    \shape{\vec{x} \oplus \vec{x}}
  \right) \\
  &=
  X \mmult{\shape{\vec{x}}} \Ident^2(\shape{X}) +
  \Tran\left(
    X
      \mmult{\shape{\vec{x}}}
    \Tran\left(\Ident^2(\shape{X}), \shape{\vec{x}}\right),
    \shape{\vec{x} \oplus \vec{x}}
  \right) \\
  &=
  X +
  \Tran\left(
    X
      \mmult{\shape{\vec{x}}}
    \Ident^2(\shape{X}),
    \shape{\vec{x} \oplus \vec{x}}
  \right) \\
  &=
  X +
  \Tran\left(
    X,
    \shape{\vec{x} \oplus \vec{x}}
  \right) \\
  &= X + X \\
  &= 2X
\end{align*}
\end{proof}

\begin{definition}[Softmax]
Let $\Softmax(X)$ be the function
$\Softmax : \mathbb{R}^{\vec{x}} \to \mathbb{R}^{\vec{x}}$
such that $\forall \bar{x} \in \Dim(\vec{x}):$
\[
  \Softmax(X)[\bar{x}] = \frac{
		\exp(X[\bar{x}])
	}{
		\sum_{\forall \bar{c} \in \Dim(\vec{x})} \exp(X[\bar{c}])
	}
\]
\end{definition}

\begin{landscape}
\begin{theorem}[Derivative of Softmax]
\label{softmax_derivative}
\begin{align*}
	\frac{d\Softmax(X)}{dX}
	&=
	\Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} \Softmax(X)
  -\Softmax(X) \mmult{0} \Softmax(X) \\
	&=
	\left(\Ident^3(\shape{X}) - \Softmax(X) \mmult{0} \Ident^2(\shape{X}) \right)
	\mmult{\shape{\shape{X}}} \Softmax(X)
\end{align*}
\end{theorem}
\begin{proof}
Notice that 
\[
	\Softmax(X) = \exp(X) \mmult{0} \left( \Sum(\exp(X)) \right)^{-1}
\]
That right hand term is a scalar so it does have a multiplicative inverse.
Next,
\begin{alignat*}{3}
	\frac{d\Softmax(X)}{dX}
	&=&&
	\frac{d \left( \exp(X) \mmult{0} \left( \Sum(\exp(X)) \right)^{-1} \right)}{dX} \\
	&=&&
	\exp(X) \mmult{0} \frac{d \left( \Sum(\exp(X)) \right)^{-1}}{dX}
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\left( \Sum(\exp(X)) \right)^{-1}
			,0
		\right)
		\mmult{0}
		\Tran\left(
			\frac{d \exp(X)}{dX}
			,\shape{\vec{x}}
		\right)
		,\shape{\left<\right> \oplus \vec{x}}
	\right) \\
	&=&&
	\exp(X) \mmult{0} \frac{d \left( \Sum(\exp(X)) \right)^{-1}}{dX}
	\nonumber\\&&&+
	\Tran\left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Tran\left(
			\frac{d \exp(X)}{dX}
			,\shape{\vec{x}}
		\right)
		,\shape{\vec{x}}
	\right) \\
	&=&&
	\exp(X) \mmult{0} \left(
		-\left( \Sum(\exp(X)) \right)^{-2}
		\mmult{0}
		\frac{d\Sum(\exp(X))}{dX}
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Tran\left(
			\Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} \exp(X)
			,\shape{\vec{x}}
		\right)
		,\shape{\vec{x}}
	\right) \\
	&=&&
	\exp(X) \mmult{0} \left(
		-\left( \Sum(\exp(X)) \right)^{-2}
		\mmult{0}
		1^{\shape{X}}
		\mmult{\shape{\shape{X}}}
		\Ident^3(\shape{X})
		\mmult{\shape{\shape{X}}}
		\exp(X)
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\left( \Sum(\exp(X)) \right)^{-1}
			\mmult{0}
			\Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} \exp(X)
		,\shape{\vec{x}}
		\right)
		,\shape{\vec{x}}
	\right) \\
	&=&&
	-\Softmax(X) \mmult{0} \left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Ident^2(\shape{X})
		\mmult{\shape{\shape{X}}}
		\exp(X)
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} \Softmax(X)
		,\shape{\vec{x}}
		\right)
	,\shape{\vec{x}}
	\right) \\
	&=&&
	-\Softmax(X) \mmult{0} \Softmax(X)
	+
	\Tran\left(
		\Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} \Softmax(X)
		,2\shape{\vec{x}}
	\right) \\
	&=&&
	\Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} \Softmax(X)
  -\Softmax(X) \mmult{0} \Softmax(X) \\
	&=&&
	\Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} \Softmax(X)
	-\Softmax(X) \mmult{0} \Ident^2(\shape{X}) \mmult{\shape{\shape{X}}} \Softmax(X) \\
	&=&&
	\left(\Ident^3(\shape{X}) - \Softmax(X) \mmult{0} \Ident^2(\shape{X}) \right)
	\mmult{\shape{\shape{X}}} \Softmax(X)
\end{alignat*}
Let's just verify that against the Wikipedia \cite{wiki:softmax} result:
\begin{align*}
	\frac{\partial \Softmax(X)[\bar{x}_1]}{\partial X[\bar{x}_2]}
	&=
	\left\{
		\left(\Ident^3(\shape{X}) - \Softmax(X) \mmult{0} \Ident^2(\shape{X}) \right)
		\mmult{\shape{\shape{X}}} \Softmax(X)
	\right\}
	[\bar{x}_1 \oplus \bar{x}_2] \\
	&=
	\sum_{\forall \bar{x}_3}
	\left(\Ident^3(\shape{X}) - \Softmax(X) \mmult{0} \Ident^2(\shape{X}) \right)
	[\bar{x}_1 \oplus \bar{x}_2 \oplus \bar{x}_3]
	\Softmax(X)[\bar{x}_3] \\
	&=
	\sum_{\forall \bar{x}_3}
	\left(
		\delta_{\bar{x}_1 \bar{x}_2 \bar{x}_3}
		- \Softmax(X)[\bar{x}_1] \delta_{\bar{x}_2 \bar{x}_3}
	\right)
	\Softmax(X)[\bar{x}_3] \\
	&=
	(\delta_{\bar{x}_1 \bar{x}_2} - \Softmax(X)[\bar{x}_1])
	\Softmax(X)[\bar{x}_2] \\
	&= \left\{
  \begin{array}{ll}
    (1-\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    (0-\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= \left\{
  \begin{array}{ll}
    \Softmax(X)[\bar{x}_1]-\Softmax(X)[\bar{x}_1]^2
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    -\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= \left\{
  \begin{array}{ll}
    (1-\Softmax(X)[\bar{x}_2])\Softmax(X)[\bar{x}_2]
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    (0-\Softmax(X)[\bar{x}_2])\Softmax(X)[\bar{x}_1]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= (\delta_{\bar{x}_1 \bar{x}_2} - \Softmax(X)[\bar{x}_2])
	\Softmax(X)[\bar{x}_1]
\end{align*}
And it looks like we're good. I wrote this proof before backreferencing the wiki
page, so that's a good sign.
\end{proof}
\end{landscape}

\begin{definition}[Feed Forward Neural Net Layer]
Feed forward neural nets are described all over the place so I won't go into too
much detail here. In short, for every layer except the input layer there is some
previous layer which feeds into current layer via a fully-connected network of
weights.

Let $L_{i-1}$ be the activations of the the previous layer and $L_i$ the activations
of the current layer. Each of these can be modeled as a multimatrix. As an the weighted
connections between them, $W_i$. Finally, there is an activation function
$a(X) : \mathbb{R}^{\shape{X}} \to \mathbb{R}^{\shape{X}}$ which is an elementwise function,
often the sigmoid function, or ReLU, or some variant thereof. It's unimportant for this
definition.

\[ L_i = a(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \]
\end{definition}

\begin{landscape}
\begin{theorem}[Derivative of FFNN Layer]
\[
  \frac{dL_i}{dW_j}
	= \left\{
  \begin{array}{ll}
		0 & \mbox{if } i < j \\
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \left(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} \Ident^2(\shape{W_i})\right)
	  & \mbox{if } i = j \\
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \Tran(W_i, \shape{\shape{L_{i-1}}})
    \mmult{\shape{\shape{L_{i-1}}}}
    \frac{d L_{i-1}}{dW_j}
		& \mbox{if } i > j
  \end{array}
  \right.
\]
\end{theorem}
\begin{proof}
\begin{ppart} When $i < j$,
$L_i$ does not depend on $W_j$, so 
$\forall \bar{x} \in \Dim(\shape{L_i}), \bar{y} \in \Dim(\shape{W_j})$
$\frac{dL_i}{dW_j} = 0$
\end{ppart}
\begin{ppart} $i = j$
Then note that $\frac{dL_{i-1}}{dW_j} = 0$ because $L_{i-1}$ does not
depend on $W_j$. So we can just use the chain rule and the right multiplicative rule to find that,
\[
  \frac{dL_i}{dW_{j=i}} =
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \left(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} \Ident^2(\shape{W_i})\right)
\]
\end{ppart}
\begin{ppart} $i > j$
In this case, $L_{i-1}$ depends on $W_j$ but $W_i$ does not. So we can use the chain rule and the
left multiplicative rule to find that,
\begin{align*}
  \frac{dL_i}{dW_{j<i}}
  &=
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \frac{d (L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i)}{dW_j} \\
  &=
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \left(
      \frac{d (L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i)}{dL_{i-1}}
      \mmult{\shape{\shape{L_{i-1}}}}
      \frac{dL_{i-1}}{dW_j}
    \right) \\
  &=
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \left(
      \left(
        \Ident^2(\shape{L_{i}})
        \mmult{\shape{\shape{W_i}}-\shape{\shape{L_{i-1}}}}
        \Tran(W_i, \shape{\shape{L_{i-1}}})
      \right)
      \mmult{\shape{\shape{L_{i-1}}}}
      \frac{d L_{i-1}}{dW_j}
    \right) \\
  &=
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \left(
      \left(
        \Ident^2(\shape{L_{i}})
        \mmult{\shape{\shape{L_i}}}
        \Tran(W_i, \shape{\shape{L_{i-1}}})
      \right)
      \mmult{\shape{\shape{L_{i-1}}}}
      \frac{d L_{i-1}}{dW_j}
    \right) \\
  &=
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \left(
      \Tran(W_i, \shape{\shape{L_{i-1}}})
      \mmult{\shape{\shape{L_{i-1}}}}
      \frac{d L_{i-1}}{dW_j}
    \right) \\
  &=
    \left(\Ident^3(\shape{L_i}) \mmult{\shape{\shape{L_i}}} a'(L_{i-1} \mmult{\shape{\shape{L_{i-1}}}} W_i) \right) 
    \mmult{\shape{\shape{L_i}}}
    \Tran(W_i, \shape{\shape{L_{i-1}}})
    \mmult{\shape{\shape{L_{i-1}}}}
    \frac{d L_{i-1}}{dW_j}
\end{align*}
\end{ppart}
\end{proof}
\end{landscape}

\chapter{Slicing and Dicing}

TODO: cheeky quote

\begin{definition}[Multimatrix Wildcard Notation]
TODO: rigorous definition

Example:
$\shape{X[\bar{a} \oplus *^{\vec{s}} \oplus \bar{b}]} = \vec{s}$
\end{definition}


\chapter{Integration}

\begin{quote}
TODO: cheeky quote
\end{quote}

I'll just throw this out there. I think the integral of a multimatrix function should
be defined like this,

\begin{definition}[Integration of Multimatrix]
If $G(X)$ is a multimatrix function of shape $\shape{G(X)} = \vec{g} \oplus \shape{X}$,
for some shape vector $\vec{g}$, then it can be integrated over $dX$ by,
\[ \shape{\int_A^B G(X) dX} = \vec{g} \]
\[
 \int_A^B G(X) dX = \lim_{n \to \infty}
 \sum_{i=1}^n
  G\left(A+\frac{i}{n}(B-A)\right)
  \mmult{\shape{\shape{X}}}
  \left(\frac{B-A}{n}\right)
\]
\end{definition}

But what does this actually mean? I'll show, below, that this actually breaks down into
a series of line integrals from over a strait line from $A$ to $B$. It will also be
shown that the integral of $dF/dX$ will return $F(X) + C$, as one would expect from an
integral.

\begin{theorem}[Line Integrals Inside Multimatrix Integral]
If $G(X)$ is an integrable multimatrix function (i.e. it has a shape
$\shape{G(X)} = \vec{g} \oplus \shape{X}$) then each output element of it's
integral is a line integral.
Specifically, for all $\bar{g}$ in $\Dim(\vec{g})$,
\[ \left(\int_A^B G(X) dX\right)[\bar{g}] = \int_C G(X)[\bar{g} \oplus *^{\shape{X}}] \mmult{\shape{\shape{X}}} dX \]
Where $C$ is the straight line from $A$ to $B$. And $\mmult{\shape{\shape{X}}} dX$ serves the same notational purpose as
$\cdot dr$ (see \cite{wiki:line_int}) because multiplication by the entire shape of $X$ is the same as a dot product.
\end{theorem}
\begin{proof}
Let us parameterize $C$ such that $C(t) = A + t(B-A)$. Then $dC/dt = B-A$ and
we can rewrite the line integral over $C$ as,
\[ \int_C G(X)[\bar{g} \oplus *^{\shape{X}}] \mmult{\shape{\shape{X}}} dX = \int_0^1 G(C(t))[\bar{g} \oplus *^{\shape{X}}] \mmult{\shape{\shape{X}}} \frac{dC}{dt} dt \]
TODO
\end{proof}

TODO:fundamental theorem

\begin{appendices}

\chapter{Solutions to the Exercises}

\solutionsection{1}

\begin{solution}
\begin{enumerate}
\item[]
\item True
\item False, $\Dim(\left<5,5,5\right>)$ contains only vectors in $\mathbb{N}^3$ 
\item False, the last value of $\left<1,2,3\right>$, $3$, is greater than it's
						 corresponding value in $\left<3,2,1\right>$, which is $1$.
\item True
\end{enumerate}
\end{solution}

\begin{solution}
\[ \shape{\frac{dF}{dX}} = \left<6,7,3,7,2\right> \]
\end{solution}

\begin{solution}
\[ \shape{\frac{df}{dX}} = \shape{X} = \left<2,3,2\right> \]
\[ \frac{df}{dX}[\bar{v}] = 1 \]
\end{solution}

\begin{solution}
\[ \shape{\frac{df}{dX}} = \shape{X} = \left<2,3,2\right> \]
\[ \frac{df}{dX}[\bar{v}] = 1 \]
\end{solution}

\solutionsection{2}

\begin{solution}
\begin{enumerate}
\item[]
\item Invalid
\item $\left<3, 3\right>$
\item Invalid
\item $\left<1, 2, 2, 1\right>$
\end{enumerate}
\end{solution}

\chapter{Derivative Rule List}

\begin{drule}
Self Derivative (Thm. \ref{self_derivative})
\[ \frac{dX}{dX} = \Ident^2(\shape{X}) \]
\end{drule}

\begin{drule}
Elementwise Derivative (Thm. \ref{elementwise_derivative})
\[ \frac{dM(X)}{dX} = \Ident^3(\shape{X}) \mmult{\shape{\shape{X}}} M'(X) \]
\end{drule}

\begin{drule}
Empty Shape Multimatrix and Scalar Domain Derivative Equivalence
(Thm. \ref{s_mm_domain_equiv})
\[ G(X) = F(X[\left<\right>]) \]
Implies,
\[ \frac{dF(x)}{dx} = \frac{dG(X)}{dX} \]
\end{drule}

\begin{drule}
Empty Shape Multimatrix and Scalar Range Derivative Equivalence
(Thm. \ref{s_mm_range_equiv})
\[ F(X)[\left<\right>] = f(X) \]
Implies,
\[ \frac{df(X)}{dX} = \frac{dF(X)}{dX} \]
\end{drule}

\begin{drule}
Transpose Derivative
(Thm. \ref{tran_derivative})
\[ \shape{X} = \vec{l} \oplus \vec{r} \]
Implies,
\[
 \frac{d\Tran(X, \shape{\vec{l}})}{dX} =
 \Tran(\Ident^2(\vec{r}) \mmult{0} \Ident^2(\vec{l}), \shape{\vec{r}})
\]
\end{drule}

\begin{drule}
Right Multiplicative Derivative
(Thm. \ref{right_mult_derivative})
\[ \frac{d(A \mmult{n} X)}{dX} = A \mmult{n} \Ident^2(\shape{X}) \]
\end{drule}

\begin{drule}
Left Multiplicative Derivative
(Thm. \ref{left_mult_derivative})
\[ \frac{d(X \mmult{n} A)}{dX} = \Ident^2(\shape{X \mmult{n} A}) \mmult{\shape{\shape{A}}-n} \Tran(A, n) \]
\end{drule}

\begin{drule}
Addition Rule
(Thm. \ref{addition_drule})
\[ \frac{d(F(X) + G(X))}{dX} = \frac{dF(X)}{dX} + \frac{dG(X)}{dX} \]
\end{drule}

\begin{drule}
Scalar Multiplication Rule
(Thm. \ref{scalar_mult_drule})
\[ \frac{d(sF(X))}{dX} = s\frac{dF(X)}{dX} \]
\end{drule}

\begin{drule}
Multiplication Rule
(Thm. \ref{multiplication_rule})

Let $\shape{F} = \vec{f} \oplus \vec{c}$, $\shape{G} = \vec{c} \oplus \vec{g}$,
and $\shape{X} = \vec{x}$. Then,
\begin{align*}
 \frac{d\left(F(X) \mmult{\shape{\vec{c}}} G(X)\right)}{dX} =&
 F(X) \mmult{\shape{\vec{c}}} \frac{dG(X)}{dX} + \\
 &\Tran\left(
   \Tran(G(X), \shape{\vec{c}})
     \mmult{\shape{\vec{c}}}
   \Tran\left(\frac{dF(X)}{dX}, \shape{\vec{f}}\right),
   \shape{\vec{g} \oplus \vec{x}}
 \right)
\end{align*}
\end{drule}

\begin{drule}
Chain Rule
(Thm. \ref{mmm_chain_rule})
\[ \frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{\shape{\shape{G}}} \frac{dG}{dX} \]
\end{drule}

\end{appendices}

\bibliography{book}
\bibliographystyle{ieeetr}

\end{document}
