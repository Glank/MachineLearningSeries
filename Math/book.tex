\documentclass[12pt]{book}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
% Use pdflscape for pdf viewers
\usepackage{pdflscape}
% Use lscape for printing as a book
%\usepackage{lscape}
\usepackage{etoolbox}
\usepackage{csquotes}
\usepackage[normalem]{ulem}
\usepackage[toc,page]{appendix}

\title{%
  Multimatrices and Their Derivatives \\
  \large An Impractical Guide to Global Conquest}
\author{Ernest Kirstein}
\date{\today}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{exercise}{Exercise}[chapter]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}[chapter]
\newtheorem{drule}{Rule}

\newtheoremstyle{ppart}{}{}{}{}{}{:}{ }{}
\theoremstyle{ppart}
\newtheorem{ppart}{Part}

\newtheoremstyle{case}{}{}{}{}{}{:}{ }{}
\theoremstyle{case}
\newtheorem{case}{Case}

\newcounter{solutionctr}
\newtheoremstyle{solution}{}{}{}{}{}{:}{\newline}{
  Exercise \refstepcounter{solutionctr}\thesolutionctr~Solution}
\theoremstyle{solution}
\newtheorem{solution}{Solution}
\newcommand{\solutionsection}[1]{
  \section{Chapter~#1 Solutions}\setcounter{solutionctr}{0}}

\newenvironment{argument}{\noindent\textit{Argument.}}{\hfill$\square$}

\AtBeginEnvironment{proof}{\setcounter{ppart}{0}}
\AtBeginEnvironment{proof}{\setcounter{case}{0}}
\AtBeginEnvironment{align}{\setcounter{equation}{0}}
\AtBeginEnvironment{alignat}{\setcounter{equation}{0}}

\DeclareMathOperator{\Dim}{Dim}
\DeclareMathOperator{\Ident}{I}
\DeclareMathOperator{\Tran}{Tran}
\DeclareMathOperator{\SoS}{\mathbb{S}}
\DeclareMathOperator{\Softmax}{Softmax}
\DeclareMathOperator{\Sum}{\Sigma}
\DeclareMathOperator{\remainder}{mod}
%\newcommand{\mmult}[1]{\stackrel{\times}{#1}}
%\newcommand{\mmult}[1]{\underset{#1}{\times}}
\newcommand{\mmult}[1]{\text{\raisebox{1ex}{$\underset{#1}{\times}$}}}
\newcommand{\dmult}[1]{\text{\raisebox{1ex}{$\underset{#1}{\,\bullet\,}$}}}

\begin{document}

\maketitle

\chapter*{Preface}
I was working my way through a hobby programming project, trying to create a so called
`deep fake' of Danny DeVito's face on Scarlett Johansson's body, and ran into a problem.
The existing notation for taking derivatives of matrix equations is terrible.
It's so deliberately obtuse that I could only conclude that some hidden cabal of
mathematicians is actively working to sabotage the rise of our glorious AI overlords.
I, for one, will not stand for such heresy.

So, I set out to invent a system of my own. Much to my surprise, some of the
results with were rather elegant. For example, I stumbled onto a
multidimensional version of the derivative chain rule (Thm. \ref{mmm_chain_rule})
that's nearly as consise as the same rule for scalars.

The ultimate goal of this notation system was to \sout{bring about the downfall
of humanity} figure out a way to take derivatives
of matrix equations without having to break apart matricies into their constituent
parts. I knew that I had succeeded when I was able to derive the derivative of the
softmax function completely in my own notation (Thm. \ref{softmax_derivative}).

Now, not knowing what else to do with the results, I've written this book
about my amature exploration into multidimensional matricies -
multimatricies - and how they can simplify derivatives of matrix equations.

\tableofcontents

\chapter{The Multimatrix and Its Derivative}

\begin{displayquote}
``If you're a bit touched, like I am, you may want to make a dedicated tool.'' -
This Old Tony, `Making Springs At Home' \cite{youtube:tony}
\end{displayquote}

\section{Motivating the Multimatrix}

To demonstrate how the problem with traditional matricies, let's take a look at a
simple matrix equation. 

Let one matrix $X$ represent the level of scientific education level of a set of
undergraduates who work in a densely packed 3 by 5 office.
Then let $B$ represent the belief each of those undergradutes has in a higher power.

As we all know, Weinersmith's hypothisis states that an undergraduate's belief in a
higher power fluctuates with respect to their scientific education in a sinusoidal
pattern (Weinersmith, 2010 \cite{weinersmith:education}).

Formally, the shape of these two matricies are defined,
$|B| = |X| = \left< 3,5 \right>$.
And each element of $B$ will be the sine of the corresponding element of
$X$. That is, \[B_{r,c} = \sin(X_{r,c})\]
Which I'll write more simply as, \[B = \sin(X)\]

Now, I would call the derivative of any element of $B$ with respect to any element
of $X$ a partial derivative of that function. Since
$\partial B_{r_1, c_1}/\partial X_{r_2, c_2}$ is defined as the rate of change of
\textit{one} undergraduate's belief in a higher power with respect to \textit{one}
undergraduate's education level - it only represents a change to part of the whole
system. 
We can calculate this partial derivative easily enough,
\[
\frac{\partial B_{r_1,c_1}}{\partial X_{r_2,c_2}} = 
\left\{
  \begin{array}{ll}
    \cos(X_{r_2,c_2})  & \mbox{if } r_1 = r_2 \mbox{ and } c_1 = c_2 \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\]
Every student's belief in a higher power changes with respect to their own
scientific education level but does not change with respect to other students'
scientific education. No mater how much I teach Jane about photosynthesis, I'm
not going to change Joe's mind about divine mysteries.

This can also be writen more simply using Kronecker deltas \cite{wiki:kronecker},
which should make one feel particularly intellegent as Kronecker deltas were used
by Einstein (and presumably someone named Kronecker) \cite{wiki:einstein}.
\[
\frac{\partial B_{r_1,c_1}}{\partial X_{r_2,c_2}} = 
\cos (X_{r_2, c_2}) \delta_{r_1 r_2} \delta_{c_1 c_2}
\]

But what about the complete deriative?
What sort of mathmatical object would represent the entire derivative
of $B$ with respect to $X$?
I need a mathematical object which perfectly
encapsulates the rate of change of \textit{all} the undergraduates' beliefs in a
higher power with respect to \textit{every} undergraduates' scientific education
and my current toolkit doesn't work!

\[\frac{dB}{dX} = \mbox{ ? }\]

If this were a function that mapped a vector to a scalar, you might say that the
complete derivate was the gradient. But in this case, our function's output is more
than a simple scalar and even our dependent variable is more complex than a vector.

By looking at the partial derivatives, we can come to some understanding of the
dimensionality of the complete derivative. Each combination of input and output
element has its own partial derivative, so it makes sense that the total size
of the complete derivative should be the product of the size of $B$ and the size
of $X$.

And so, the size of the complete derivate would be,
\[
\mbox{size}\left(\frac{dB}{dX}\right) = \mbox{size}(B) \mbox{size}(X)
\]
Perhaps we could even let this help us define the `shape' of the complete
derivative. Note that there seem to be four independent indicies to each element
of this matrix, namely $r_1$, $c_1$, $r_2$, and $c_2$.
And since a complete derivative would need to be composed of partial derivatives
it would make sense to just smush the shapes of the input matrix and the output
matrix together,
\begin{align*}
\left|\frac{dB}{dX}\right| &= |B| \oplus |X| \\
 &= \left< 3, 5 \right> \oplus \left< 3, 5 \right> \\
 &= \left< 3, 5, 3, 5 \right>
\end{align*}
Where $\oplus$ is the concatenate operator.

The ordering here is arbitrary. There is no reason why the dimensionality of
of the independent variable \textit{needs} to be concatenated to the right of the
dimensionality of the output. What's important is that the dimensionality of
both output and indepenedent variable are preserved. Let us declare this to
be our convention. I've typeset it in \LaTeX ~therefore it can no longer be
changed. Those are the rules that I've just made up and I'm backing them up with
dozens more pages of incoherent rambling.

Our total derivate is therefore a multidimensional object with four indicies.
Suppose we try to pull a single partial derivative from this complete derivative,
what could our notation look like? Four underscript indicies would be unwieldy,
so maybe something like this:
\[
\frac{dB}{dX}[1,2,3,4] = \frac{\partial B_{1,2}}{\partial X_{3,4}}
\]

And since we're defining our own notation, let's splurge and reference our original
matricies in a similar manner:
\[
\frac{dB}{dX}[1,2,3,4] = \frac{\partial B[1,2]}{\partial X[3,4]}
\]

So what is this object we've defined? In all ways that matter, it seems to be
a multidimensional matrix. It has four dimensions of indices, rather than just
a row and column but each uniquely indexed cell in this object may have a
unique value, just like a standard two dimensional matrix. Let's define this
bit of notation formaly.

\begin{definition}[Multimatrix]
A multimatrix $M$ is a multidimensional object that has some shape,
$\vec{v} \in \mathbb{N}^n$, where $n \in \mathbb{N} \cup \{0\}$
is the number of dimensions of $M$.

Let $\vec{v}$ be the vector which describes the shape of
$M$, then $\vec{v} = |M|$. And for any other $\bar{v} \in \mathbb{N}^n$, $\bar{v}$
is a valid index of $M$ if and only if each value of $\bar{v}$ is less than or equal
to the value of it's corresponding index in $\vec{v}$.
That is, $\forall i: 1 \le \bar{v}_i \le \vec{v}_i$.

We can further simplify that notation for our set of valid indicies. Let's say we have
some $\vec{v} \in \mathbb{N}^n$ that defines the shape of some matrix. Then we'll
define the set of valid indicies of that matrix to be $\Dim(\vec{v})$. 

Each value of $M$ is a real value, so we'll define the set of all real valued
multidimensional matricies with shape $\vec{v}$ as $\mathbb{R}^{\vec{v}}$.

We'll use the notation $M[\bar{v}]$ to reference any individual scalar value of $M$
at indicies $\bar{v} \in \Dim(|M|)$. 
\end{definition}

That's a huge informtion dump so look at an example.

\begin{example}
Let $M$ be one of these newfangled `multimatrix' things. According to our
definition, we also need to define a shape for the multimatrix. Let's
say that our shape is $\left<5,7,9,2,3\right>$. In our notation, that
means that,
\[ |M| = \left<5,7,9,2,3\right> \]
Let's name this shape vector, $|M| = \vec{m}$. Notice that this is
a 5 dimensional multimatrix because $|\vec{m}| = 5$. Also note the
subtle difference in $||M||$ which is 5 and $|M|$ which is $\vec{m}$.

Let's look at some individual elements of $M$. We can say that,
\[ M[5,4,3,2,1] = 3 \]
And,
\[ M[4,3,2,1,1] = -8.5 \]
We can say that because we're just making up this multimatrix, but also
because $\left<5,4,3,2,1\right>$ and $\left<4,3,2,1,1\right>$
are both in $\Dim(\vec{m})$. That is to say, they're both valid
indexes of $M$.

Remember that $\Dim(\vec{m})$ is the set of valid indexes of $M$.
You can tell that the size of  $\Dim(\vec{m})$ is
$5 \times 7 \times 9 \times 2 \times 3 = 1890$, which is the number of
elements of $M$.

If $\left<5,4,3,2,1\right> = \bar{a}$ and $\left<4,3,2,1,1\right> = \bar{b}$
then we can say that $\bar{a} \in \Dim(\vec{m})$ and $\bar{b} \in \Dim(\vec{m})$
because both are valid indexes of $M$.

We can uses the indexes to reference $M$ like this,
\[ M[\bar{a}] = 3 \]
And this,
\[ M[\bar{b}] = -8.5 \]
\end{example}

Now that you seem to understand.. Don't give me that look... well if you're confused
you should have been asking questions, moving on. Now that \textit{most of you}
seem to have understood, I'll give you a theorem to prove.

\begin{theorem}[Dimensional Set and Index Concatenation]
\label{dim_set_ind_concat_thm}
If $\bar{a} \in \Dim(\vec{a})$ and $\bar{b} \in \Dim(\vec{b})$ then,
\[ \bar{a} \oplus \bar{b} \in \Dim(\vec{a} \oplus \vec{b}) \]
\end{theorem}
\begin{proof}
Left as an exercise for the reader (Exercise \ref{dim_set_ind_concat_ex}).
\end{proof}

\section{Defining the Derivative}

Going back to our original motivating problem, we (that is to say, myself and my
split personalities whom I reference as to give my writing the percieved authority
of more than one author) can express our derivative notation thusly,

\begin{definition}[Multimatrix on Multimatrix Derivative]
\label{mm_derivative}
Let $F(X)$ be some function from $\mathbb{R}^{\vec{x}}$ to $\mathbb{R}^{\vec{f}}$.
The complete derivative of $F$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{f} \oplus \vec{x}}$. That is,
\[ |F(X)| = \vec{f} \]
\[ |X| = \vec{x} \]
\[ \left|\frac{dF}{dX}\right| = \vec{f} \oplus \vec{x} \]
\[
\forall \bar{f} \in \Dim(\vec{f}),
        \bar{x} \in \Dim(\vec{x}):
\]
\[
\frac{dF}{dX}[\bar{f} \oplus \bar{x}] =
\frac{\partial F[\bar{f}]}{\partial X[\bar{x}]}
\]
\end{definition}

But, of course, there are other combinations of functions than from one
multimatrix to another. You might have a function from a multimatrix to a
scalar (such as summing up all the values of a multimatrix), or from a scalar
to a multimatrix (such as filling a multimatrix with a single scalar value).

We can define the total derivative for those cases as well.

\begin{definition}[Scalar on Multimatrix Derivative]
\label{sm_derivative}
Let $f(X)$ be some function from $\mathbb{R}^{\vec{x}}$ to $\mathbb{R}$.
The complete derivative of $f$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{x}}$. That is,
\begin{align*}
f(X) &\in \mathbb{R} \\
|X| &= \vec{x} \\
\left|\frac{df}{dX}\right| &= \vec{x}
\end{align*}
\[
\forall \bar{x} \in \Dim(\vec{x}):
        \frac{df}{dX}[\bar{x}] =
        \frac{\partial f}{\partial X[\bar{x}]}
\]
\end{definition}

\begin{definition}[Multimatrix on Scalar Derivative]
\label{ms_derivative}
Let $F(x)$ be some function from $\mathbb{R}$ to $\mathbb{R}^{\vec{f}}$.
The complete derivative of $F$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{f}}$. That is,
\begin{align*}
|F(x)| &= \vec{f} \\
x &\in \mathbb{R} \\
\left|\frac{dF}{dx}\right| &= \vec{f}
\end{align*}
\[
\forall \bar{f} \in \Dim(\vec{f}):
        \frac{dF}{dx}[\bar{f}] =
        \frac{\partial F[\bar{f}]}{\partial x}
\]
\end{definition}

Ok, that's neat and all but what does that actually mean? Can we answer our motivating 
question?
\[\frac{dB}{dX} = ? \]
Well... sort of. We have to abuse the notation a little and say that $B$ and $X$ are
just multimatricies with 2 dimensions. Then we can say that the total derivative
of $B$ with respect to $X$ is a multimatrix with shape $|B| \oplus |X|$,
whose elements we can define as,
\[\forall \bar{b} \in \Dim(|B|), \bar{x} \in \Dim(|X|):\]
\begin{align*}
\left( \frac{dB}{dX} \right)[\bar{b} \oplus \bar{x}]
&= \frac{\partial B[\bar{b}]}{\partial X[\bar{x}]} \\
&= \delta_{\bar{b}\bar{x}}\cos(\bar{x})
\end{align*}

Which isn't a huge improvement over our original notation. What would be great is
if we could define the total derivative without going down into the partial derivatives
at all, which we'll be able to do in later chapters. As a sneak peak (a whole
chapter ahead), that'll look like this:
\[
\frac{dB}{dX} = \Ident^3(|X|) \mmult{||X||} \cos(X)
\]

But that single line will require defining multimatrix multiplication and
a mysterious, and possibly dangerious, entity call a cubic identity, $\Ident^3(|X|)$.

\section{Exercises}

\begin{exercise}
\label{dim_set_ind_concat_ex}
Prove Theorem \ref{dim_set_ind_concat_thm}. Which is that,
\[ \bar{a} \oplus \bar{b} \in \Dim(\vec{a} \oplus \vec{b}) \]
\end{exercise}

\begin{exercise}
True or false?
\begin{enumerate}
\item $\left<1,1,1\right> \in \Dim(\left<5,5,5\right>)$
\item $\left<1,1\right> \in \Dim(\left<5,5,5\right>)$
\item $\left<1,2,3\right> \in \Dim(\left<3,2,1\right>)$
\item $\left<3,7\right> \in \Dim(\left<3,7\right>)$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $F(X)$ be a function from $\mathbb{R}^{\left<3,7,2\right>}$ to
$\mathbb{R}^{\left<6, 7\right>}$. That is, $|F(X)| = \left<6,7\right>$
and for any valid input to $F(X)$, $|X| = \left<3,7,2\right>$.
Then what is the shape of $dF/dX$?
\end{exercise}

\begin{exercise}
Let $f(X)$ be the sum of all values of $X$, where
$X \in \mathbb{R}^{\left<2,3,2\right>}$. That is,
\[ f(X) = \sum_{\forall \bar{x} \in \Dim(\left<2,3,2\right>)} X[\bar{x}] \]
What is the shape of $df/dX$? If $\bar{v} \in \Dim(|df/dX|)$ then what is
\[ \frac{df}{dX}[\bar{v}] = ? \]
\end{exercise}

\chapter{Multimatrix Multiplication}

\begin{displayquote}
``The universe is a cruel, uncaring void. The key to being happy isn't a search
for meaning. It's to just keep yourself busy with unimportant nonsense, and eventually,
you'll be dead.'' - Mr. Peanutbutter (BoJack Horseman Season 1, Episode 12
\cite{bojack})
\end{displayquote}

\section{Defining Multiplication}

The multiplication of traditional matricies is a calculation that, when one first
encouters it, inspires thoughts such as, ``What the shit?'' or, ``Get me a knife,
I gotta to stab someone.''

After seeing the method used in practice, one slowly comes to understand the
reason behind the madness. That is, that madness is the natural state of things
and matrix multiplication is used to punish those few brave souls who dare to
fight against the darkness.

Multimatrix multiplication is much the same.

Just to review, for two matricies $A$ and $B$, multipication is only defined
for $A \times B = C$ if $A$ has the same number of columns as $B$ has rows.
That is, if $|A| = \left< r_a, c_a \right>$ and $|B| = \left< r_b, c_b \right>$
then $A \times B$ is valid if and only if $c_a = r_b$. And furthermore,
the remaining dimensions of $A$ and $B$ define the diminsions of $C$,
$|C| = \left< r_a, c_b \right>$. Each element of $C$ is a sum of the product of
every combination of the corresponding row of $A$ and the corresponding column of
$B$,

\[ C[r_c, c_c] = \sum_{\forall i} A[r_c, i] B[i, c_c] \]

Easily done. The hard part isn't doing the calculation. The hard part getting over
your naive belief that mathematical operations should be rooted in some concrete
concept rather than ``this is the way it's done because it makes the equations
work.''

So now consider how we would multiply two matricies $A$ and $B$ if they had more
than two dimensions. Let's use a concrete example to make the problems more
structurally sound.
Let $|A| = \left<5,7,7\right>$ and $|B| = \left<7,7,2,4\right>$.
First of all, can we even multiply the two together? Hm... maybe? $|A|$ ends with
$\left<\ldots, 7\right>$ and $|B|$ starts with $\left<7,\ldots\right>$ so it would
make sense that we can somehow squish them together into something. By that reasoning,
the shape of our after-multiplication output would use the remaining dimensions
$\left<5, 7, 7, 2, 4\right> = |A \times B|$.

But $|A|$ also ends with $\left<\ldots, 7, 7\right>$ and $|B|$ starts with
$\left<7, 7\ldots\right>$ so maybe our multiplication operation should work on that
additional dimension, leaving us with $\left<5, 2, 4\right> = |A \times B|$.
But the first definition looks like it could still be useful! It could let us cover 
two dimensional matrix multiplication completely within our definition. This second
definition seems more... er... multidimensional... but it also seems to
arbitrarily depend on the shape of the two matricies.

What we really need is an operation which covers both lines of reasoning. To do
so, we need to compleately abandon conventional algebras, and indeed any hope of
returning to sanity. We need... a ternary operator.

I would argue that there shall always be some component of, 
``How many dimensions are you combining?'' inherent
in the operation. So we can distinguish our two types of multiplication, and indeed
an infinite number of types of multiplication, with a new operator, $\mmult{n}$ where
$|A \mmult{1} B| = \left<5, 7, 7, 2, 4\right>$ and 
$|A \mmult{2} B| = \left<5, 2, 4\right>$.

But the shape is only a small component of the multipliation  operation. Let's
just jump into the formal definition and I'll explain with an example:

\begin{definition}[Multimatrix Multiplication]
\label{mm_mult}
Let $A$ and $B$ be multimatricies such that $|A| = \vec{a} \oplus \vec{c}$,
$|B| = \vec{c} \oplus \vec{b}$, and $|\vec{c}| = n$ with
$n \in \{0\} \cup \mathbb{N}$. We shall call $n$ the 'size' or the 'width'
or the 'overlap' of the multiplication operation.
The shape of the result is defined, $|A \mmult{n} B| = \vec{a} \oplus \vec{b}$
and the value of the result is defined,
\[
\forall \bar{a} \in \Dim(\vec{a}), \bar{b} \in \Dim(\vec{b}):
(A \mmult{n}  B)[\bar{a} \oplus \bar{b}] =
\sum_{\forall \bar{c} \in \Dim(\vec{c})}
  A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}]
\]
\end{definition}

\newpage
\begin{example}
Let $|A| = \left<2, 3, 2\right>$ and $|B| = \left<3, 2, 2\right>$. I would
define their values in a three dimensional grid but I fear that my three dimensional
\LaTeX skills aren't up to the task (and three dimensional monitors are quite hard to
come by). So here is where we leave behind nice graphical representations of
things, and I'll just define the elements of $A$ and $B$ in the tables below.
\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c | c}
$\bar{a}_1$ & $\bar{a}_2$ & $\bar{a}_3$ & $A[\bar{a}]$ \\
\hline
1           & 1           & 1           & -0.5         \\
1           & 1           & 2           & 1            \\
1           & 2           & 1           & 2            \\
1           & 2           & 2           & -1           \\
1           & 3           & 1           & 5            \\
1           & 3           & 2           & 1            \\
2           & 1           & 1           & 3            \\
2           & 1           & 2           & -7           \\
2           & 2           & 1           & 5            \\
2           & 2           & 2           & 2            \\
2           & 3           & 1           & -3           \\
2           & 3           & 2           & -1
\end{tabular}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c | c}
$\bar{b}_1$ & $\bar{b}_2$ & $\bar{b}_3$ & $B[\bar{b}]$ \\
\hline
1           & 1           & 1           & 1            \\
1           & 1           & 2           & 0            \\
1           & 2           & 1           & 1            \\
1           & 2           & 2           & 0            \\
2           & 1           & 1           & 1            \\
2           & 1           & 2           & 0            \\
2           & 2           & 1           & 1            \\
2           & 2           & 2           & 0            \\
3           & 1           & 1           & 1            \\
3           & 1           & 2           & 0            \\
3           & 2           & 1           & 1            \\
3           & 2           & 2           & 0
\end{tabular}
\end{center}
\end{table}

Given $|A|$ and $|B|$, and by our definition of multimatrix multiplication,
you can see that $|A \mmult{2} B| = \left<2, 2\right>$. And the values of the
resulting multimatrix are defined:

\begin{table}[h!]
\begin{center}
\begin{tabular}{c c | c | c | c}
$\bar{x}_1$ & $\bar{x}_2$ & $(A \mmult{2} B)[\bar{x}]$ & = & = \\
\hline
1 & 1 &
  $A[1,1,1]B[1,1,1] + A[1,1,2]B[1,2,1] +$ & $(-0.5)(1) + (1)(1) +$ & 7.5 \\
&&$A[1,2,1]B[2,1,1] + A[1,2,2]B[2,2,1] +$ & $(2)(1) + (-1)(1) +$ & \\
&&$A[1,3,1]B[3,1,1] + A[1,3,2]B[3,2,1]$   & $(5)(1) + (1)(1)$ & \\
1 & 2 &
  $A[1,1,1]B[1,1,2] + A[1,1,2]B[1,2,2] +$ & $(-0.5)(0) + (1)(0) +$ & 0 \\
&&$A[1,2,1]B[2,1,2] + A[1,2,2]B[2,2,2] +$ & $(2)(0) + (-1)(0) +$ & \\
&&$A[1,3,1]B[3,1,2] + A[1,3,2]B[3,2,2]$   & $(5)(0) + (1)(0)$ & \\
2 & 1 &
  $A[2,1,1]B[1,1,1] + A[2,1,2]B[1,2,1] +$ & $(3)(1) + (-7)(1) +$ & 1 \\
&&$A[2,2,1]B[2,1,1] + A[2,2,2]B[2,2,1] +$ & $(5)(1) + (2)(1) +$ & \\
&&$A[2,3,1]B[3,1,1] + A[2,3,2]B[3,2,1]$   & $(-3)(1) + (1)(1)$ & \\
2 & 2 &
  $A[2,1,1]B[1,1,2] + A[2,1,2]B[1,2,2] +$ & $(3)(0) + (-7)(0) +$ & 0 \\
&&$A[2,2,1]B[2,1,2] + A[2,2,2]B[2,2,2] +$ & $(5)(0) + (2)(0) +$ & \\
&&$A[2,3,1]B[3,1,2] + A[2,3,2]B[3,2,2]$   & $(-3)(0) + (1)(0)$ &
\end{tabular}
\end{center}
\end{table}

\end{example}
\newpage

It's important to notice that this operation is identital to standard
matrix multiplication for two dimensional multimatrices. For instance,
if $A$ and $B$ are two dimensional multimatricies with shapes
$|A| = \left<l_a, r_a\right>$ and $|B| = \left<l_b, r_b\right>$
their product $A \mmult{1} B$ is valid if and only if $r_a = l_b$ and
the result is of shape $|A \mmult{1} B| = \left<l_a, r_b\right>$.
And of course, the value at each index
$\left<l_c, r_c\right> \in \Dim(|A \mmult{1} B|)$ is,
\[
  (A \mmult{1} B)[l_c, r_c]
  =
  \sum_{\forall 1 \le x \le r_a} 
  A[l_c, x] B[x, r_c]
\]
All of which would be the same if this was regular matrix multiplication.

\section{Rules of Multimatrix Multiplication}

Like two dimensional matrix multiplication, multimatrix multiplication is
not (usually) communicative. And it is only weakly associative. That sounds like
nonsense, and ideed it is, but I shall prove it to you.

Multimatrix multiplication may not even make sense dimensionally when you move
around parentheses. Take for example multimatricies $A$, $B$, and $C$ such that,
$|A| = \left<5,3,2\right>$,
$|B| = \left<2,7\right>$, and
$|C| = \left<3,7,2\right>$ where you should be able to see that
$(A \mmult{1} B) \mmult{2} C$ is a valid operation since
$|A \mmult{1} B| = \left<5,3,7\right>$ and
$|(A \mmult{1} B) \mmult{2} C| = \left<5,2\right>$. But
$A \mmult{1} (B \mmult{2} C)$ isn't a valid set of operations since even
$B \mmult{2} C$ is an invalid operation.

But even when the dimensional analysis makes sense, multimatrix multiplication
may not be associative (Honestly, how could you be so naive?). 
Take, for example, multimatricies $A$, $B$, and $C$ such
that $|A| = \left<2,2,2,2\right>$, $|B| = \left<2,2,2\right>$, and
$|C| = \left<2,2,2,2\right>$. Let,
\[
 A[\bar{a}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{a} = \left<1,1,1,2\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
\[
 B[\bar{b}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{b} = \left<1,2,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
And
\[
 C[\bar{c}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{c} = \left<1,1,1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
Then,
\[
 (A \mmult{2} B)[\bar{x}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{x} = \left<1,1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
So,
\[
 ((A \mmult{2} B) \mmult{2} C)[\bar{x}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{x} = \left<1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
But, $(B \mmult{2} C)[\bar{x}] = 0$ so $(A \mmult{2} (B \mmult{2} C))[\bar{x}] = 0$
Which goes to show that even though both $(A \mmult{2} B) \mmult{2} C$ and
$A \mmult{2} (B \mmult{2} C)$ are valid calculations which result in multimatrixes
of the same dimensionality, they are not equal so multiplicative associativity
doesn't hold. It also goes to show that your intuition is bad and you should feel bad.

However, there are a broad catagory of cases where associativity \textit{does} hold:

\begin{theorem}[Associativity of Multimatrix Multiplication]
\label{mm_associativity}
Let $A$, $B$, and $C$ be multimatricies and $m$ and $n$ in
$\{0\} \cup \mathbb{N}$.
If both operations $(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$
are valid dimensionally and $m+n \le ||B||$, then,
\[ (A \mmult{m} B) \mmult{n} C = A \mmult{m} (B \mmult{n} C) \]
\end{theorem}
\begin{proof}
Let $A$, $B$, and $C$ be matricies such that the calculations
$(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$ are both valid
and $m + n \le ||B||$.
Then the following must be true by the definiton of multimatrix multiplication
(Def. \ref{mm_mult}).

Since $A \mmult{m} B$ is defined, there exist vectors $\vec{a}, \vec{m},$ and
$\vec{b_1}$ such that 
$|\vec{m}| = m$,
$|A| = \vec{a} \oplus \vec{m}$,
and
$|B| = \vec{m} \oplus \vec{b_1}$

Since $B \mmult{n} C$ is defined, there exist vectors $\vec{b_2}, \vec{n},$ and
$\vec{c}$ such that
$|\vec{n}| = n$,
$|B| = \vec{b_2} \oplus \vec{n}$
and
$|C| = \vec{n} \oplus \vec{c}$

Since $|B| = \vec{m} \oplus \vec{b_1} = \vec{b_2} \oplus \vec{n}$ and
$||B|| \ge |\vec{m}| + |\vec{n}|$, it must follow that, there is some vector $\vec{b}$
such that,
\[
 |B| = \vec{m} \oplus \vec{b} \oplus \vec{n}
\]
Even if $\vec{b}$ is an empty vector. Then,
\[ |A \mmult{m} B| = \vec{a} \oplus \vec{b} \oplus \vec{n} \]
\[ |(A \mmult{m} B) \mmult{n} C| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
\[ |B \mmult{n} C| = \vec{m} \oplus \vec{b} \oplus \vec{c} \]
\[ |A \mmult{m} (B \mmult{n} C)| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
So,
\[ |(A \mmult{m} B) \mmult{n} C| = |A \mmult{m} (B \mmult{n} C)| \]
Which is necessary, but not sufficient, for our proof.

Looking at each element of the result matricies, let $j$ and $k$ be the values
of some parallel cells in $(A \mmult{m} B) \mmult{n} C$ and
$A \mmult{m} (B \mmult{n} C)$ respectively. Then for some indicies of those cells,
$\bar{a} \oplus \bar{b} \oplus \bar{c} \in
\Dim(\vec{a} \oplus \vec{b} \oplus \vec{c})$,
\begin{align*}
 j
 &= ((A \mmult{m} B) \mmult{n} C)[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
 (A \mmult{m} B)[\bar{a} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n}}
 \left(
  \sum_{\forall \bar{m} \in \Dim(\vec{m})}
  A[\bar{a} \oplus \bar{m}]B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 \right)
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n}, \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{m}, \bar{n}}
 A[\bar{a} \oplus \bar{m}]
 B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 \left(
 \sum_{\forall \bar{n}}
  B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
  C[\bar{n} \oplus \bar{c}]
 \right) \\
 &= \sum_{\forall \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 (B \mmult{n} C)[\bar{m} \oplus \bar{b} \oplus \bar{c}] \\
 &= (A \mmult{m} (B \mmult{n} C))[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
 &= k
\end{align*}
Since $(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$ are both
the same shape and every cell with the same index in each is equal, the two
values must be equal.
\end{proof}

Along the same vein, it's also true that there is a consistent time when
multimatrix multiplication is communicative - when both multimatricies are
the same shape and the multiplication is combining them completely into a
scalar by combining their full width.

\begin{theorem}[Communicativity of Multiplication]
Let $|A| = \vec{x}$ and $|B| = \vec{x}$. Then,
\[ A \mmult{|\vec{x}|} B = B \mmult{|\vec{x}|} A \]
\end{theorem}
\begin{proof}
Let $A$, $B$, and $\vec{x}$ be as described above. Notice that then $|A \mmult{|\vec{x}|} B|$
results in a zero dimensional multimatrix - a multimatrix with one element which is effectively
just a scalar.
\begin{align*}
(A \mmult{|\vec{x}|} B)[\{\}]
  &= \sum_{\forall \bar{x} \in \Dim(\vec{x})} A[\bar{x}]B[\bar{x}] \\
  &= \sum_{\forall \bar{x} \in \Dim(\vec{x})} B[\bar{x}]A[\bar{x}] \\
  &= (B \mmult{|\vec{x}|} A)[\{\}]
\end{align*}
\end{proof}

\section{Multiplicative Identities}

Multiplicative identities for multimatricies depend on the shape of the
multimatrix they are being multiplied against, just as for regular matricies.
However, the multiplicative identity also greatly depends on the size of
the particular multiplication operation. Making a confusing and frightening
situation even more so, like loosing a troup of clowns in children's cancer ward.

\begin{definition}[Multimatrix Identities]
\label{mm_mult_ident}
Let us define $\Ident^n(\vec{v})$ as the $n^{\text{th}}$ order identity with base shape
$\vec{v}$. This $\Ident^n(\vec{v})$ is a multimatrix with shape,
\[ |\Ident^n(\vec{v})| = \bigoplus_{i \in [1, n]} \vec{v} \]
That is, it's shape is $n$ successive concatinations of $\vec{v}$.
Let each element of $\Ident^n(\vec{v})$ is defined as,
\[ \forall \bar{v_1}, \bar{v_2}, \ldots, \bar{v_n} \text{ each in } \Dim(\vec{v}) : \]
\[
 \Ident^n(\vec{v})[\bar{v_1}, \bar{v_2}, \ldots, \bar{v_n}]
 = \left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{v_1} = \bar{v_2} = \ldots = \bar{v_n} \\
    0 & \mbox{otherwise}
  \end{array}
 \right.
\]

In particular, let us call $\Ident^2(\vec{v})$ the square identity of $\vec{v}$ and
$\Ident^3(\vec{v})$ the cubic identity of $\vec{v}$.
\end{definition}

That definition seems rather arbitrary but stick with me, I know what I'm
talking about. I've got a \sout{doctorate}
masters degree in \sout{mathematics} \sout{a real engineering field}, fine, software
engineering. Whatever. Just look at how the
square identity it relates to a multiplication operation.

\begin{theorem}[Multimatrix Multiplicative Identity]
\label{mm_ident}
For all $\vec{m}$ and $\vec{n}$ on which the operations are valid,
\[
 A \mmult{|\vec{m}|} \Ident^2(\vec{m}) = A
\]
and
\[
 \Ident^2(\vec{n}) \mmult{|\vec{n}|} A = A
\]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $A$ be a matrix with shape $|A| = \vec{a} \oplus \vec{m}$.
Then by the definition of multimatrix multiplication (Def. \ref{mm_mult}),
\[ \forall \bar{a} \in \Dim(\vec{a}), \bar{m_1} \in \Dim(\vec{m}) : \]
\[
 (A \mmult{|\vec{m}|} \Ident^2(\vec{m}))[\bar{a} \oplus \bar{m_1}]
 =
 \sum_{\forall \bar{m_2} \in \Dim(\vec{m})}
 A[\bar{a} \oplus \bar{m_2}] \Ident^2(\vec{m})[\bar{m_2} \oplus \bar{m_1}]
\]
Then, by the definition of $\Ident^2(\vec{m})$ (Def. \ref{mm_ident}) it follows
that,
\begin{align*}
 (A \mmult{|\vec{m}|} \Ident^2(\vec{m}))[\bar{a} \oplus \bar{m_1}]
 &=
 \left(
  \sum_{\forall \bar{m_2} \ne \bar{m_1}}
  A[\bar{a} \oplus \bar{m_2}] (0)
 \right)
 +
 \left(
  \sum_{\forall \bar{m_2} = \bar{m_1}}
  A[\bar{a} \oplus \bar{m_2}] (1)
 \right) \\
 \\
 &= 0 + A[\bar{a} \oplus \bar{m_1}] \\
 &= A[\bar{a} \oplus \bar{m_1}]
\end{align*}
Therefore $A \mmult{|\vec{m}|} \Ident^2(\vec{m}) = A$
\end{ppart}
\begin{ppart}
Let $A$ be a matrix with shape $|A| = \vec{n} \oplus \vec{a}$.
Then by the definition of multimatrix multiplication (Def. \ref{mm_mult}),
\[ \forall  \bar{n_1} \in \Dim(\vec{n}), \bar{a} \in \Dim(\vec{a}) : \]
\[
 (\Ident^2(\vec{n}) \mmult{|\vec{n}|} A)[\bar{n_1} \oplus \bar{a}]
 =
 \sum_{\forall \bar{n_2} \in \Dim(\vec{n})}
 \Ident^2(\vec{n})[\bar{n_1} \oplus \bar{n_2}]A[\bar{n_2} \oplus \bar{a}] 
\]
Then, by the definition of $\Ident^2(\vec{n})$ (Def. \ref{mm_ident}) it follows
that,
\begin{align*}
 (\Ident^2(\vec{n}) \mmult{|\vec{n}|} A)[\bar{n_1} \oplus \bar{a}]
 &=
 \left(
  \sum_{\forall \bar{n_2} \ne \bar{n_1}}
  (0) A[\bar{n_2} \oplus \bar{a}]
 \right)
 +
 \left(
  \sum_{\forall \bar{n_2} = \bar{n_1}}
  (1) A[\bar{n_2} \oplus \bar{a}]
 \right) \\
 \\
 &= 0 + A[\bar{n_1} \oplus \bar{a}] \\
 &= A[\bar{n_1} \oplus \bar{a}]
\end{align*}
Therefore $\Ident^2(\vec{n}) \mmult{|\vec{n}|} A = A$
\end{ppart}
\end{proof}

The square identity also pops up in the derivative of a multimatrix
with respect to itself.

\begin{theorem}[Self Derivative]
\label{self_derivative}
Let $X$ be some multimatrix. Then,
\[ \frac{dX}{dX} = \Ident^2(|X|) \]
\end{theorem}
\begin{proof}
Let $X$ be a multimatrix of shape $|X|=\vec{x}$.
By the derivative definition (Def. \ref{mm_derivative}),
\[
 \forall \bar{x_1} \in \Dim(\vec{x}),
         \bar{x_2} \in \Dim(\vec{x}):
\]
\[
 \frac{dX}{dX}[\bar{x_1} \oplus \bar{x_2}] = 
 \frac{\partial X[\bar{x_1}]}{\partial X[\bar{x_2}]}
\]
Since each element of $X$ is independent with respect to itself,
\begin{align*}
 \frac{dX}{dX}[\bar{x_1} \oplus \bar{x_2}]
 &= \left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{x_1} = \bar{x_2} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \Ident^2(\vec{x})[\bar{x_1} \oplus \bar{x_2}]
\end{align*}
Therefore $dX/dX = \Ident^2(|X|)$
\end{proof}

The cubic identity, strangely enough, appear in the derivative of elementwise
functions. Which is one of the reaspons I needed to expand my definition of
identities beyond $\Ident^2(\vec{v})$.

\begin{theorem}[Elementwise Derivative]
\label{elementwise_derivative}
Let $M$ be an element wise function on $X$. That is, Let $M(X)$ be a function
such that,
\[ M(X) : \mathbb{R}^{\vec{x}} \to \mathbb{R}^{\vec{x}} \]
And which applies the same scalar function to each element of $X$, 
\[ M(X)[\bar{x}] = m(X[\bar{x}]) \]
Also, let $M'$ be the elementwise function on $X$ which applies the derivative
of $M$'s associated scalar function to each element of $X$. That is,
\[ M'(X)[\bar{x}] = m'(X[\bar{x}]) \]
Then,
\[ \frac{dM}{dX} = \Ident^3(|X|) \mmult{||X||} M'(X) \]
\end{theorem}
\begin{proof}
By the definition of a multimatrix derivative,
\[ \forall \bar{m} \in \Dim(|M|), \bar{x} \in \Dim(|X|) : \]
\begin{align*}
 \frac{dM}{dX}[\bar{m} \oplus \bar{x}]
 &= \frac{\partial M[\bar{m}]}{\partial X[\bar{x}]} \\
 &= \frac{\partial m(X[\bar{m}])}{\partial X[\bar{x}]} \\
 &= \left\{
  \begin{array}{ll}
    m'(X[\bar{m}]) & \mbox{if } \bar{m} = \bar{x} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \delta_{\bar{m} \bar{x}} m'(X[\bar{m}])
\end{align*}
Since, by our definition of elementwise function, $|M| = |X|$, we can
say that, 
\begin{align*}
 (\Ident^3(|X|) \mmult{||X||} M'(X))[\bar{m} \oplus \bar{x}]
 &= \sum_{\forall \bar{y} \in \Dim(|X|)}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] M'(X)[\bar{y}] \\
 &= \sum_{\forall \bar{y} \in \Dim(|X|)}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] m'(X[\bar{y}]) \\
 &= \sum_{\forall \bar{y} = \bar{m} = \bar{x}}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] m'(X[\bar{y}]) \\
 &= \left\{
  \begin{array}{ll}
    m'(X[\bar{m}]) & \mbox{if } \bar{m} = \bar{x} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \delta_{\bar{m} \bar{x}} m'(X[\bar{m}])
\end{align*}
Therefore $dM/dX = \Ident^3(|X|) \mmult{||X||} M'(X)$
\end{proof}

Multimatrix identities also tend to cancel eachother out when multiplied by
multiples of their base shape lengths.

\begin{landscape}
\begin{theorem}[Identity Contraction]
For all $m, n, k \in \mathbb{N}$ and $\vec{x}$ as a valid multimatrix shape,
where $m > k$ and $n > k$,
\[ \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) = \Ident^{m+n-2k}(\vec{x}) \]
\end{theorem}
\begin{proof}
Consider that for some indicies
$\{\bar{a_1}, \bar{a_2}, \ldots \bar{a}_{m-k}\}$
and $\{\bar{b_1}, \bar{b_2}, \ldots \bar{b}_{n-k}\}$ where each
$\bar{a_i} \in \Dim(\vec{x})$ and each $\bar{b_i} \in \Dim(\vec{x})$,

\begin{align*}
&\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] = \\
&\sum_{\forall \bar{c_1}, \bar{c_2}, \ldots \bar{c_k} \in \Dim(\vec{x})}
\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c_1} \oplus \bar{c_2} \oplus \ldots]
\Ident^n(\vec{x})
[\bar{c_1} \oplus \bar{c_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots]
\end{align*}

Consider that $\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c_1} \oplus \bar{c_2} \oplus \ldots]$
will be 0 whenever the any $\bar{c_i} \ne \bar{c_j}$ - we can simplify that to all
the same $c$.

\begin{align*}
&\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] = \\
&\sum_{\forall \bar{c} \in \Dim(\vec{x})}
\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c} \oplus \bar{c} \oplus \ldots]
\Ident^n(\vec{x})
[\bar{c} \oplus \bar{c} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots]
\end{align*}

Consider also that there can be at most one $\bar{c}$ for which
$\bar{c} = \bar{a_1} = \bar{a_2} \ldots$. And since our right hand side is
the sum of products of ones and zeros, we can therefore conclude it is either 1 or 0.

We can futher reason that the right hand side can only be one when all of the
$\bar{a_i}$ and all of the $\bar{b_i}$ equal $\bar{c}$. Furthermore, if all
$\bar{a_i}$ and all $\bar{b_i}$ are equal then that $\bar{c}$ must exist since it can
be any value in the range of all $\bar{a_i}$ and $\bar{b_i}$ and this is not a
zero width multiplication ($k >= 1$). Therefore,

\begin{align*}
\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] =
\left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{a_1} = \bar{a_2} = \ldots = \bar{b_1} = \bar{b_2} = \ldots \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\end{align*} 

Which is an identity matrix! And since the dimensional rules of multimatrix 
multiplication show that,

\[
 \left|\left| \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right|\right|
 =
 ||\Ident^m(\vec{x})|| + ||\Ident^n(\vec{x})|| - 2k|\vec{x}|
\]

We've shown that,

\[ \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) = \Ident^{m+n-2k}(\vec{x}) \]
\end{proof}
\end{landscape}

A final useful theorem (definition?) happens to involve 0 overlap multiplication.
You'll notice that I very carefully constructed my definition to allow for this
definition (theorem?). Does it still count as a theorem if it only works because
I tweeked the definition of multimatrix multiplication to make it fit? I feel
a bit like Tommy Wiseau writing my own sex scene into the script... twice... God,
what a tool.

\begin{theorem}[Zero Overlap Multiplication]
Anyways, if $|A| = \vec{a}$ and $|B| = \vec{b}$, for every $\bar{a} \in \Dim(\vec{a})$
and $\bar{b} \in \Dim(\vec{b}):$

\[ (A \mmult{0} B)[\bar{a} \oplus \bar{b}] = A[\bar{a}] B[\bar{b}] \]

\end{theorem}
\begin{proof}
\begin{align*}
	(A \mmult{0} B)[\bar{a} \oplus \bar{b}]
	&=
	\sum_{\forall \bar{v} \in \Dim(\left<\right>)}
	A[\bar{a} \oplus \bar{v}] B[\bar{v} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{v} \in \left\{\left<\right>\right\}}
	A[\bar{a} \oplus \bar{v}] B[\bar{v} \oplus \bar{b}] \\
	&=
	A[\bar{a} \oplus \left<\right>] B[\left<\right> \oplus \bar{b}] \\
	&=
	A[\bar{a}] B[\bar{b}] \\
\end{align*}
\end{proof}

\section{Multiplicative Inverses}

Given a multimatrix, $M$, you might ask, ``What is the multiplicative inverse of $M$?''
And I would have to tell you that your question is not well formed.

Since multimatrix multiplication is a trinary operation, you need to factor in the
index of multipliction. Consider multiplication by $M$ a function on $X$ - some other
multimatrix. That is, 
\[ F(X; M, m) = M \mmult{m} X \]

What we're looking for in a `multiplicative inverse' is actually a function which,
when composed with $F$, results in $X$. That is,

\[ (F^{-1} \circ F)(X) = F^{-1}(M \mmult{m} X) = X \]

That inverse function, $F^{-1}$ also needs to be a multiplication operation to
really be a `multiplicative inverse', so we can assume that it is defined by
some multimatrix $N$ and some inverting index of multiplication $n$, such that,

\[ F^{-1} (Y; N, n) = N \mmult{n} Y \]

Let's say that $M$ has some shape $\vec{l} \oplus \vec{r}$ such that $|\vec{r}| = m$.
Then it is only applicable for an $m$ overlap multiplication on some $X$ where,
$|X| = \vec{r} \oplus \vec{x}$ for some $\vec{x}$.

So since,
\[ |M \mmult{m} X| = \vec{l} \oplus \vec{x}\]
and
\[ |N \mmult{n} (M \mmult{m} X)| = |X| = \vec{r} \oplus \vec{x} \]
It must be true that, $n = |\vec{l}|$ and $|N| = \vec{r} \oplus \vec{l}$.

TODO

\section{Hadamard Product}

What the hell is a `Hadamard product'? The Hadamard product \cite{wiki:hadamard}
of two matricies is how one would actually expect matrix multiplication to work.

In short, if two matricies, $A$ and $B$ have the same shape,
\[ |A| = |B| = \left<r,c\right> \]
Then their Hadamard product, $A \bullet B$, is the same shape and each element
is just the product of the corresponding elements of $A$ and $B$. That is,
\[ (A \bullet B)[r,c] = A[r,c] B[r,c] \]

This seems to extend quite naturally into the world of multimatricies.
But hold on - there's a catch. While the Hadamard product only logically applies
to regular matricies when their shapes are exactly equal, there are case where
the Hadamard product can easily be applied to differently shaped multimatricies,
they just need to have overlapping portions of their shape vectors, just as
with multimatrix multiplication.

\begin{definition}[Multimatrix Hadamard Product]
\label{multi_had_prod}
Let $A$ and $B$ be multimatricies such that $|A| = \vec{a} \oplus \vec{c}$
and $|B| = \vec{c} \oplus \vec{b}$. Also let $c = |\vec{c}|$.
Then, the multimatrix Hadamard product, $A \dmult{c} B$, is a multimatrix
such that,
\[ |A \dmult{n} B| = \vec{a} \oplus \vec{c} \oplus \vec{b} \]
And for any $\bar{a} \in \Dim(\vec{a})$, $\bar{b} \in \Dim(\vec{b})$,
and $\bar{c} \in \Dim(\vec{c})$,
\[
  (A \dmult{n} B)[\bar{a} \oplus \bar{c} \oplus \bar{b}]
  = A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}]
\]
\end{definition}

TODO: exposition

\begin{theorem}[Equivalent Zero Hadamard Product and Multiplication]
\[ A \dmult{0} B = A \mmult{0} B \]
\end{theorem}
\begin{proof}
TODO: exercise?
\end{proof}

TODO: associative?

TODO: exposition

\begin{theorem}[Equivalent Hadamard Product and Multiplication Chains]
When $X_1 \ldots X_{m-1}$ are all in $\mathbb{R}^{\vec{x}}$ and
$|X_m| = \vec{x} \oplus \vec{r}$ for any $\vec{r}$,
\[
  (((\Ident^n(\vec{x})
  \mmult{|\vec{x}|} X_1) \mmult{|\vec{x}|} X_2) \mmult{|\vec{x}|} X_3)
  \ldots X_m
  =
  \Ident^{n-m}(\vec{x})
  \dmult{|\vec{x}|} X_1 \dmult{|\vec{x}|} X_2 \dmult{|\vec{x}|} X_3
  \ldots X_m
\]
And when $X_2 \ldots X_m$ are all in $\mathbb{R}^{\vec{x}}$ and
$|X_1| = \vec{l} \oplus \vec{x}$ for any $\vec{l}$,
\[
  X_1 \mmult{|\vec{x}|} (X_2 \mmult{|\vec{x}|} (X_3
  \ldots (X_m \mmult{|\vec{x}|} 
  \Ident^n(\vec{x}))))
  =
  X_1 \dmult{|\vec{x}|} X_2 \dmult{|\vec{x}|} X_3
  \ldots X_m \dmult{|\vec{x}|} 
  \Ident^{n-m}(\vec{x})
\]
\end{theorem}
\begin{proof}
TODO 
\end{proof}

TODO: Communicative property

\section{Exercises}

\begin{exercise}
Let $A$, $B$, and $C$ be multimatricies such that $|A| = \left<1,2,3\right>$,
$|B| = \left<3,1,2\right>$ and $|C| = \left<3,2,1\right>$. For each of these
multiplications determine if it is valid, and if so, what the resulting
shape would be:
\begin{enumerate}
\item $A \mmult{2} B$
\item $B \mmult{2} A$
\item $A \mmult{2} C$
\item $A \mmult{1} C$
\end{enumerate}
\end{exercise}

\begin{exercise}
In which of these cases does $A \mmult{m} (B \mmult{n} C) = (A \mmult{m} B) \mmult{n} C$?
\begin{enumerate}
\item When,
	\begin{itemize}
		\item $|A| = \left<3, 7, 9\right>$
		\item $|B| = \left<9, 5\right>$
		\item $|C| = \left<5, 7, 3\right>$
		\item and $m = n = 1$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $|A| = \left<6, 6, 6\right>$
		\item $|B| = \left<6, 6, 6\right>$
		\item $|C| = \left<6, 6, 6\right>$
		\item and $m = n = 2$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $|A| = \left<3, 5, 3\right>$
		\item $|B| = \left<5, 3, 11\right>$
		\item $|C| = \left<11, 2, 7\right>$
		\item $m = 2$
		\item and $n = 1$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $|A| = \left<3, 5, 3\right>$
		\item $|B| = \left<5, 3, 3, 5\right>$
		\item $|C| = \left<3, 3, 5, 2\right>$
		\item $m = 2$
		\item and $n = 3$
	\end{itemize}
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $|X| = \vec{x}$. Simplify the following where possible:
\begin{enumerate}
\item $\Ident^5(\vec{x}) \mmult{3 |\vec{x}|} \Ident^3(\vec{x})$
\item $\Ident^2(\vec{x}) \mmult{|\vec{x}|} X$
\item $\Ident^3(\vec{x}) \mmult{2|\vec{x}|} X$
\item $\Ident^2(\vec{x}) \mmult{|\vec{x}|} \left(X \mmult{|\vec{x}|} \Ident^3(|\vec{x}|)
				\right) \mmult{2|\vec{x}|} \Ident^3(|\vec{x}|)$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $M$ be some multimatrix such that $|M| = \left<5,2,3\right>$. Then what is
$dM/dM$?
\end{exercise}

\begin{exercise}
Let $Y = \tan(X)$ be the elementwise application of the scalar tangent function to
the elements of $X$, that is, for all $\bar{v} \in \Dim(|X|)$
\[ Y[\bar{v}] = \tan(X[\bar{v}]) \]
What is $dY/dX$?
\end{exercise}

\chapter{Other Operations With Multimatricies}

\begin{displayquote}
``You can't just start a section with a subsection heading. At least put a quote
in there or something,'' - Dr. William Tepfenhart, my software architecture professor.
\end{displayquote}

\section{Scalars}

One of those obvious operations we'll need to do is multiply
a multimatrix by a constant. I guess that means we need another definition,

\begin{definition}[Multimatrix Scalar Multiplication]
Let $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$. If
\[ sA = As = B \]
Then
\[ \forall \bar{x} \in \Dim(\vec{x}):
   B[\bar{x}] = (s)(A[\bar{x}]) \]
In other words, multiplying $A$ by a scalar $s$ simply multiplies each of
it's elements by that scalar.
\end{definition}

But actually, you'll find that scalars can be modeled equally well by multricies
with the shpe $\left<\right>$. It works with multiplication and derivatives. Look:

\begin{theorem}[Empty Shape Multimatrix and Scalar Multiplication Equivalence]
\label{s_mm_mult_equiv}
For any multiplication operation defined between multimatricies and scalars,
the scalar may be replaced by an equivalent empty-shaped multimatrix whose
single value is that of the scalar.

In other words, let $s \in \mathbb{R}$ and $S \in \mathbb{R}^{\left<\right>}$ such that
$S[\left<\right>] = s$. Then, for any multimatrix $A$, 
\[ sA = As = S \mmult{0} A = A \mmult{0} S \]
\end{theorem}
\begin{proof}
Let $|A| = \vec{a}$. Then for all $\bar{a} \in \Dim(\vec{a})$,
\begin{align*}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= A[\bar{a}](s) \\
	&= (As)[\bar{a}]
\end{align*}
So $sA = As$. Also,
\begin{align*}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= S[\left<\right>]A[\bar{a}] \\
	&= \sum_{\forall \bar{c} \in \Dim(\left<\right>)}
		S[\left<\right> \oplus \bar{c}]A[\bar{c} \oplus \bar{a}] \\
	&= \sum_{\forall \bar{c} \in \{\left<\right>\}}
		S[\left<\right> \oplus \bar{c}]A[\bar{c} \oplus \bar{a}] \\
	&= S[\left<\right> \oplus \left<\right>]A[\left<\right> \oplus \bar{a}] \\
	&= S[\left<\right>]A[\bar{a}] \\
	&= (S \mmult{0} A)[\left<\right> \oplus \bar{a}] \\
	&= (S \mmult{0} A)[\bar{a}] \\
\end{align*}
So, $sA = S \mmult{0} A$.
Finally, by similiar reasoning,
\begin{align*}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= A[\bar{a}](s) \\
	&= A[\bar{a}]S[\left<\right>] \\
	&= \sum_{\forall \bar{c} \in \Dim(\left<\right>)}
		A[\bar{a} \oplus \bar{c}] S[\bar{c} \oplus \left<\right>] \\
	&= \sum_{\forall \bar{c} \in \{\left<\right>\}}
		A[\bar{a} \oplus \bar{c}] S[\bar{c} \oplus \left<\right>] \\
	&= A[\bar{a} \oplus \left<\right>] S[\left<\right> \oplus \left<\right>] \\
	&= A[\bar{a}]S[\left<\right>] \\
	&= (A \mmult{0} S)[\bar{a} \oplus \left<\right>] \\
	&= (A \mmult{0} S)[\bar{a}]
\end{align*}
So $sA = A \mmult{0} S$. Therefore we've shown that
$sA = As = S \mmult{0} A = A \mmult{0} S$
\end{proof}

It wasn't until I noticed this that I began to suspect that the same might be true
of the derivative rules.
A quick check confirmed that the dimensionality worked out and it seems intuitive
that we would always be able to treat scalars
as empty-shape multimatricies but let's canonize it.

\begin{theorem}[Empty Shape Multimatrix and Scalar Domain Derivative Equivalence]
\label{s_mm_domain_equiv}
That really rolls off the tongue.

Let $F(x)$ be a function $F : \mathbb{R} \to \mathbb{R}^{\vec{f}}$. And let
$G(X)$ be an equivalent function,
$G : \mathbb{R}^{\left<\right>} \to \mathbb{R}^{\vec{f}}$
such that for all $X \in \mathbb{R}^{\left<\right>}$ it is true that
$G(X) = F(X[\left<\right>])$
Then,
\[ \frac{dF(x)}{dx} = \frac{dG(X)}{dX} \]
\end{theorem}
\begin{argument}
Let $\bar{f} \in \Dim(\vec{f})$ then for $F(x)[\vec{f}] = G(X)[\vec{f}]$.
Since $\partial F(x)[\vec{f}] / \partial x$ is defined as the rate of
change of $F(x)[\vec{f}]$ with respect to $x$, it stands to reason that,
$\partial F(X[\left<\right>])[\vec{f}] / \partial X[\left<\right>]$ is
equivalent by definition. By are given $G(X) = F(X[\left<\right>])$ we
can then also say that,
\[
\frac{ \partial F(X[\left<\right>])[\vec{f}] }{ \partial X[\left<\right>] }
=
\frac{ \partial G(X)[\vec{f}] }{ \partial X[\left<\right>] }
\]
Therefore, by our definitions of derivatives,
\[
\frac{ d F(x) }{ d x }
=
\frac{ d G(X) }{ d X }
\]
\end{argument}

\begin{theorem}[Empty Shape Multimatrix and Scalar Range Derivative Equivalence]
\label{s_mm_range_equiv} Or `E.S.M.a.S.R.D.E', for short.

Let $f(X)$ be a function $f : \mathbb{R}^{\vec{x}} \to \mathbb{R}$. And let
$F(X)$ be an equivalent function,
$F : \mathbb{R}^{\vec{x}} \to \mathbb{R}^{\left<\right>}$
such that for all $X \in \mathbb{R}^{\vec{x}}$ it is true that
$F(X)[\left<\right>] = f(X)$
Then,
\[ \frac{df(X)}{dX} = \frac{dF(X)}{dX} \]
\end{theorem}
\begin{argument}
Let $\bar{x} \in \Dim(\vec{x})$. We note that as $\partial f(X) / \partial X[\bar{x}]$
is defined as the rate of change of $f(X)$ as $X[\bar{x}]$ changes, and each value of
$f(X)$ is equal to $F(X)[\left<\right>]$, we can reason that the rate of change of
$F(X)[\left<\right>]$ as $X[\bar{x}]$ changes is equal to the rate of change of $f(X)$
as $X[\bar{x}]$ changes. That is,
\[
	\frac{\partial f(X)}{\partial X[\bar{x}]}
		=
	\frac{\partial F(X)[\left<\right>]}{\partial X[\bar{x}]}
\]
And so by our definition of derivatives,
\[ \frac{df(X)}{dX} = \frac{dF(X)}{dX} \]
\end{argument}

\section{Addition}

Ah addition. What can I say about multimatrix addition that hasn't been
said so many times before. Well... anything, actually. I don't think anyone's
written about multimatrix addition before. Probably should define it first
though.

\begin{definition}[Multimatrix Addition]
Let $A, B, C \in \mathbb{R}^{\vec{x}}$. If
\[ A + B = C \]
Then
\[ \forall \bar{x} \in \Dim(\vec{x}):
   C[\bar{x}] = A[\bar{x}] + B[\bar{x}] \]
In other words, adding two multimatricies (which must be of the same shape) results
in a multimatrix of the same shape with elements which are the sum of the corresponding
elements of the added multimatricies.
\end{definition}

Multimatrix subtraction is defined as the inverse of multimatrix addition.

\begin{definition}[Multimatrix Subtraction]
Let $A, B \in \mathbb{R}^{\vec{x}}$. Then,
\[ (A + B) - B = A \]
\end{definition}

The following should be obvious, condesendingly so,

\begin{theorem}[Equivalent Addition for Subtraction]
\label{eq_add_sub_thm}
For any multimatricies $A$ and $B$ where $|A| = |B|$,
\[ A - B = A + (-1)B \]
\end{theorem}
\begin{proof}
Left as an exercise for the reader (Exercise \ref{eq_add_sub_ex}).
\end{proof}

As with scalar, vector, or regular matrix addition, multimatrix addition is
communicative. 
\begin{theorem}[Communicative Property of Multimatrix Addition]
Let $A, B \in \mathbb{R}^{\vec{x}}$. Then,
\[ A + B = B + A \]
\end{theorem}
\begin{proof}
Let $C = A + B$ then,
\[ \forall \bar{x} \in \Dim(\vec{x}) : C[\bar{x}] = A[\bar{x}] + B[\bar{x}] \]
By standard scalar addition rules that also means that,
$C[\bar{x}] = B[\bar{x}] + A[\bar{x}]$. So $C = B + A = A + B$.
\end{proof}

It's similarly trial to show that multimatrix addition is associative.

\begin{theorem}[Associative Property of Multimatrix Addition]
Let $A, B, C \in \mathbb{R}^{\vec{x}}$. Then,
\[ (A + B) + C = A + (B + C) \]
\end{theorem}
\begin{proof}
For all $\bar{x} \in \Dim(\vec{x})$,
\begin{align*}
\left( (A+B)+C \right)[\bar{x}]
&= (A[\bar{x}]+B[\bar{x}])+C[\bar{x}] \\
&= A[\bar{x}]+(B[\bar{x}]+C[\bar{x}]) \\
&= \left( A+(B+C) \right)[\bar{x}]
\end{align*}
\end{proof}

A little less obvious, in that a coffee-deprived undergraduate might
struggle with it for a second, is that multimatrix multiplication can be
distributed over it's addition.

\begin{landscape}
\begin{theorem}[Multimatrix on Multimatrix Distributive Property]
If $|A| = \vec{a} \oplus \vec{n}$ and $|B|=|C|=\vec{n} \oplus \vec{r}$ then,
\[ A \mmult{n} (B + C) = A \mmult{n} B + A \mmult{n} C \]
Similarly, if $|C| = \vec{n} \oplus \vec{c}$ and $|A|=|B|=\vec{l} \oplus \vec{n}$ then,
\[ (A + B) \mmult{n} C = A \mmult{n} C + B \mmult{n} C \]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $|A| = \vec{a} \oplus \vec{n}$ and $|B|=|C|=\vec{n} \oplus \vec{r}$.
So, $\forall \bar{a} \in \Dim(\vec{a}), \bar{r} \in \Dim(\vec{r}) :$
\begin{align*}
 \left( A \mmult{n} (B + C) \right)[\bar{a} \oplus \bar{r}]
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] (B + C)[\bar{n} \oplus \bar{r}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] (B[\bar{n} \oplus \bar{r}] + C[\bar{n} \oplus \bar{r}]) \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] B[\bar{n} \oplus \bar{r}]
  + A[\bar{a} \oplus \bar{n}]C[\bar{n} \oplus \bar{r}] \\
 &= \left( \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] B[\bar{n} \oplus \bar{r}] \right)
    +
    \left( \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}]C[\bar{n} \oplus \bar{r}] \right) \\
 &= \left( A \mmult{n} B \right)[\bar{a} \oplus \bar{r}] +
     \left( A \mmult{n} C \right)[\bar{a} \oplus \bar{r}]
\end{align*}
So $A \mmult{n} (B + C) = A \mmult{n} B + A \mmult{n} C$
\end{ppart}
\begin{ppart}
Let $|C| = \vec{n} \oplus \vec{c}$ and $|A|=|B|=\vec{l} \oplus \vec{n}$.
So, $\forall \bar{c} \in \Dim(\vec{c}), \bar{l} \in \Dim(\vec{l}) :$
\begin{align*}
 \left( (A + B) \mmult{n} C \right)[\bar{l} \oplus \bar{c}]
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    (A+B)[\bar{l} \oplus \bar{n}] C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    (A[\bar{l} \oplus \bar{n}]+B[\bar{l} \oplus \bar{n}])C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]
    +B[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}] \\
 &= \left(\sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]\right)
    +
    \left(\sum_{\forall \bar{n} \in \Dim(\vec{n})}
    B[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]\right) \\
 &= \left(A \mmult{n} C\right)[\bar{l} \oplus \bar{c}]
    +\left(B \mmult{n} C\right)[\bar{l} \oplus \bar{c}]
\end{align*}
So $(A+B) \mmult{n} C = A \mmult{n} C + B \mmult{n} C$ 
\end{ppart}
\end{proof}
\end{landscape}

But it should be obvious - seriously, how have you not figured this out yet -
that that the same applies to scalar  multiplication distributing over multimatrix
addition..
\begin{theorem}[Scalar on Multimatrix Distributive Property]
If $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$ then,
\[ s(A + B) = sA + sB \]
\end{theorem}
\begin{proof}
Let $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$.
Then for all $\bar{x} \in \Dim(\vec{x})$
\begin{align*}
(s(A+B))[\bar{x}]
&= (s)((A+B)[\bar{x}]) \\
&= (s)(A[\bar{x}] + B[\bar{x}]) \\
&= (s)(A[\bar{x}]) + s(B[\bar{x}])
\end{align*}
Therefore $s(A+B) = sA + sB$
\end{proof}

\section{Transposition}

Multimatrix transposition clearly requires an additional parameter defining where
the cut happens in the shape vector.

\begin{definition}[Multimatrix Transposition]
\label{tran_def}
Let $|X| = \vec{l} \oplus \vec{r}$ where $|\vec{l}| = l$ and $|\vec{r}| = r$
\[ |\Tran(X, l)| = \vec{r} \oplus \vec{l} \]
And,
\[ \forall \bar{l} \in \Dim(\vec{l}), \bar{r} \in \Dim(\vec{r}) : \]
\[ \Tran(X, l)[\bar{r} \oplus \bar{l}] = X[\bar{l} \oplus \bar{r}] \]
We call $l$ the index of transpose.
\end{definition}

Here's an example - get out your crayons and follow along.

\begin{example}
Let's construct some multimatrix, $M$, with a shape:
\[ |M| = \left<3,2,7,5,9\right> \]
Then lets take the first transpose of that
multimatrix:
\[ |\Tran(M, 1)| = \left<2,7,5,9,3\right> \]
And... how about the third:
\[ |\Tran(M, 3)| = \left<5,9,3,2,7\right> \]
You can see, it's as though I cut off the front of the shape vector and transfered
it to the end of the vector.

You do the same thing with indicies. So,
\[ M[1,2,3,4,5] = \Tran(M, 3)[4,5,1,2,3] \]
\end{example}

However, you'll see cases in later chapters of multimatrix transposes where
the index of transpose is larger than the size of the shape matrix. So we need
to make sense of that operation as well. Fortunately, there is a logical way
to extend the definition. Consider that this transposition can be reasonably
shown to be equivalent to moving the first index of the shape matrix
to the end of the shape matrix over and over again, this can be done any number
of times, therefore doing so for more than the size of the shape matrix still works,
it just requires a bit of modular arithmetic.

\begin{definition}[Multimatrix Transpose Extension]
\label{tran_ext}
For any multimatrix, the following transposes are equivalent,
\[ \Tran(X, n) = \Tran(X, \remainder(n, ||X||)) \]
Where $\remainder(n, ||X||)$ is the remainder of $n$ divided by $||X||$.
That is, a value $0 \le \remainder(n, ||X||) < ||X||$ such that there exists an
integer $k$ where, $n = k||X|| + \remainder(n, ||X||)$.
\end{definition}

This extension is consisent with the prevous definition because, for any
index of transpose, $l$, that would be covered by the original definition,
$0 \le l \le ||X||$, you can see that, in all but one case $\remainder(l, ||X||) = l$.
The exception, $l = ||X||$ is covered by showing that
$\remainder(||X||, ||X||) = 0$ and $\Tran(X, 0) = \Tran(X, ||X||)$. 

Thus, we (by which I mean \textit{me} since \textit{you} certainly haven't done any
of the work) have shown a natural equivalent class \cite{book:abstract} of transpose
operations. On multimatricies with a shape of some length $n$ there are $n$ unique
transpose; $\Tran(X, i)$ for all $0 \le i < n$. All other transposes are equivalent
to one of these base transposes by our extended definition.

To make use of this equivalence, we need to first show how these classes are
isomorphic \cite{book:abstract} to a more manageable group.

\begin{theorem}[Isomorphism of Transpostion and Modular Integers]
\label{tran_int_iso}
Let $\mathbb{T}_n$ be the set of possible transpositions on multimatricies with
shapes of length $n$. Let $\circ$ be the operation of composition. Then
$(\mathbb{T}_n, \circ)$ is a group which is isomorphic to the group of
integers mod $n$ under addition. That is,
\[ (\mathbb{T}_n, \circ) \cong (\mathbb{Z}_n, +) \]
\end{theorem}
\begin{proof}
I'll need to tackle this proof in two parts. First I'll need to show that
that set of transpositions is a group under composition, then I'll need to
provide a one-to-one and onto map that preseves the respective group operations
between transpositions and integers mod $n$.

\begin{ppart}
To begin, I'll show that the set of all transpositions on multimatricies 
with shapes of length $n$ form a group under composition.

Let's look at the composition of some two transposes on a multimatrix.
So, let $A$ be a multimatrix with shape of length $n$. Now let $x$ and $y$
be indicies of transpositions. Then what is $\Tran(\Tran(A, x), y)$?
To apply the the first transpose we need to break $|A|$ into two vectors,
so let $|A| = \vec{l} \oplus \vec{r}$ where $|\vec{l}| = x$.

Then,
\[ |\Tran(A, x)| = \vec{r} \oplus \vec{l} \]
And for all
$\bar{l} \in \Dim(\vec{l})$ and $\bar{r} \in \Dim(\vec{r})$,
\[ \Tran(A, x)[\bar{r} \oplus \bar{l}] = A[\bar{l} \oplus \bar{r}] \]

Now to apply a transposition with index of transposition $y$ to $\Tran(A,x)$
we need to break up $|\Tran(A,x)|$ into two vector, such as
$|\Tran(A,x)| = \vec{l}' \oplus \vec{r}'$.

Since we've already shown that $|\Tran(A,x)| = \vec{r} \oplus \vec{l}$
we end up with two conceptual splits of that shape vector that need to
be reconciled.

\begin{case} $|\vec{l}'| \le |\vec{r}|$

Then we can divide $|\Tran(A,x)|$ into three vectors, $\vec{a}$, $\vec{b}$, and $\vec{c}$
such that
\[ |\Tran(A,x)| = \vec{b} \oplus \vec{c} \oplus \vec{a} \]
\[ \vec{l}' = \vec{b} \]
\[ \vec{r}' = \vec{c} \oplus \vec{a} \]
\[ \vec{r} = \vec{b} \oplus \vec{c} \]
\[ \vec{l} = \vec{a} \]
It helps to think of the $\oplus$ operation as a cut in two separate places in the same
vector because of the two conceptual splits from the two transpositions to see why this
logic is valid.

So we can then say,
\[ |A| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]

And since,
\[ |\Tran(\Tran(A, x), y)| = \vec{r}' \oplus \vec{l}' \]
It follows that,
\[ |\Tran(\Tran(A, x), y)| = \vec{c} \oplus \vec{a} \oplus \vec{b} \]

Now, let $\bar{a} \in \Dim(\vec{a})$, $\bar{b} \in \Dim(\vec{b})$, and
$\bar{c} \in \Dim(\vec{c})$. Notice that because of Theorem 
\ref{dim_set_ind_concat_thm}, $\bar{c} \oplus \bar{a} \in \Dim(\vec{r}')$ and
$\bar{b} \oplus \bar{c} \in \Dim(\vec{r})$.

Continuing our definition of transposition,
\begin{align*}
  \Tran(\Tran(A, x), y)[\bar{c} \oplus \bar{a} \oplus \bar{b}]
  &=
  \Tran(A, x)[\bar{b} \oplus \bar{c} \oplus \bar{a}] \\
  &=
  A[\bar{a} \oplus \bar{b} \oplus \bar{c}]
\end{align*}

See now that this composit operation is also a transposition, $\Tran(A, z)$ where,
$z = |\vec{a} \oplus \vec{b}|$.
So by definition,
\begin{align*}
  |\Tran(A, z)|
  &=
  \vec{c} \oplus \vec{b} \oplus \vec{a} \\
  &=
  |\Tran(\Tran(A, x), y)|  
\end{align*}
And,
\begin{align*}
  \Tran(A, z)[\bar{c} \oplus \bar{a} \oplus \bar{b}]
  &=
  A[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
  &=
  \Tran(\Tran(A, x), y)[\bar{c} \oplus \bar{a} \oplus \bar{b}]
\end{align*}
\end{case}

\begin{case} $|\vec{l}'| > |\vec{r}|$
Then we can divide $|\Tran(A,x)|$ into three vectors, $\vec{a}$, $\vec{b}$, and $\vec{c}$
such that
\[ |\Tran(A,x)| = \vec{c} \oplus \vec{a} \oplus \vec{b} \]
\[ \vec{l}' = \vec{c} \oplus \vec{a} \]
\[ \vec{r}' = \vec{b} \]
\[ \vec{r} = \vec{c} \]
\[ \vec{l} = \vec{a} \oplus \vec{b} \]

So we can then say,
\[ |A| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]

And since,
\[ |\Tran(\Tran(A, x), y)| = \vec{r}' \oplus \vec{l}' \]
It follows that,
\[ |\Tran(\Tran(A, x), y)| = \vec{b} \oplus \vec{c} \oplus \vec{a} \]

Now, let $\bar{a} \in \Dim(\vec{a})$, $\bar{b} \in \Dim(\vec{b})$, and
$\bar{c} \in \Dim(\vec{c})$. Notice that because of Theorem 
\ref{dim_set_ind_concat_thm}, $\bar{c} \oplus \bar{a} \in \Dim(\vec{l}')$ and
$\bar{a} \oplus \bar{b} \in \Dim(\vec{l})$.

Continuing our definition of transposition,
\begin{align*}
  \Tran(\Tran(A, x), y)[\bar{b} \oplus \bar{c} \oplus \bar{a}]
  &=
  \Tran(A, x)[\bar{c} \oplus \bar{a} \oplus \bar{b}] \\
  &=
  A[\bar{a} \oplus \bar{b} \oplus \bar{c}]
\end{align*}

See now that this composit operation is also a transposition, $\Tran(A, z)$ where,
$z = |\vec{a}|$.
So by definition,
\begin{align*}
  |\Tran(A, z)|
  &=
  \vec{b} \oplus \vec{c} \oplus \vec{a} \\
  &=
  |\Tran(\Tran(A, x), y)|  
\end{align*}
And,
\begin{align*}
  \Tran(A, z)[\bar{b} \oplus \bar{c} \oplus \bar{a}]
  &=
  A[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
  &=
  \Tran(\Tran(A, x), y)[\bar{b} \oplus \bar{c} \oplus \bar{a}]
\end{align*}
\end{case}

So I've shown that in both cases, the composit function of two transposes is
itself a transpose. So transposes of multimatrcies of shape of length $n$ are
a closed group under composition. Of.
\end{ppart}

\begin{ppart}
In this last part, I'll define a bijective map
$\phi: \mathbb{T}_n \to \mathbb{Z}_n$ for which
\[ \phi(\Tran_y \circ \Tran_x) = \phi(\Tran(y)) + \phi(\Tran(x)) \]
Which is the definition of an isomorphism.

Note that I'm letting $\Tran_i$ be a shorthand for ``a transposition
with index of transposition $i$ that is in $\mathbb{T}_n$''.

So let's just define $\phi$.
\[ \phi(\Tran_i) = i \]
Done. I'm a genious.

Ok, the hard part is showing that it preserves the group operations.
Let's go back to our example in the first part of this proof. Let $A$ be
any multimatrix with shape length $n$. Then let there be two transpositons
$\Tran_x$ and $\Tran_y$ which we'll apply in series. First remember that
to define $\Tran_x$ we needed to divide $|A|$ into $|A| = \vec{l} \oplus \vec{r}$
where $|\vec{l}| = x$. Then we'd say that
\[ |\Tran(A, x)| = \vec{r} \oplus \vec{l} \]
Then to further apply $\Tran(\Tran(A, x), y)$ we need to divide $|\Tran(A,x)|$
into,
\[ |\Tran(A, x)| = \vec{l}' \oplus \vec{r}' \]
Where $|\vec{l}'| = y$.
That leads to two cases,

\setcounter{case}{0}
\begin{case} $|\vec{l}'| \le |\vec{r}|$

And in this case we showed that we needed to split up $|A|$ into,
\[ |A| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
\[ \vec{l}' = \vec{b} \]
\[ \vec{r}' = \vec{c} \oplus \vec{a} \]
\[ \vec{r} = \vec{b} \oplus \vec{c} \]
\[ \vec{l} = \vec{a} \]

So that,
\[ |\Tran(A, x)| = \vec{b} \oplus \vec{c} \oplus \vec{a} \]
\[ |\Tran(\Tran(A, x), y)| = \vec{c} \oplus \vec{a} \oplus \vec{b} \]
And note that,
\[ x = |\vec{a}| \]
\[ y = |\vec{b}| \]

And we showed that the composit $\Tran_z = \Tran_y \circ \Tran_x$ was such
that,
\[ z = |\vec{a} \oplus \vec{b}| \]

Consider that $\phi(\Tran_z) = z$, and so,
\begin{align*}
  z
  &= |\vec{a} \oplus \vec{b}| \\
  &= |\vec{a}| + |\vec{b}| \\
  &= x + y \\
  &= y + x \\
  z &\equiv y + x \mod n \\
  \phi(\Tran_z) &= \phi(\Tran_y) + \phi(\Tran_x) \\
  \phi(\Tran_y \circ \Tran_x) &= \phi(\Tran_y) + \phi(\Tran_x)
\end{align*}
So in this case the isomorphism works.
\end{case}

\begin{case} $|\vec{l}'| > |\vec{r}|$

And in this case we showed that we needed to split up $|A|$ into,
\[ |A| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
\[ \vec{l}' = \vec{c} \oplus \vec{a} \]
\[ \vec{r}' = \vec{b} \]
\[ \vec{r} = \vec{c} \]
\[ \vec{l} = \vec{a} \oplus \vec{b} \]

So that,
\[ |\Tran(A, x)| = \vec{c} \oplus \vec{a} \oplus \vec{b} \]
\[ |\Tran(\Tran(A, x), y)| = \vec{b} \oplus \vec{c} \oplus \vec{a} \]
And note that,
\[ x = |\vec{a} \oplus \vec{b}| \]
\[ y = |\vec{c} \oplus \vec{a}| \]

And we showed that the composit $\Tran_z = \Tran_y \circ \Tran_x$ was such
that,
\[ z = |\vec{a}| \]

Consider that $\phi(\Tran_z) = z$, and so,
\begin{align*}
  z
  &= |\vec{a}| \\
  &\equiv |\vec{a}| + n \mod n \\
  &\equiv |\vec{a}| + |\vec{a} \oplus \vec{b} \oplus \vec{c}| \mod n \\
  &\equiv |\vec{a} \oplus \vec{a} \oplus \vec{b} \oplus \vec{c}| \mod n \\
  &\equiv |\vec{c} \oplus \vec{a} \oplus \vec{a} \oplus \vec{b}| \mod n \\
  &\equiv |\vec{c} \oplus \vec{a}| + |\vec{a} \oplus \vec{b}| \mod n \\
  &\equiv y + x \mod n \\
  \phi(\Tran_z) &= \phi(\Tran_y) + \phi(\Tran_x) \\
  \phi(\Tran_y \circ \Tran_x) &= \phi(\Tran_y) + \phi(\Tran_x)
\end{align*}
So also in this case the isomorphism works.
\end{case}

So, in both cases $\phi$ is a valid isomorphism between $(\mathbb{T}_n, \circ)$
and $(\mathbb{Z}_n, +)$.
\end{ppart}
\end{proof}

Well what does that mean? I'm so glad you asked... weirdo who doesn't know what
an isomorphism is but is several chapters into a hobby math book. It means that...

\begin{corollary}[Stacked Transpose]
For any multimatrix $A$ whose shape is $n$ long (an $n$ dimensional multimatrix
in other words), it is always true that
\[ \Tran(\Tran(A, x),y) = \Tran(A, \remainder(x+y,n)) \]
\end{corollary}

\begin{example}
Let's take a look at a concrete example. Say we have a
multimatrix $A$ who's shape is $\left<11, 3, 7, 5\right>$. 
Then see that,
\[ |\Tran(A, 2)| = \left<7, 5, 11, 3\right> \]
And applying another transpose,
\[ |\Tran(\Tran(A, 2), 3)| = \left<3, 7, 5, 11\right> \]
Notice that our corollary says that this double transpose should be
equal to the transpose $\Tran(A, \remainder(2+3, 4))$ which would
be $\Tran(A, 1)$ and indeed it is:
\begin{align*}
  |\Tran(A, 1)|
  &= \left<3, 7, 5, 11\right> \\
  &= |\Tran(\Tran(A, 2), 3)|
\end{align*}
\end{example}

Another useful corollary from Theorem \ref{tran_int_iso} shows you how to
undo transposition operations with other transposition operations.

\begin{corollary}[Inverse Transpose]
Let $A$ be a multimatrix such that $||A|| = n$. And let
$x$ be some integer. Then, the inverse of $\Tran_x$ is
$\Tran_{-x}$. That is,
\[ \Tran(\Tran(A, x), -x) = X \]
\end{corollary}

\begin{example}
Say you have a multimatrix, $A$, of shape $\left<2,7,1,8,3\right>$.
Let's apply a transpose to it, $\Tran_3$, and we'll get the shape,
\[ |\Tran(A, 3)| = \left<8,3,2,7,1\right> \]
Now, to get back to $A$ we can apply another transpose $\Tran_{-3}$ according
to the above corollary.
Our extended definition of transposition (Def. \ref{tran_ext})
tells us that this is equal to $\Tran_{\remainder(-3, 5)} = \Tran_{2}$.
So the inverse of $\Tran_3$ is $\Tran_2$, which we can see here:
\begin{align*}
  |\Tran(\Tran(A, 3), 2)|
  &= \left<2,7,1,8,3\right> \\
  &= |A|
\end{align*}
\end{example}

Now that you understand the basics of transposition, let's take a look
at some theorems involving it.

\begin{theorem}[Identity Transpose]
Let $n$ be a positive integer and let $\vec{x}$ be a positive length
shape vector. Then for any integer $k$,
\[ \Tran(\Ident^n(\vec{x}), k|\vec{x}|) = \Ident^n(\vec{x}) \]
\end{theorem}
\begin{proof}
Consider that for some ordered set of indicies
$\{\bar{x}_1, \bar{x}_2, \ldots \bar{x}_n\} = X$ such that for each
$\bar{x}_i$ it is true that $\bar{x}_i \in \Dim(\vec{x})$.
Let $X'$ be some ordered permutation of $X$, then we can reason that,
\begin{align*}
 \Ident^n(\vec{x})\left[\bigoplus X'\right]
 &= 
	\left\{
  \begin{array}{ll}
    1 & \mbox{if all } \bar{x}_i \in X' \mbox{ are equal.}\\
    0 & \mbox{otherwise}
  \end{array}
	\right.\\
 &= 
	\left\{
  \begin{array}{ll}
    1 & \mbox{if all } \bar{x}_i \in X \mbox{ are equal.}\\
    0 & \mbox{otherwise}
  \end{array}
	\right.\\
 &=\Ident^n(\vec{n})\left[\bigoplus X\right]
\end{align*}
Since for the permutation
$X' = \left\{\bar{x}_{k+1}, \bar{x}_{k+2} \ldots \bar{x}_{n},
\bar{x}_1, \bar{x}_2 \ldots \bar{x}_k\right\}$
\begin{align*}
  \Tran(\Ident^n(\vec{x}), k|\vec{x}|)\left[\bigoplus X'\right]
	&=
  \Ident^n(\vec{x})\left[\bigoplus X\right]\\
	&=
  \Ident^n(\vec{x})\left[\bigoplus X'\right]
\end{align*}
Therefore
$\Tran(\Ident^n(\vec{x}), k|\vec{x}|) = \Ident^n(\vec{x})$
\end{proof}

The above theorem is useful whenever you come accross a transpose of a
multimatrix identity.

Another useful theorem for whenever you need to change the order of
multimatrix multiplications is this one,

\begin{theorem}[Transposition of Multimatrix Multiplication]
\label{mm_tran_mult}
Let $|A| = \vec{a} \oplus \vec{c}$ and $|B| = \vec{c} \oplus \vec{b}$ then
\[
 \Tran(A \mmult{|\vec{c}|} B, |\vec{a}|)
 =  \Tran(B, |\vec{c}|) \mmult{|\vec{c}|} \Tran(A, |\vec{a}|)
\]
\end{theorem}
\begin{proof}
Let $\bar{a} \in \Dim(\vec{a})$ and $\bar{b} \in \Dim(\vec{b})$.
Then,
\begin{align*}
	\Tran(A \mmult{|\vec{c}|} B, |\vec{a}|)[\bar{b} \oplus \bar{a}]
	&=
	\left(A \mmult{|\vec{c}|} B \right)[\bar{a} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	B[\bar{c} \oplus \bar{b}] A[\bar{a} \oplus \bar{c}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	\Tran(B, |\vec{c}|)[\bar{b} \oplus \bar{c}]
	\Tran(A, |\vec{a}|)[\bar{c} \oplus \bar{a}] \\
	&=
	\left(\Tran(B, |\vec{c}|) \mmult{|\vec{c}|} \Tran(A, |\vec{a}|) \right)
	[\bar{b} \oplus \bar{a}]
\end{align*}
\end{proof}

TODO: exposition here

\begin{theorem}[Communicativity of Scalar Multiplication and Transposition]
Let $s$ be a scalar, $x$ be in $\mathbb{N} \cup \{0\}$, and $A$ be
a multimatrix on which $\Tran(A, x)$ is defined. Then,
\[ s\Tran(A, x) = \Tran(sA, x) \]
\end{theorem}
\begin{proof}
Let $|A| = \vec{x} \oplus \vec{a}$ where $|\vec{x}| = x$. Then
let $\bar{x} \in \Dim(\vec{x})$ and $\bar{a} \in \Dim(\vec{a})$. So,
\begin{align*}
  (s\Tran(A, x))[\bar{a} \oplus \bar{x}]
  &= s \Tran(A, x)[\bar{a} \oplus \bar{x}] \\
  &= s A[\bar{x} \oplus \bar{a}] \\
  &= (s A)[\bar{x} \oplus \bar{a}] \\
  &= \Tran(sA, x)[\bar{a} \oplus \bar{x}]
\end{align*}
Therefore $s\Tran(A, x) = \Tran(sA, x)$.
\end{proof}

TODO: exposition here

\begin{theorem}[Communicativity of Addition and Transposition]
\label{com_add_tran_thm}
Let $x$ be in $\mathbb{N} \cup \{0\}$ and let $A$ and $B$ be
multimatricies such that $|A| = |B|$ and $\Tran(A, x)$ is defined.
Then,
\[ \Tran(A, x) + \Tran(B, x) = \Tran(A+B, x) \]
\end{theorem}
\begin{proof}
Left as an exercise for the reader (Exercise \ref{com_add_tran_ex}).
\end{proof}

\section{Exercises}

\begin{exercise}
\label{eq_add_sub_ex}
Prove Theorem \ref{eq_add_sub_thm} which is that,
\[ A - B = A + (-1)B \]
\end{exercise}

\begin{exercise}
\label{com_add_tran_ex}
Prove Theorem \ref{com_add_tran_thm} which is that,
\[ \Tran(A, x) + \Tran(B, x) = \Tran(A+B, x) \]
\end{exercise}

\begin{exercise}
Let $A$ be a multimatrix such that $|A| = <1,2,3,4,5>$,
Find the shape of the following trasnposes:
\begin{enumerate}
\item $|\Tran(A, 3)| = ?$
\item $|\Tran(A, 4)| = ?$
\item $|\Tran(A, 5)| = ?$
\item $|\Tran(A, 0)| = ?$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $M$ be any multimatrix.
Find the value, in terms of $M$, of the following equations.
For example $\Tran(M, 1)[2,3,4,5,1] = M[1,2,3,4,5]$
\begin{enumerate}
\item $\Tran(M, 3)[1,2,3,4] = ?$
\item $\Tran(M, 0)[6,5,3] = ?$
\item $\Tran(M, 2)[1,1,1] = ?$
\end{enumerate}
\end{exercise}

\chapter{Generic Derivative Rules}

\begin{displayquote}
``It goes without saying that all writers owe a debt to their teachers... who
naturally bear all responsibility for any errors contained herein...''
- Paul Rentel, 'Manifolds, Tensors, and Forms'
\end{displayquote}

\begin{theorem}[Transpose Derivative]
\label{tran_derivative}
$|X| = \vec{l} \oplus \vec{r}$
\[
 \frac{d\Tran(X, |\vec{l}|)}{dX} =
 \Tran(\Ident^2(\vec{r}) \mmult{0} \Ident^2(\vec{l}), |\vec{r}|)
\]
\end{theorem}
\begin{proof}
\begin{align*}
 \frac{d\Tran(X, |\vec{l}|)}{dX}
 [\bar{r_1} \oplus \bar{l_1} \oplus \bar{l_2} \oplus \bar{r_2}]
 &= \delta_{\bar{r_1}\bar{r_2}} \delta_{\bar{l_1}\bar{l_2}} \\
 &= \delta_{\bar{r_2}\bar{r_1}} \delta_{\bar{l_1}\bar{l_2}} \\
 &=
  \Ident^2(|\vec{r}|)[\bar{r_2}\oplus\bar{r_1}]
  \Ident^2(|\vec{l}|)[\bar{l_1}\oplus\bar{l_2}] \\
 &=
   \left(
    \Ident^2(|\vec{r}|)
    \mmult{0}
    \Ident^2(|\vec{l}|)
  \right)
  [\bar{r_2}\oplus\bar{r_1}\oplus\bar{l_1}\oplus\bar{l_2}] \\
 &=
  \Tran(
    \Ident^2(|\vec{r}|)
    \mmult{0}
    \Ident^2(|\vec{l}|)
  , |\vec{r}|) 
 [\bar{r_1} \oplus \bar{l_1} \oplus \bar{l_2} \oplus \bar{r_2}]
\end{align*}
\end{proof}

\begin{theorem}[Right Multiplicative Derivative]
\label{right_mult_derivative}
Let $A$ and $X$ be multimatricies such that $A \mmult{n} X$ is
defined. Then,
\[ \frac{d(A \mmult{n} X)}{dX} = A \mmult{n} \Ident^2(|X|) \]
\end{theorem}
\begin{proof}
Let $|A| = \vec{a} \oplus \vec{n}$ and $|X| = \vec{n} \oplus \vec{x}$
where $|\vec{n}| = n$ which is equivalent to saying $A \mmult{n} X$ is defined.
Also let $\bar{a} \in \Dim(\vec{a})$, $\bar{n} \in \Dim(\vec{n})$,
$\bar{x}_1 \in \Dim(\vec{x})$ and $\bar{x}_2 \in \Dim(\vec{x})$.
Then, starting with the definition of the derivative,
\begin{align*}
  \frac{d(A\mmult{n}X)}{dX}[\bar{a} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2]
  &=
  \frac{
    \partial (A \mmult{n} X)[\bar{a} \oplus \bar{x}_1]
  }{
    \partial X[\bar{n} \oplus \bar{x}_2]
  } \\
  &=
  \frac{
    \partial \left(
      \sum_{\forall \bar{m} \in \Dim(\vec{n})}
        A[\bar{a} \oplus \bar{m}]
        X[\bar{m} \oplus \bar{x}_1]
    \right)
  }{
    \partial X[\bar{n} \oplus \bar{x}_2]
  } \\
  &=
  \sum_{\forall \bar{m} \in \Dim(\vec{n})}
    \frac{
      \partial 
      A[\bar{a} \oplus \bar{m}]
      X[\bar{m} \oplus \bar{x}_1]
    }{
      \partial X[\bar{n} \oplus \bar{x}_2]
    } \\
  &=
  \left(
    \sum_{\forall \bar{m} \ne \bar{n}} 0
  \right)
  +
  \left(
    \sum_{\forall \bar{m} = \bar{n}}
    \frac{
      \partial 
      A[\bar{a} \oplus \bar{m}]
      X[\bar{m} \oplus \bar{x}_1]
    }{
      \partial X[\bar{n} \oplus \bar{x}_2]
    }
  \right) \\
  &=
  \frac{
    \partial 
    A[\bar{a} \oplus \bar{n}]
    X[\bar{n} \oplus \bar{x}_1]
  }{
    \partial X[\bar{n} \oplus \bar{x}_2]
  } \\
  &= A[\bar{a} \oplus \bar{n}] \delta_{\bar{x}_1 \bar{x}_2}
\end{align*}
Then, note that,
\begin{align*}
  A \mmult{n} \Ident^2(|X|)[\bar{a} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2]
  &=
  \sum_{\forall \bar{m} \in \Dim(\vec{n})}
  A[\bar{a} \oplus \bar{m}]
  \Ident^2(|X|)[\bar{m} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2] \\
  &=
  \sum_{\forall \bar{m} \in \Dim(\vec{n})}
  A[\bar{a} \oplus \bar{m}]
  \delta_{\bar{m} \bar{n}}
  \delta_{\bar{x}_1 \bar{x}_2} \\
  &=
  \left(
    \sum_{\forall \bar{m} \ne \bar{n}} 0
  \right)
  +
  \left(
    \sum_{\forall \bar{m} = \bar{n}}
    A[\bar{a} \oplus \bar{m}]
    \delta_{\bar{m} \bar{n}}
    \delta_{\bar{x}_1 \bar{x}_2}
  \right) \\
  &=
  A[\bar{a} \oplus \bar{n}]
  \delta_{\bar{n} \bar{n}}
  \delta_{\bar{x}_1 \bar{x}_2} \\
  &=
  A[\bar{a} \oplus \bar{n}]
  \delta_{\bar{x}_1 \bar{x}_2}
\end{align*}
So, 
\[
  \frac{d(A\mmult{n}X)}{dX}[\bar{a} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2]
  =
  A \mmult{n} \Ident^2(|X|)[\bar{a} \oplus \bar{x}_1 \oplus \bar{n} \oplus \bar{x}_2]
\]
For all $\bar{a}$, $\bar{x}_1$, $\bar{n}$ and $\bar{x}_2$. Therefore,
\[
  \frac{d(A\mmult{n}X)}{dX}
  =
  A \mmult{n} \Ident^2(|X|)
\]
\end{proof}

\begin{landscape}
\begin{theorem}[Left Multiplicative Derivative]
\label{left_mult_derivative}
If $|X| = \vec{x} \oplus \vec{n}$ and $|A| = \vec{n} \oplus \vec{a}$
and $n = |\vec{n}|$ and $a = |\vec{a}|$ then,
\[ \frac{d(X \mmult{n} A)}{dX} = \Ident^2(|X \mmult{n} A|) \mmult{a} \Tran(A, n) \]
\end{theorem}
\begin{proof}
Let $|X| = \vec{x} \oplus \vec{n}$ and $|A| = \vec{n} \oplus \vec{a}$. Then,
for all
$\bar{x}_1, \bar{x}_2 \in \Dim(\vec{x}), \bar{a}_1 \in \Dim(\vec{a}),
\bar{n}_1 \in \Dim(\vec{n}):$
\begin{align*}
	\frac{d(X \mmult{n} A)}{dX}
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	&= 
	\frac{
		\partial (X \mmult{n} A) [\bar{x}_1 \oplus \bar{a}_1]
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\frac{
		\partial \left(
			\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
			X[\bar{x}_1 \oplus \bar{n}_2] A[\bar{n}_2 \oplus \bar{a}_1]
		\right)
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
	\frac{
		\partial \left(
			X[\bar{x}_1 \oplus \bar{n}_2] A[\bar{n}_2 \oplus \bar{a}_1]
		\right)
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
	\delta_{\bar{x}_1\bar{x}_2}
	\delta_{\bar{n}_1\bar{n}_2}
	A[\bar{n}_2 \oplus \bar{a}_1] \\
	&= 
	\delta_{\bar{x}_1\bar{x}_2}
	A[\bar{n}_1 \oplus \bar{a}_1]
\end{align*}
We can also show that,
\begin{align*}
	\left(\Ident^2(|X \mmult{n} A|) \mmult{a} \Tran(A, n)\right)
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a}_1)}
	\Ident^2(|X \mmult{n} A|)[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{a}_2]
	\Tran(A, n)[\bar{a}_2 \oplus \bar{n}_1] \\
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a})}
	\delta_{\bar{x}_1 \bar{x}_2} \delta_{\bar{a}_1 \bar{a}_2}
	\Tran(A, n)[\bar{a}_2 \oplus \bar{n}_1] \\
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a})}
	\delta_{\bar{x}_1 \bar{x}_2} \delta_{\bar{a}_1 \bar{a}_2}
	A[\bar{n}_1 \oplus \bar{a}_2] \\
	&=
	\delta_{\bar{x}_1 \bar{x}_2}
	A[\bar{n}_1 \oplus \bar{a}_1]
\end{align*}
Then since,
\[
	\frac{d(X \mmult{n} A)}{dX}
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	=
	\left(\Ident^2(|X \mmult{n} A|) \mmult{a} \Tran(A, n)\right)
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
\]
we have shown that,
\[
	\frac{d(X \mmult{n} A)}{dX}
	=
	\Ident^2(|X \mmult{n} A|) \mmult{a} \Tran(A, n)
\]
\end{proof}
\end{landscape}


\begin{theorem}[Addition Rule]
\label{addition_drule}
\[ \frac{d(F(X) + G(X))}{dX} = \frac{dF(X)}{dX} + \frac{dG(X)}{dX} \]
\end{theorem}
\begin{proof}
Let $|F(X)| = |G(X)| = \vec{f}$ and $|X| = \vec{x}$. Then,
$\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):$
\begin{align*}
	\frac{d(F(X) + G(X))}{dX}[\bar{f} \oplus \bar{x}]
	&=
	\frac{\partial (F(X) + G(X))[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial (F(X)[\bar{f}] + G(X)[\bar{f}])}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial F(X)[\bar{f}]}{\partial X[\bar{x}]} +
	\frac{\partial G(X)[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{dF(X)}{dX}[\bar{f} \oplus \bar{x}]+
	\frac{dG(X)}{dX}[\bar{f} \oplus \bar{x}]
\end{align*}
So $d(F(X) + G(X))/dX = dF(X)/dX + dG(X)/dX$
\end{proof}

A rule for derivatives involving scalar multiplication also seems generally
useful. Of course, by the Theorem \ref{s_mm_mult_equiv}, this is just a very specific
case of Theorem \ref{right_mult_derivative} but the proof is simple and worth
the additional ink, I think,

\begin{theorem}[Scalar Multiplication Rule]
\label{scalar_mult_drule}
\[ \frac{d(sF(X))}{dX} = s\frac{dF(X)}{dX} \]
\end{theorem}
\begin{proof}
Let $|F(X)| = \vec{f}$ and $|X| = \vec{x}$. Then
$\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):$
\begin{align*}
	\frac{d(sF(X))}{dX}[\bar{f} \oplus \bar{x}]
	&=
	\frac{\partial (sF(X))[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial (s)(F(X)[\bar{f}])}{\partial X[\bar{x}]} \\
	&=
	s\frac{\partial F(X)[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	s\frac{dF(X)}{dX}[\bar{f} \oplus \bar{x}]
\end{align*}
Therefore $d(sF(X))/dX = s(dF(X)/dX)$
\end{proof}

\begin{landscape}
\begin{theorem}[Multiplication Rule]
\label{multiplication_rule}
Let $|F| = \vec{f} \oplus \vec{c}$, $|G| = \vec{c} \oplus \vec{g}$,
and $|X| = \vec{x}$. Then,
\begin{align*}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX} =
 F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} +
 \Tran\left(
   \Tran(G(X), |\vec{c}|)
     \mmult{|\vec{c}|}
   \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right),
   |\vec{g} \oplus \vec{x}|
 \right)
\end{align*}
\end{theorem}
\begin{proof}
\[
 \forall
  \bar{f} \in \Dim(\vec{f}),
  \bar{g} \in \Dim(\vec{g}),
  \bar{x} \in \Dim(\vec{x})
 :
\]
\begin{align*}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
 &= \frac{
       \partial \left(F(X) \mmult{|\vec{c}|} G(X)\right)[\bar{f} \oplus \bar{g}]
    }{
       \partial X[\bar{x}]
    } \\
 &= \frac{
       \partial \left(
        \sum_{\forall \bar{c} \in \Dim(\vec{c})}
         F(X)[\bar{f} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{g}]
      \right)
    }{
       \partial X[\bar{x}]
    } \\
 &= \sum_{\forall \bar{c} \in \Dim(\vec{c})}
    \frac{
      \partial F(X)[\bar{f} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{g}]
    }{
      \partial X[\bar{x}]
    }\\
 &= \sum_{\forall \bar{c} \in \Dim(\vec{c})}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]}
    +
    \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
   \left(
    \sum_{\forall \bar{c}}
      F(X)[\bar{f} \oplus \bar{c}]
      \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]}
   \right)
   +
   \left(
    \sum_{\forall \bar{c}}
      \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
      G(X)[\bar{c} \oplus \bar{g}]
   \right) \\
 &=
    l + r
\end{align*}
Which each become,
\begin{align*}
 l
 &=
  \sum_{\forall \bar{c}}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]} \\
 &=
  \sum_{\forall \bar{c}}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{dG(X)}{dX}[\bar{c} \oplus \bar{g} \oplus \bar{x}] \\
 &=
   \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right)
   [\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{align*}
\begin{align*}
 r
 &=
  \sum_{\forall \bar{c}}
    \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
  \sum_{\forall \bar{c}}
    \frac{dF(X)}{dX}[\bar{f} \oplus \bar{c} \oplus \bar{x}]
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
  \sum_{\forall \bar{c}}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
      [\bar{c} \oplus \bar{x} \oplus \bar{f}]
    \Tran(G(X), |\vec{c}|)[\bar{g} \oplus \bar{c}] \\
 &=
  \sum_{\forall \bar{c}}
    \Tran(G(X), |\vec{c}|)[\bar{g} \oplus \bar{c}]
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
      [\bar{c} \oplus \bar{x} \oplus \bar{f}] \\
 &=
  \left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
  \right)[\bar{g} \oplus \bar{x} \oplus \bar{f}] \\
 &= 
  \Tran\left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
    , |\vec{g} \oplus \vec{x}|
  \right)[\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{align*}
Therefore 
\begin{alignat*}{3}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
  &=&&
  \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right)
  [\bar{f} \oplus \bar{g} \oplus \bar{x}] \\
  &&&+
  \Tran\left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
    , |\vec{g} \oplus \vec{x}|
  \right)[\bar{f} \oplus \bar{g} \oplus \bar{x}] \\
  &=&&
  \left\{
    \begin{array}{l}
      \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right) \\
      +
      \Tran\left(
        \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
        \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
        , |\vec{g} \oplus \vec{x}|
      \right)
    \end{array}
  \right\}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{alignat*}
\end{proof}
\end{landscape}

\begin{theorem}[Chain Rule]
\label{mmm_chain_rule}
Let $F(G)$ be a function $\mathbb{R}^{\vec{g}} \rightarrow \mathbb{R}^{\vec{f}}$
and $G(X)$ be a function $\mathbb{R}^{\vec{x}} \rightarrow \mathbb{R}^{\vec{g}}$.
Then,
\[
\frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{|\vec{g}|} \frac{dG}{dX}
\]
\end{theorem}
\begin{proof}
By the definition of a derivative (\ref{mm_derivative}),
\[
\forall \bar{f} \in \Dim(\vec{f}), \bar{g} \in \Dim(\vec{g}):
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
= \frac{\partial F(G)[\bar{f}]}{\partial G[\bar{g}]}
\]
\[
\forall \bar{g} \in \Dim(\vec{g}), \bar{x} \in \Dim(\vec{x}):
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
= \frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
By the scalar total derivative rule \cite{wiki:totalderiv},
\[
\frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{\partial F(G)[\bar{f}]}{\partial G[\bar{g}]}
\frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
Substituting in our above definitions this yields,
\[
\frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
And again, by the definition of a derivative (\ref{mm_derivative}),
\[
\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):
\frac{dF(G(X))}{dX}[\bar{f} \oplus \bar{x}]
= \frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
\]
Therefore,
\[
\frac{dF(G(X))}{dX}[\bar{f} \oplus \bar{x}]
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
Which, taking a look at the definition of multidimensional matrix multiplication
(\ref{mm_mult}), tells us that,
\[
\frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{|\vec{g}|} \frac{dG}{dX}
\]
\end{proof}

\chapter{Machine Learning Specific Derivatives}

\begin{displayquote}
``I'm not saying Machine Learning is a portal to a demon universe, I'm just saying
that some doors are best left unopened.'' - James Mickens, AKA `Galactic Viceroy of Research Excellence', USENIX Security 2018
\end{displayquote}

\begin{definition}[Sum]
Let $\Sum(X)$ be the function $\Sum : \mathbb{R}^{\vec{x}} \to \mathbb{R}$
such that \[ \Sum(X) = \sum_{\forall \bar{x} \in \Dim(\vec{x})} X[\bar{x}] \]
\end{definition}

\begin{theorem}[Derivative of Sum]
Let $X$ be any multimatrix, then,
\[\frac{d\Sum(X)}{dX} = 1^{|X|}\]
\end{theorem}
\begin{proof}
Let $|X| = \vec{x}$ and $\bar{x} = \Dim(\vec{x})$. Then,
\begin{align*}
  \frac{d\Sum(X)}{dX}[\bar{x}]
  &= \frac{\partial \Sum(X)}{\partial X[\bar{x}]} \\
  &= \frac{
    \partial \left(
      \sum_{\forall \bar{y} \in \Dim(\vec{x})} X[\bar{y}]
    \right)
  }{
    \partial X[\bar{x}]
  } \\
  &= \sum_{\forall \bar{y} \in \Dim(\vec{x})}
  \frac{
    \partial X[\bar{y}]
  }{
    \partial X[\bar{x}]
  } \\
  &= \left(
    \sum_{\forall \bar{y} \ne \bar{x}} 0
  \right)
  +
  \left(
    \sum_{\forall \bar{y} = \bar{x}}
    \frac{\partial X[\bar{y}]}{\partial X[\bar{x}]}
  \right) \\
  &= \frac{\partial X[\bar{x}]}{\partial X[\bar{x}]} \\
  &= 1 \\
  &= 1^{\vec{x}}[\bar{x}]
\end{align*}
So,
\[\frac{d\Sum(X)}{dX} = 1^{|X|}\]
\end{proof}

\begin{definition}[Sum of Squares]
Let $\SoS(X)$ be the function $\SoS : \mathbb{R}^{\vec{x}} \to \mathbb{R}$
such that 
\[ \SoS(X) = \sum_{\forall \bar{x} \in \Dim(\vec{x})} X[\bar{x}]^2 \]
\end{definition}

\begin{theorem}[Derivative of Sum of Squares]
Let $X$ be any multimatrix then,
\[ \frac{d \SoS(X)}{dX} = 2X \]
\end{theorem}
\begin{proof}
Let $|X| = \vec{x}$. Notice that $\SoS(X) = X \mmult{|\vec{x}|} X$ since,
$|X \mmult{|\vec{x}|} X| = \left<\right>$ and,
\[
  \left(X \mmult{|\vec{x}|} X\right)[\left<\right> \oplus \left<\right>]
  =
  \sum_{\forall \bar{x} \in \Dim(\vec{x})}
    X[\left<\right> \oplus \bar{x}]
    X[\bar{x} \oplus \left<\right>]
\]
Then, by the multiplication rule (Thm. \ref{multiplication_rule}),
\begin{align*}
  \frac{d\left(X \mmult{|\vec{x}|} X\right)}{dX}
  &=
  X \mmult{|\vec{x}|} \frac{dX}{dX} +
  \Tran\left(
    \Tran(X, |\vec{x}|)
      \mmult{|\vec{x}|}
    \Tran\left(\frac{dX}{dX}, |\vec{x}|\right),
    |\vec{x} \oplus \vec{x}|
  \right) \\
  &=
  X \mmult{|\vec{x}|} \Ident^2(|X|) +
  \Tran\left(
    X
      \mmult{|\vec{x}|}
    \Tran\left(\Ident^2(|X|), |\vec{x}|\right),
    |\vec{x} \oplus \vec{x}|
  \right) \\
  &=
  X +
  \Tran\left(
    X
      \mmult{|\vec{x}|}
    \Ident^2(|X|),
    |\vec{x} \oplus \vec{x}|
  \right) \\
  &=
  X +
  \Tran\left(
    X,
    |\vec{x} \oplus \vec{x}|
  \right) \\
  &= X + X \\
  &= 2X
\end{align*}
\end{proof}

\begin{definition}[Softmax]
Let $\Softmax(X)$ be the function
$\Softmax : \mathbb{R}^{\vec{X}} \to \mathbb{R}^{\vec{X}}$
such that $\forall \bar{x} \in \Dim(\vec{x}):$
\[
  \Softmax(X)[\bar{x}] = \frac{
		\exp(X[\bar{x}])
	}{
		\sum_{\forall \bar{c} \in \Dim(\vec{x})} \exp(X[\bar{c}])
	}
\]
\end{definition}

\begin{landscape}
\begin{theorem}[Derivative of Softmax]
\label{softmax_derivative}
\begin{align*}
	\frac{d\Softmax(X)}{dX}
	&=
	\Ident^3(|X|) \mmult{||X||} \Softmax(X)
  -\Softmax(X) \mmult{0} \Softmax(X) \\
	&=
	\left(\Ident^3(|X|) - \Softmax(X) \mmult{0} \Ident^2(|X|) \right)
	\mmult{||X||} \Softmax(X)
\end{align*}
\end{theorem}
\begin{proof}
Notice that 
\[
	\Softmax(X) = \exp(X) \mmult{0} \left( \Sum(\exp(X)) \right)^{-1}
\]
So,
\begin{alignat}{3}
	\frac{d\Softmax(X)}{dX}
	&=&&
	\frac{d \left( \exp(X) \mmult{0} \left( \Sum(\exp(X)) \right)^{-1} \right)}{dX} \\
	&=&&
	\exp(X) \mmult{0} \frac{d \left( \Sum(\exp(X)) \right)^{-1}}{dX}
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\left( \Sum(\exp(X)) \right)^{-1}
			,0
		\right)
		\mmult{0}
		\Tran\left(
			\frac{d \exp(X)}{dX}
			,|\vec{x}|
		\right)
		,|\left<\right> \oplus \vec{x}|
	\right) \\
	&=&&
	\exp(X) \mmult{0} \frac{d \left( \Sum(\exp(X)) \right)^{-1}}{dX}
	\nonumber\\&&&+
	\Tran\left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Tran\left(
			\frac{d \exp(X)}{dX}
			,|\vec{x}|
		\right)
		,|\vec{x}|
	\right) \\
	&=&&
	\exp(X) \mmult{0} \left(
		-\left( \Sum(\exp(X)) \right)^{-2}
		\mmult{0}
		\frac{d\Sum(\exp(X))}{dX}
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Tran\left(
			\Ident^3(|X|) \mmult{||X||} \exp(X)
			,|\vec{x}|
		\right)
		,|\vec{x}|
	\right) \\
	&=&&
	\exp(X) \mmult{0} \left(
		-\left( \Sum(\exp(X)) \right)^{-2}
		\mmult{0}
		1^{|X|}
		\mmult{||X||}
		\Ident^3(|X|)
		\mmult{||X||}
		\exp(X)
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\left( \Sum(\exp(X)) \right)^{-1}
			\mmult{0}
			\Ident^3(|X|) \mmult{||X||} \exp(X)
		,|\vec{x}|
		\right)
		,|\vec{x}|
	\right) \\
	&=&&
	-\Softmax(X) \mmult{0} \left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Ident^2(|X|)
		\mmult{||X||}
		\exp(X)
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\Ident^3(|X|) \mmult{||X||} \Softmax(X)
		,|\vec{x}|
		\right)
	,|\vec{x}|
	\right) \\
	&=&&
	-\Softmax(X) \mmult{0} \Softmax(X)
	+
	\Tran\left(
		\Ident^3(|X|) \mmult{||X||} \Softmax(X)
		,2|\vec{x}|
	\right) \\
	&=&&
	\Ident^3(|X|) \mmult{||X||} \Softmax(X)
  -\Softmax(X) \mmult{0} \Softmax(X) \\
	&=&&
	\Ident^3(|X|) \mmult{||X||} \Softmax(X)
	-\Softmax(X) \mmult{0} \Ident^2(|X|) \mmult{||X||} \Softmax(X) \\
	&=&&
	\left(\Ident^3(|X|) - \Softmax(X) \mmult{0} \Ident^2(|X|) \right)
	\mmult{||X||} \Softmax(X)
\end{alignat}
Let's just verify that against the Wikipedia \cite{wiki:softmax} result:
\begin{align*}
	\frac{\partial \Softmax(X)[\bar{x}_1]}{\partial X[\bar{x}_2]}
	&=
	\left\{
		\left(\Ident^3(|X|) - \Softmax(X) \mmult{0} \Ident^2(|X|) \right)
		\mmult{||X||} \Softmax(X)
	\right\}
	[\bar{x}_1 \oplus \bar{x}_2] \\
	&=
	\sum_{\forall \bar{x}_3}
	\left(\Ident^3(|X|) - \Softmax(X) \mmult{0} \Ident^2(|X|) \right)
	[\bar{x}_1 \oplus \bar{x}_2 \oplus \bar{x}_3]
	\Softmax(X)[\bar{x}_3] \\
	&=
	\sum_{\forall \bar{x}_3}
	\left(
		\delta_{\bar{x}_1 \bar{x}_2 \bar{x}_3}
		- \Softmax(X)[\bar{x}_1] \delta_{\bar{x}_2 \bar{x}_3}
	\right)
	\Softmax(X)[\bar{x}_3] \\
	&=
	(\delta_{\bar{x}_1 \bar{x}_2} - \Softmax(X)[\bar{x}_1])
	\Softmax(X)[\bar{x}_2] \\
	&= \left\{
  \begin{array}{ll}
    (1-\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    (0-\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= \left\{
  \begin{array}{ll}
    \Softmax(X)[\bar{x}_1]-\Softmax(X)[\bar{x}_1]^2
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    -\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= \left\{
  \begin{array}{ll}
    (1-\Softmax(X)[\bar{x}_2])\Softmax(X)[\bar{x}_2]
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    (0-\Softmax(X)[\bar{x}_2])\Softmax(X)[\bar{x}_1]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= (\delta_{\bar{x}_1 \bar{x}_2} - \Softmax(X)[\bar{x}_2])
	\Softmax(X)[\bar{x}_1]
\end{align*}
And it looks like we're good. I wrote this proof before backreferencing the wiki
page, so that's a good sign.
\end{proof}
\end{landscape}

\begin{definition}[Feed Forward Neural Net Layer]
Feed forward neural nets are described all over the place so I won't go into too
much detail here. In short, for every layer except the input layer there is some
previous layer which feeds into current layer via a fully-connected network of
weights.

Let $L_{i-1}$ be the activations of the the previous layer and $L_i$ the activations
of the current layer. Each of these can be modeled as a multimatrix. As an the weighted
connections between them, $W_i$. Finally, there is an activation function
$a(X) : \mathbb{R}^{|X|} \to \mathbb{R}^{|X|}$ which is an elementwise function,
often the sigmoid function, or ReLU, or some variant thereof. It's unimportant for this
definition.

\[ L_i = a(L_{i-1} \mmult{||L_{i-1}||} W_i) \]
\end{definition}

\begin{theorem}[Derivative of FFNN Layer]
\[ \frac{dL_i}{dW_j} = ? \]
\end{theorem}
\begin{proof}
\begin{ppart} When $i < j$,
$L_i$ does not depend on $W_j$, so 
$\forall \bar{x} \in \Dim(|L_i|), \bar{y} \in \Dim(|W_j|)$
$\frac{dL_i}{dW_j} = 0$
\end{ppart}
\begin{ppart} $i = j$
TODO
\end{ppart}
\begin{ppart} $i > j$
TODO
\end{ppart}
\end{proof}

\chapter{Integration}

I think, I'm not sure but I think, that the integral of a multimatrix function should
be defined,

\[
 \int_A^B F(X) dX = \lim_{n \to \infty}
 \sum_{i=1}^n
  F\left(A+\frac{i}{n}(B-A)\right)
  \mmult{||X||}
  \left(\frac{B-A}{n}\right)
\]

Which, I have very little idea what that means geometriclly but
if you let $G(X) = \int_A^X F(Y) dY$ you seem to end up with $dG/dX = F(X)$.

Maybe... well look at it this way. $F(X)$ is a function which produces some,
afine-like transformation multimatrix for multimatricies of shape $|X|$,
since it must have a right hand shape of $\ldots \oplus |X|$. This integration
occures over a linear path of $X$ as it moves from $A$ to $B$ applying that
transformation and adding up the results of the transformed multimatricies with
a weight proportional to the step size... and in the direction of the step.

So, it's sort of like a line integral but not really. They're related somehow.

\begin{landscape}
Let's see if I can prove that theory,
\begin{theorem}[Fundamental Rule of Multimatrix Calculus]
Let $G(X) = \int_A^X F(Y) dY$ then $dG(X)/dX = F(X)$.
\end{theorem}
\begin{proof}
\begin{align*}
  G(X)
  &= \int_A^X F(Y) dY \\
  &= 
    \lim_{n \to \infty}
     \sum_{i=1}^n
      F\left(A+\frac{i}{n}(X-A)\right)
      \mmult{||Y||}
      \left(\frac{X-A}{n}\right)
\end{align*}
\begin{alignat*}{3}
  \frac{dG(X)}{dX}
  &=&&
    \frac{
      d \left\{
      \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
        \mmult{||Y||}
        \left(\frac{X-A}{n}\right)
      \right\}
    }{dX} \\
  &=&&
    \lim_{n \to \infty}
    \frac{
      d \left\{
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
        \mmult{||Y||}
        \left(\frac{X-A}{n}\right)
      \right\}
    }{dX} \\
  &=&&
    \lim_{n \to \infty}
    \sum_{i=1}^n
      \frac{
        d \left\{
          F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \left(\frac{X-A}{n}\right)
        \right\}
      }{dX} \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \frac{d \left(\frac{X-A}{n}\right)}{dX}
        \\&&&+
        \Tran\left(
          \Tran\left(\frac{X-A}{n}, ||Y||\right)
          \mmult{||Y||}
          \Tran\left(\frac{
            d \left\{F\left(A+\frac{i}{n}(X-A)\right)\right\}
          }{dX}, |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right) \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \frac{1}{n}\Ident^2(|X|)
        \\&&&+
        \Tran\left(
          \frac{X-A}{n}
          \mmult{||Y||}
          \Tran\left(
            \frac{dF(X)}{dX}
            \mmult{|X|}
            \frac{i}{n}\Ident^2(|X|), |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right) \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        \frac{1}{n}F\left(A+\frac{i}{n}(X-A)\right)
        \\&&&+
        \Tran\left(
          \frac{X-A}{n}
          \mmult{||Y||}
          \Tran\left(
            \frac{i}{n}\frac{dF(X)}{dX}
            , |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right)
\end{alignat*}
At this point I'm fairly stuck.

Ok, lets think of this in another way. Is this true?
\[
	\lim_{\epsilon \to 0}
	\frac{
		G(X+\epsilon H) - G(X)
	}{\epsilon}
	=
	F(X) \mmult{||X||} H
\]
Where $SS(H) = 1$. More specifically, $H = \frac{X-A}{\sqrt{SS(X-A)}}$

\begin{align*}
\lim_{\epsilon \to 0}
\frac{
	G(X+\epsilon H) - G(X)
}{\epsilon}
&= 
\lim_{\epsilon \to 0}
\frac{
	\left( \int_A^{X+\epsilon H} F(Y) dY \right) - \left( \int_A^X F(Y) dY \right)
}{\epsilon} \\
&= 
\lim_{\epsilon \to 0}
\frac{
	\int_X^{X+\epsilon H} F(Y) dY
}{\epsilon}
\end{align*}
\end{proof}
\end{landscape}

\begin{appendices}

\chapter{Solutions to the Exercises}

\solutionsection{1}

\begin{solution}
\begin{enumerate}
\item[]
\item True
\item False, $\Dim(\left<5,5,5\right>)$ contains only vectors in $\mathbb{N}^3$ 
\item False, the last value of $\left<1,2,3\right>$, $3$, is greater than it's
						 corresponding value in $\left<3,2,1\right>$, which is $1$.
\item True
\end{enumerate}
\end{solution}

\begin{solution}
\[ \left|\frac{dF}{dX}\right| = \left<6,7,3,7,2\right> \]
\end{solution}

\begin{solution}
\[ \left|\frac{df}{dX}\right| = |X| = \left<2,3,2\right> \]
\[ \frac{df}{dX}[\bar{v}] = 1 \]
\end{solution}

\begin{solution}
\[ \left|\frac{df}{dX}\right| = |X| = \left<2,3,2\right> \]
\[ \frac{df}{dX}[\bar{v}] = 1 \]
\end{solution}

\solutionsection{2}

\begin{solution}
\begin{enumerate}
\item[]
\item Invalid
\item $\left<3, 3\right>$
\item Invalid
\item $\left<1, 2, 2, 1\right>$
\end{enumerate}
\end{solution}

\chapter{Derivative Rule List}

\begin{drule}
Self Derivative (Thm. \ref{self_derivative})
\[ \frac{dX}{dX} = \Ident^2(|X|) \]
\end{drule}

\begin{drule}
Elementwise Derivative (Thm. \ref{elementwise_derivative})
\[ \frac{dM(X)}{dX} = \Ident^3(|X|) \mmult{||X||} M'(X) \]
\end{drule}

\begin{drule}
Empty Shape Multimatrix and Scalar Domain Derivative Equivalence
(Thm. \ref{s_mm_domain_equiv})
\[ G(X) = F(X[\left<\right>]) \]
Implies,
\[ \frac{dF(x)}{dx} = \frac{dG(X)}{dX} \]
\end{drule}

\begin{drule}
Empty Shape Multimatrix and Scalar Range Derivative Equivalence
(Thm. \ref{s_mm_range_equiv})
\[ F(X)[\left<\right>] = f(X) \]
Implies,
\[ \frac{df(X)}{dX} = \frac{dF(X)}{dX} \]
\end{drule}

\begin{drule}
Transpose Derivative
(Thm. \ref{tran_derivative})
\[ |X| = \vec{l} \oplus \vec{r} \]
Implies,
\[
 \frac{d\Tran(X, |\vec{l}|)}{dX} =
 \Tran(\Ident^2(\vec{r}) \mmult{0} \Ident^2(\vec{l}), |\vec{r}|)
\]
\end{drule}

\begin{drule}
Right Multiplicative Derivative
(Thm. \ref{right_mult_derivative})
\[ \frac{d(A \mmult{n} X)}{dX} = A \mmult{n} \Ident^2(|X|) \]
\end{drule}

\begin{drule}
Left Multiplicative Derivative
(Thm. \ref{left_mult_derivative})
\[ \frac{d(X \mmult{n} A)}{dX} = \Ident^2(|X \mmult{n} A|) \mmult{n} \Tran(A, n) \]
\end{drule}

\begin{drule}
Addition Rule
(Thm. \ref{addition_drule})
\[ \frac{d(F(X) + G(X))}{dX} = \frac{dF(X)}{dX} + \frac{dG(X)}{dX} \]
\end{drule}

\begin{drule}
Scalar Multiplication Rule
(Thm. \ref{scalar_mult_drule})
\[ \frac{d(sF(X))}{dX} = s\frac{dF(X)}{dX} \]
\end{drule}

\begin{drule}
Multiplication Rule
(Thm. \ref{multiplication_rule})

Let $|F| = \vec{f} \oplus \vec{c}$, $|G| = \vec{c} \oplus \vec{g}$,
and $|X| = \vec{x}$. Then,
\begin{align*}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX} =&
 F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} + \\
 &\Tran\left(
   \Tran(G(X), |\vec{c}|)
     \mmult{|\vec{c}|}
   \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right),
   |\vec{g} \oplus \vec{x}|
 \right)
\end{align*}
\end{drule}

\begin{drule}
Chain Rule
(Thm. \ref{mmm_chain_rule})
\[ \frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{||G||} \frac{dG}{dX} \]
\end{drule}

\end{appendices}

\bibliography{book}
\bibliographystyle{ieeetr}

\end{document}
