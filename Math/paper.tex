\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{pdflscape}
\usepackage{etoolbox}

\title{Multimatrices and Their Derivatives}
\author{Ernest Kirstein}
\date{\today}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newtheoremstyle{ppart}{}{}{}{}{}{:}{ }{}
\theoremstyle{ppart}
\newtheorem{ppart}{Part}
\AtBeginEnvironment{proof}{\setcounter{ppart}{0}}

\DeclareMathOperator{\Dim}{Dim}
\DeclareMathOperator{\Ident}{I}
\DeclareMathOperator{\Tran}{Tran}
%\newcommand{\mmult}[1]{\stackrel{\times}{#1}}
%\newcommand{\mmult}[1]{\underset{#1}{\times}}
\newcommand{\mmult}[1]{\text{\raisebox{1ex}{$\underset{#1}{\times}$}}}


\begin{document}

\maketitle

\begin{abstract}
I was working my way through the implementation of a machine learning system as
a hobby project and I was having a hell of a time wrapping my head around the
existing notation for derivatives of matrix equations. Every notation system
either requires users to pull apart the matricies they're working with or otherwise
bypass the issue of taking derivatives of matrix equations.

So, I came up with a system of my own. And, much to my surprise, some of the
results I came up with were surprisingly elegant. For example, I proved a
multidimensional version of the derivative chain rule (Thm. \ref{mmm_chain_rule})
that's more consise than anything I've found in other works.

This book is about my exploration into multidimensional matricies - multimatricies -
and how they can simplify derivatives of matrix equations.
\end{abstract}

\section{Defining the Derivative}

Let's consider a traditional matrix equation.
We'll define some matricies $M$ and $X$ such that
$|M| = |X| = \left< 3,5 \right>$
and \[M_{r,c} = \sin(X_{r,c})\]
Or more simply, \[M = \sin(X)\]

\noindent
The partial derivatives of this equation are simple enough. Assuming that all elements
of $X$ are independent,
\[
\frac{\partial M_{r_1,c_1}}{\partial X_{r_2,c_2}} = 
\left\{
  \begin{array}{ll}
    \cos(X_{r_2,c_2})  & \mbox{if } r_1 = r_2 \mbox{ and } c_1 = c_2 \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\]
Which can also be writen more simply using Kronecker deltas,
\[
\frac{\partial M_{r_1,c_1}}{\partial X_{r_2,c_2}} = 
\cos (X_{r_2, c_2}) \delta_{r_1 r_2} \delta_{c_1 c_2}
\]

\noindent
But what about the complete deriative? What would that even mean? What sort of
mathmatical object would represent the entire derivative of $M$ with respect to $X$.

\[\frac{dM}{dX} = \mbox{ ? }\]

If this were a function that mapped a vector to a scalar, you might say that the
complete derivate was the gradient. But in this case, our function's output is more
than a simple scalar and even our dependent variable is more complex than a vector.

By looking at the partial derivatives, we can come to some understanding of the
dimensionality of the complete derivative. Each combination of input and output
element has its own partial derivative, so it makes sense that the total size
of the complete derivative should be the product of the size of $M$ and the size
of $X$.

And so, the size of the complete derivate would be,
\[
\mbox{size}\left(\frac{dM}{dX}\right) = \mbox{size}(M) \mbox{size}(X)
\]
And perhaps we could even let this help us define the 'shape' of the complete
derivative. This complete derivative would need to be composed of partial
derivatives, so it would make sense to just smush the shapes of the input
matrix and the output matrix together,
\begin{align*}
\left|\frac{dM}{dX}\right| &= |M| \oplus |X| \\
 &= \left< 3, 5 \right> \oplus \left< 3, 5 \right> \\
 &= \left< 3, 5, 3, 5 \right>
\end{align*}
Where $\oplus$ is the concatenate operator.

The ordering here is arbitrary, there is no reason why the dimensionality of
of the independent variable \textit{needs} to be concatenated with to the right of the
dimensionality of the output. What's important is that the dimensionality of
both output and indepenedent variable are preserved - but let us declare this to
be our convention for the remainder of this article.

We have, therefore, a multidimensional object with four indicies. Suppose we try to
pull a single partial derivative from this complete derivative, what could our
notation look like? Four underscript indicies would be unwieldy, so maybe something
like this:
\[
\frac{dM}{dX}[1,2,3,4] = \frac{\partial M_{1,2}}{\partial X_{3,4}}
\]

And since we're defining our own notation, let's go ahead and define our original
matricies in a similar manner:
\[
\frac{dM}{dX}[1,2,3,4] = \frac{\partial M[1,2]}{\partial X[3,4]}
\]

So what is this object we've defined? In all ways that matter, it seams to be
a multidimensional matrix. It has four dimensions of indices, rather than just
a row and column, but each uniquely indexed cell in this object may have a
unique value, just like a standard two dimensional matrix.

\begin{definition}[Multimatrix]
Let's define a bit of notation. A multimatrix $M$ is a multidimensional matrix
that has some shape, $|M| \in \mathbb{N}^n$, where $n$ is the number of
dimensions of $M$.

We'll say that if $\vec{v} = |M|$, then any $\bar{v} \in \mathbb{N}^n$ is a valid
index of $M$ if and only if value of $\bar{v}$ is less than or equal to the value
of it's parallel index in $\vec{v}$. That is, $\forall i: 1 \le \bar{v}_i \le \vec{v}_i$.

We can further simplify that notation for our set of valid indicies. Let's say we have
some $\vec{v} \in \mathbb{N}^n$ that defines the shape of some matrix. Then well
define the set of valid indicies of that matrix to be $\Dim(\vec{v})$. 

Each value of $M$ is a real value, so we'll define the set of all real valued
multidimensional matricies with shape $\vec{v}$ as $\mathbb{R}^{\vec{v}}$.

We'll use the notation $M[\bar{v}]$ to reference any individual scalar value of $M$
at indicies $\bar{v} \in \Dim(|M|)$. 
\end{definition}

Going back to our original motivating problem, we can express our derivative
notation thusly,

\begin{definition}[Multimatrix on Multimatrix Derivative]
\label{mm_derivative}
Let $F(X)$ be some function from $\mathbb{R}^{\vec{x}}$ to $\mathbb{R}^{\vec{f}}$.
The complete derivative of $F$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{f} \oplus \vec{x}}$. That is,
\[ |F(X)| = \vec{f} \]
\[ |X| = \vec{x} \]
\[ \left|\frac{dF}{dX}\right| = \vec{f} \oplus \vec{x} \]
\[
\forall \bar{f} \in \Dim(\vec{f}),
        \bar{x} \in \Dim(\vec{x}):
\]
\[
\frac{dF}{dX}[\bar{f} \oplus \bar{x}] =
\frac{\partial F[\bar{f}]}{\partial X[\bar{x}]}
\]
\end{definition}

But, of course, there are other combinations of functions than from one
multimatrix to another. You might have a function from a multimatrix to a
scalar (such as summing up all the values of a multimatrix), or from a scalar
to a multimatrix (such as filling a multimatrix with a single scalar value).

We can define the total derivative for those cases as well.

\begin{definition}[Scalar on Multimatrix Derivative]
\label{sm_derivative}
Let $f(X)$ be some function from $\mathbb{R}^{\vec{x}}$ to $\mathbb{R}$.
The complete derivative of $f$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{x}}$. That is,
\begin{align*}
f(X) &\in \mathbb{R} \\
|X| &= \vec{x} \\
\left|\frac{df}{dX}\right| &= \vec{x}
\end{align*}
\[
\forall \bar{x} \in \Dim(\vec{x}):
        \frac{df}{dX}[\bar{x}] =
        \frac{\partial f}{\partial X[\bar{x}]}
\]
\end{definition}

\begin{definition}[Multimatrix on Scalar Derivative]
\label{ms_derivative}
Let $F(x)$ be some function from $\mathbb{R}$ to $\mathbb{R}^{\vec{f}}$.
The complete derivative of $F$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{f}}$. That is,
\begin{align*}
|F(x)| &= \vec{f} \\
x &\in \mathbb{R} \\
\left|\frac{dF}{dx}\right| &= \vec{f}
\end{align*}
\[
\forall \bar{f} \in \Dim(\vec{f}):
        \frac{dF}{dx}[\bar{f}] =
        \frac{\partial F[\bar{f}]}{\partial x}
\]
\end{definition}

Ok, that's neat and all but what does that actually mean? Can we answer our motivating 
question?
\[\frac{dM}{dX} = ? \]
Well... sort of. We can say that the total derivative of $M$ with respect to $X$ is
a matrix with shape $|M| \oplus |X|$, who's elements we can define as,
\[\forall \bar{m} \in \Dim(|M|), \bar{x} \in \Dim(|X|):\]
\begin{align*}
\left( \frac{dM}{dX} \right)[\bar{m} \oplus \bar{x}]
&= \frac{\partial M[\bar{m}]}{\partial X[\bar{x}]} \\
&= \delta_{\bar{m}\bar{x}}\cos(\bar{x})
\end{align*}

Which isn't a huge improvement over our original notation. What would be great is
if we could define the total derivative without going down into the partial derivatives
at all, which we'll be able to do in later sections. As a sneak peak, that'll look
like this:
\[
\frac{dM}{dX} = \Ident^3(|X|) \mmult{||X||} \cos(X)
\]

But that single line will require defining something I'm going to call a cubic
identity, $\Ident^3(|X|)$, as well as multimatrix multiplication.

\subsection*{Sample Problems}
TODO

\section{Multimatrix Multiplication}

Multiplication of traditional matricies is one of those things that, when you
first learn about it, you wonder "Who the hell defined it this way?" And then
when you see it used in practice, you slowly come to understand they why behind
it.

Just to review, for two matricies $A$ and $B$, multipication is only defined
for $A \times B = C$ if $A$ has the same number of columns as $B$ has rows.
That is, if $|A| = \left< r_a, c_a \right>$ and $|B| = \left< r_b, c_b \right>$
then $A \times B$ is valid if and only if $c_a = r_b$. And furthermore,
the remaining dimensions of $A$ and $B$ define the diminsions of $C$,
$|C| = \left< r_a, c_b \right>$. Each element of $C$ is a sum of the product of
every combination of the corresponding row of $A$ and the corresponding column of
$B$,

\[ C[r_a, c_b] = \sum_{\forall i} A[r_a, i] B[i, c_b] \]

So now consider how we would multiply two matricies $A$ and $B$ if they had more
than two dimensions. Let's use a concrete example to make the problems more apparent.
Let $|A| = \left<5,7,7\right>$ and $|B| = \left<7,7,2,4\right>$.
First of all, can we even multiply the two together? Hm... maybe? $|A|$ ends with
$\left<\ldots, 7\right>$ and $|B|$ starts with $\left<7,\ldots\right>$ so it would
make sense that we can somehow squish them together into something. By that reasoning,
the shape of our after-multiplication output would use the remaining dimensions
$\left<5, 7, 7, 2, 4\right> = |A \times B|$.

But $|A|$ also ends with $\left<\ldots, 7, 7\right>$ and $|B|$ starts with
$\left<7, 7\ldots\right>$ so maybe our multiplication operation should work on that
additional dimension, leaving us with $\left<5, 2, 4\right> = |A \times B|$.
But the first definition looks like it could still be useful! It could let us cover 
two dimensional matrix multiplication completely within our definition. This second
definition seems more... multidimensional but it also seems to arbitrarily depend
on the shape of the two matricies.

So - a compromize. Maybe multimatrix isn't a binary operation. Maybe it's a trinary
operation. There's some component of, ``How many dimensions are you combining?'' inherent
in the operation. So we can distinguish our two types of multiplication, and indeed
an infinite number of types of multiplication, with a new operator, $\mmult{n}$ where
$|A \mmult{1} B| = \left<5, 7, 7, 2, 4\right>$ and 
$|A \mmult{2} B| = \left<5, 2, 4\right>$.

But the shape is only a small component of the multipliation  operation. Let's
just jump into the formal definition and I'll explain with an example:

\begin{definition}[Multimatrix Multiplication]
\label{mm_mult}
Let $A$ and $B$ be multimatricies such that $|A| = \vec{a} \oplus \vec{c}$,
$|B| = \vec{c} \oplus \vec{b}$, and $|\vec{c}| = n$ with
$n \in \left\{ 0 \right\} \cup \mathbb{Z}^{+}$. We shall call $n$ the 'size'
of the multiplication operation.
The shape of the result is defined, $|A \mmult{n} B| = \vec{a} \oplus \vec{b}$
and the value of the result is defined,
\[
\forall \bar{a} \in \Dim(\vec{a}), \bar{b} \in \Dim(\vec{b}):
(A \mmult{n}  B)[\bar{a} \oplus \bar{b}] =
\sum_{\forall \bar{c} \in \Dim(\vec{c})}
  A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}]
\]
\end{definition}

Like two dimensional matrix multiplication, multimatrix multiplication is
\textit{not} communicative. And it is only weakly associative. That sounds like
nonsense, but here's what I mean.

Multimatrix multiplication may not even make sense dimensionally when you move
around parentheses. Take for example multimatricies $A$, $B$, and $C$ such that,
$|A| = \left<5,3,2\right>$,
$|B| = \left<2,7\right>$, and
$|C| = \left<3,7,2\right>$ where you should be able to see that
$(A \mmult{1} B) \mmult{2} C$ is a valid operation since
$|A \mmult{1} B| = \left<5,3,7\right>$ and
$|(A \mmult{1} B) \mmult{2} C| = \left<5,7\right>$. But
$A \mmult{1} (B \mmult{2} C)$ isn't a valid set of operations since even
$B \mmult{2} C$ is an invalid operation.

But even when the dimensional analysis makes sense, multimatrix multiplication
may not be associative. Take, for example, multimatricies $A$, $B$, and $C$ such
that $|A| = \left<2,2,2,2\right>$, $|B| = \left<2,2,2\right>$, and
$|C| = \left<2,2,2,2\right>$. Let,
\[
 A[\bar{a}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{a} = \left<1,1,1,2\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
\[
 B[\bar{b}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{b} = \left<1,2,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
And
\[
 C[\bar{c}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{c} = \left<1,1,1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
Then,
\[
 (A \mmult{2} B)[\bar{x}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{x} = \left<1,1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
So,
\[
 ((A \mmult{2} B) \mmult{2} C)[\bar{x}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{x} = \left<1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
But, $(B \mmult{2} C)[\bar{x}] = 0$ so $(A \mmult{2} (B \mmult{2} C))[\bar{x}] = 0$
Which goes to show that even though both $(A \mmult{2} B) \mmult{2} C$ and
$A \mmult{2} (B \mmult{2} C)$ are valid calculations which result in multimatrixes
of the same dimensionality, they are not equal so multiplicative associativity
doesn't hold.

However, there are a broad catagory of cases where associativity \textit{does} hold:

\begin{theorem}[Associativity of Multimatrix Multiplication]
\label{mm_associativity}
Let $A$, $B$, and $C$ be multimatricies and $m$ and $n$ in
$\left\{ 0 \right\} \cup \mathbb{Z}^{+}$.
If both operations $(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$
are valid dimensionally and $m+n \le ||B||$, then,
\[ (A \mmult{m} B) \mmult{n} C = A \mmult{m} (B \mmult{n} C) \]
\end{theorem}
\begin{proof}
Let $A$, $B$, and $C$ be matricies such that the calculations
$(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$ are both valid
and $m + n \le ||B||$.
Then the following must be true by the definiton of multimatrix multiplication
(Def. \ref{mm_mult}).

\noindent
Since $A \mmult{m} B$ is defined,
\[
 \exists \vec{a}, \vec{m}, \vec{b_1} :
 |\vec{m}| = m \land |A| = \vec{a} \oplus \vec{m} \land |B| = \vec{m} \oplus \vec{b_1} 
\]
Since $B \mmult{n} C$ is defined,
\[
 \exists \vec{b_2}, \vec{n}, \vec{c} :
 |\vec{n}| = n \land |B| = \vec{b_2} \oplus \vec{n} \land |C| = \vec{n} \oplus \vec{c}
\]
Since $|B| = \vec{m} \oplus \vec{b_1} = \vec{b_2} \oplus \vec{n}$ and
$||B|| \ge |\vec{m}| + |\vec{n}|$, it must follow that,
\[
 \exists \vec{b} :
 |B| = \vec{m} \oplus \vec{b} \oplus \vec{n}
\]
Even if $\vec{b}$ is an empty vector. Then,
\[ |A \mmult{m} B| = \vec{a} \oplus \vec{b} \oplus \vec{n} \]
\[ |(A \mmult{m} B) \mmult{n} C| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
\[ |B \mmult{n} C| = \vec{m} \oplus \vec{b} \oplus \vec{c} \]
\[ |A \mmult{m} (B \mmult{n} C)| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
So,
\[ |(A \mmult{m} B) \mmult{n} C| = |A \mmult{m} (B \mmult{n} C)| \]
Which is necessary, but not sufficient, for our proof.

Looking at each element of the result matricies, let $j$ and $k$ be the values
of some parallel cells in $(A \mmult{m} B) \mmult{n} C$ and
$A \mmult{m} (B \mmult{n} C)$ respectively. Then for some indicies of those cells,
$\bar{a} \oplus \bar{b} \oplus \bar{c} \in
\Dim(\vec{a} \oplus \vec{b} \oplus \vec{c})$,
\begin{align*}
 j
 &= ((A \mmult{m} B) \mmult{n} C)[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
 (A \mmult{m} B)[\bar{a} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n}}
 \left(
  \sum_{\forall \bar{m} \in \Dim(\vec{m})}
  A[\bar{a} \oplus \bar{m}]B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 \right)
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n}, \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{m}, \bar{n}}
 A[\bar{a} \oplus \bar{m}]
 B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 \left(
 \sum_{\forall \bar{n}}
  B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
  C[\bar{n} \oplus \bar{c}]
 \right) \\
 &= \sum_{\forall \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 (B \mmult{n} C)[\bar{m} \oplus \bar{b} \oplus \bar{c}] \\
 &= (A \mmult{m} (B \mmult{n} C))[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
 &= k
\end{align*}
Since $(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$ are both
the same shape and every cell with the same index in each is equal, the two
values must be equal.
\end{proof}

Multiplicative identities for multimatricies depend on the shape of the
multimatrix they are being multiplied against, just as for regular matricies.
However, the multiplicative identity also greatly depends on the size of
the particular multiplication operation. 

\begin{definition}[Multimatrix Identities]
\label{mm_mult_ident}
Let us define $\Ident^n(\vec{v})$ as the $n^{\text{th}}$ order identity with base shape
$\vec{v}$. This $\Ident^n(\vec{v})$ is a multimatrix with shape,
\[ |\Ident^n(\vec{v})| = \bigoplus_{i \in [1, n]} \vec{v} \]
That is, it's shape is $n$ successive concatinations of $\vec{v}$.
Let each element of $\Ident^n(\vec{v})$ is defined as,
\[ \forall \bar{v_1}, \bar{v_2}, \ldots, \bar{v_n} \text{ each in } \Dim(\vec{v}) : \]
\[
 \Ident^n(\vec{v})[\bar{v_1}, \bar{v_2}, \ldots, \bar{v_n}]
 = \left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{v_1} = \bar{v_2} = \ldots = \bar{v_n} \\
    0 & \mbox{otherwise}
  \end{array}
 \right.
\]

In particular, let us call $\Ident^2(\vec{v})$ the square identity of $\vec{v}$ and
$\Ident^3(\vec{v})$ the cubic identity of $\vec{v}$.
\end{definition}

That definition seems rather arbitrary, but you'll see various orders of identity
pop up in derivatives down the line. For now, let's just look at how the
square identity it relates to a multiplication operation.

\begin{theorem}[Multimatrix Multiplicative Identity]
\label{mm_ident}
For all $\vec{m}$ and $\vec{n}$ on which the operations are valid,
\[
 A \mmult{|\vec{m}|} \Ident^2(\vec{m}) = A
\]
and
\[
 \Ident^2(\vec{n}) \mmult{|\vec{n}|} A = A
\]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $A$ be a matrix with shape $|A| = \vec{a} \oplus \vec{m}$.
Then by the definition of multimatrix multiplication (Def. \ref{mm_mult}),
\[ \forall \bar{a} \in \Dim(\vec{a}), \bar{m_1} \in \Dim(\vec{m}) : \]
\[
 (A \mmult{|\vec{m}|} \Ident^2(\vec{m}))[\bar{a} \oplus \bar{m_1}]
 =
 \sum_{\forall \bar{m_2} \in \Dim(\vec{m})}
 A[\bar{a} \oplus \bar{m_2}] \Ident^2(\vec{m})[\bar{m_2} \oplus \bar{m_1}]
\]
Then, by the definition of $\Ident^2(\vec{m})$ (Def. \ref{mm_ident}) it follows
that,
\begin{align*}
 (A \mmult{|\vec{m}|} \Ident^2(\vec{m}))[\bar{a} \oplus \bar{m_1}]
 &=
 \left(
  \sum_{\forall \bar{m_2} \ne \bar{m_1}}
  A[\bar{a} \oplus \bar{m_2}] (0)
 \right)
 +
 \left(
  \sum_{\forall \bar{m_2} = \bar{m_1}}
  A[\bar{a} \oplus \bar{m_2}] (1)
 \right) \\
 \\
 &= 0 + A[\bar{a} \oplus \bar{m_1}] \\
 &= A[\bar{a} \oplus \bar{m_1}]
\end{align*}
Therefore $A \mmult{|\vec{m}|} \Ident^2(\vec{m}) = A$
\end{ppart}
\begin{ppart}
Let $A$ be a matrix with shape $|A| = \vec{n} \oplus \vec{a}$.
Then by the definition of multimatrix multiplication (Def. \ref{mm_mult}),
\[ \forall  \bar{n_1} \in \Dim(\vec{n}), \bar{a} \in \Dim(\vec{a}) : \]
\[
 (\Ident^2(\vec{n}) \mmult{|\vec{n}|} A)[\bar{n_1} \oplus \bar{a}]
 =
 \sum_{\forall \bar{n_2} \in \Dim(\vec{n})}
 \Ident^2(\vec{n})[\bar{n_1} \oplus \bar{n_2}]A[\bar{n_2} \oplus \bar{a}] 
\]
Then, by the definition of $\Ident^2(\vec{n})$ (Def. \ref{mm_ident}) it follows
that,
\begin{align*}
 (\Ident^2(\vec{n}) \mmult{|\vec{n}|} A)[\bar{n_1} \oplus \bar{a}]
 &=
 \left(
  \sum_{\forall \bar{n_2} \ne \bar{n_1}}
  (0) A[\bar{n_2} \oplus \bar{a}]
 \right)
 +
 \left(
  \sum_{\forall \bar{n_2} = \bar{n_1}}
  (1) A[\bar{n_2} \oplus \bar{a}]
 \right) \\
 \\
 &= 0 + A[\bar{n_1} \oplus \bar{a}] \\
 &= A[\bar{n_1} \oplus \bar{a}]
\end{align*}
Therefore $\Ident^2(\vec{n}) \mmult{|\vec{n}|} A = A$
\end{ppart}
\end{proof}

The square identity also pops up in the derivative of a multimatrix
with respect to itself.

\begin{theorem}[Self Derivative]
Let $X$ be a multimatrix with independent elements. Then,
\[ \frac{dX}{dX} = \Ident^2(|X|) \]
\end{theorem}
\begin{proof}
Let $X$ be a multimatrix of shape $|X|=\vec{x}$ with independent elements.
By the derivative definition (Def. \ref{mm_derivative}),
\[
 \forall \bar{x_1} \in \Dim(\vec{x}),
         \bar{x_2} \in \Dim(\vec{x}):
\]
\[
 \frac{dX}{dX}[\bar{x_1} \oplus \bar{x_2}] = 
 \frac{\partial X[\bar{x_1}]}{\partial X[\bar{x_2}]}
\]
Since each element of $X$ is independent with respect to itself,
\begin{align*}
 \frac{dX}{dX}[\bar{x_1} \oplus \bar{x_2}]
 &= \left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{x_1} = \bar{x_2} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \Ident^2(\vec{x})[\bar{x_1} \oplus \bar{x_2}]
\end{align*}
Therefore $dX/dX = \Ident^2(|X|)$
\end{proof}

The cubic identity, strangely enough, pops up in the derivative of elementwise
functions.

\begin{theorem}[Elementwise Derivative]
Let $M$ be an element wise function on $X$. That is, Let $M(X)$ be a function
such that,
\[ M(X) : \mathbb{R}^{\vec{x}} \to \mathbb{R}^{\vec{x}} \]
And which applies the same scalar function to each element of $X$, 
\[ M(X)[\bar{x}] = m(X[\bar{x}]) \]
Also, let $M'$ be the elementwise function on $X$ which applies the derivative
of $M$'s associated scalar function to each element of $X$. That is,
\[ M'(X)[\bar{x}] = m'(X[\bar{x}]) \]
Then,
\[ \frac{dM}{dX} = \Ident^3(|X|) \mmult{||X||} M'(X) \]
\end{theorem}
\begin{proof}
By the definition of a multimatrix derivative,
\[ \forall \bar{m} \in \Dim(|M|), \bar{x} \in \Dim(|X|) : \]
\begin{align*}
 \frac{dM}{dX}[\bar{m} \oplus \bar{x}]
 &= \frac{\partial M[\bar{m}]}{\partial X[\bar{x}]} \\
 &= \frac{\partial m(X[\bar{m}])}{\partial X[\bar{x}]} \\
 &= \left\{
  \begin{array}{ll}
    m'(X[\bar{m}]) & \mbox{if } \bar{m} = \bar{x} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \delta_{\bar{m} \bar{x}} m'(X[\bar{m}])
\end{align*}
Since, by our definition of elementwise function, $|M| = |X|$, we can
say that, 
\begin{align*}
 (\Ident^3(|X|) \mmult{||X||} M'(X))[\bar{m} \oplus \bar{x}]
 &= \sum_{\forall \bar{y} \in \Dim(|X|)}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] M'(X)[\bar{y}] \\
 &= \sum_{\forall \bar{y} \in \Dim(|X|)}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] m'(X[\bar{y}]) \\
 &= \sum_{\forall \bar{y} = \bar{m} = \bar{x}}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] m'(X[\bar{y}]) \\
 &= \left\{
  \begin{array}{ll}
    m'(X[\bar{m}]) & \mbox{if } \bar{m} = \bar{x} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \delta_{\bar{m} \bar{x}} m'(X[\bar{m}])
\end{align*}
Therefore $dM/dX = \Ident^3(|X|) \mmult{||X||} M'(X)$
\end{proof}

Multimatrix identities also tend to cancel eachother out when multiplied by
multiples of their base shape lengths.

\begin{landscape}
\begin{theorem}[Identity Contraction]
For all $m, n, k \in \mathbb{Z}^+$ and $\vec{x}$ as a valid multimatrix shape,
where $m > k$ and $n > k$,
\[ \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) = \Ident^{m+n-2k}(\vec{x}) \]
\end{theorem}
\begin{proof}
Consider that for some indicies
$\{\bar{a_1}, \bar{a_2}, \ldots \bar{a}_{m-k}\}$
and $\{\bar{b_1}, \bar{b_2}, \ldots \bar{b}_{n-k}\}$ where each
$\bar{a_i} \in \Dim(\vec{x})$ and each $\bar{b_i} \in \Dim(\vec{x})$,

\begin{align*}
&\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] = \\
&\sum_{\forall \bar{c_1}, \bar{c_2}, \ldots \bar{c_k} \in \Dim(\vec{x})}
\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c_1} \oplus \bar{c_2} \oplus \ldots]
\Ident^n(\vec{x})
[\bar{c_1} \oplus \bar{c_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots]
\end{align*}

Consider that $\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c_1} \oplus \bar{c_2} \oplus \ldots]$
will be 0 whenever the any $\bar{c_i} \ne \bar{c_j}$ - we can simplify that to all
the same $c$.

\begin{align*}
&\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] = \\
&\sum_{\forall \bar{c} \in \Dim(\vec{x})}
\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c} \oplus \bar{c} \oplus \ldots]
\Ident^n(\vec{x})
[\bar{c} \oplus \bar{c} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots]
\end{align*}

Consider also that there can be at most one $\bar{c}$ for which
$\bar{c} = \bar{a_1} = \bar{a_2} \ldots$. And since our right hand side is
the sum of products of ones and zeros, we can therefore conclude it is either 1 or 0.

We can futher reason that the right hand side can only be one when all of the
$\bar{a_i}$ and all of the $\bar{b_i}$ equal $\bar{c}$. Furthermore, if all
$\bar{a_i}$ and all $\bar{b_i}$ are equal then that $\bar{c}$ must exist since it can
be any value in the range of all $\bar{a_i}$ and $\bar{b_i}$ and this is not a
zero width multiplication ($k >= 1$). Therefore,

\begin{align*}
\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] =
\left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{a_1} = \bar{a_2} = \ldots = \bar{b_1} = \bar{b_2} = \ldots \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\end{align*} 

Which is an identity matrix! And since the dimensional rules of multimatrix 
multiplication show that,

\[
 \left|\left| \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right|\right|
 =
 ||\Ident^m(\vec{x})|| + ||\Ident^n(\vec{x})|| - 2k|\vec{x}|
\]

We've shown that,

\[ \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) = \Ident^{m+n-2k}(\vec{x}) \]
\end{proof}
\end{landscape}

\subsection*{Sample Problems}
TODO

\section{Other Operations With Multimatricies}
Other operations include...

\begin{definition}[Multimatrix Scalar Multiplication]
Let $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$. If
\[ sA = As = B \]
Then
\[ \forall \bar{x} \in \Dim(\vec{x}):
   B[\bar{x}] = (s)(A[\bar{x}]) \]
In other words, multiplying $A$ by a scalar $s$ simply multiplies each of
it's elements by that scalar.
\end{definition}

\subsection{Addition}

\begin{definition}[Multimatrix Addition]
Let $A, B, C \in \mathbb{R}^{\vec{x}}$. If
\[ A + B = C \]
Then
\[ \forall \bar{x} \in \Dim(\vec{x}):
   C[\bar{x}] = A[\bar{x}] + B[\bar{x}] \]
In other words, adding two multimatricies (which must be of the same shape) results
in a multimatrix of the same shape with elements which are the sum of the corresponding
elements of the added multimatricies.
\end{definition}

\begin{definition}[Multimatrix Subtraction]
Let $A, B \in \mathbb{R}^{\vec{x}}$. Then,
\[ A - B = A + (-1)B \]
It should be obvious that, as with scalars, multimatrix subtraction is the
inverse of addition. In other words, $(A + B) - B = A$
\end{definition}

As with scalar, vector, or regular matrix addition, multimatrix addition is
communicative. 
\begin{theorem}[Communicative Property of Multimatrix Addition]
Let $A, B \in \mathbb{R}^{\vec{x}}$. Then,
\[ A + B = B + A \]
\end{theorem}
\begin{proof}
Let $C = A + B$ then,
\[ \forall \bar{x} \in \Dim(\vec{x}) : C[\bar{x}] = A[\bar{x}] + B[\bar{x}] \]
By standard scalar addition rules that also means that,
$C[\bar{x}] = B[\bar{x}] + A[\bar{x}]$. So $C = B + A = A + B$.
\end{proof}

It's similarly trial to show that multimatrix addition is associative.

\begin{theorem}[Associative Property of Multimatrix Addition]
Let $A, B, C \in \mathbb{R}^{\vec{x}}$. Then,
\[ (A + B) + C = A + (B + C) \]
\end{theorem}
\begin{proof}
For all $\bar{x} \in \Dim(\vec{x})$,
\begin{align*}
\left( (A+B)+C \right)[\bar{x}]
&= (A[\bar{x}]+B[\bar{x}])+C[\bar{x}] \\
&= A[\bar{x}]+(B[\bar{x}]+C[\bar{x}]) \\
&= \left( A+(B+C) \right)[\bar{x}]
\end{align*}
\end{proof}

A little less obvious is that multimatrix multiplication can be distributed over
it's addition.

\begin{landscape}
\begin{theorem}[Multimatrix on Multimatrix Distributive Property]
If $|A| = \vec{a} \oplus \vec{n}$ and $|B|=|C|=\vec{n} \oplus \vec{r}$ then,
\[ A \mmult{n} (B + C) = A \mmult{n} B + A \mmult{n} C \]
Similarly, if $|C| = \vec{n} \oplus \vec{c}$ and $|A|=|B|=\vec{l} \oplus \vec{n}$ then,
\[ (A + B) \mmult{n} C = A \mmult{n} C + B \mmult{n} C \]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $|A| = \vec{a} \oplus \vec{n}$ and $|B|=|C|=\vec{n} \oplus \vec{r}$.
So, $\forall \bar{a} \in \Dim(\vec{a}), \bar{r} \in \Dim(\vec{r}) :$
\begin{align*}
 \left( A \mmult{n} (B + C) \right)[\bar{a} \oplus \bar{r}]
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] (B + C)[\bar{n} \oplus \bar{r}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] (B[\bar{n} \oplus \bar{r}] + C[\bar{n} \oplus \bar{r}]) \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] B[\bar{n} \oplus \bar{r}]
  + A[\bar{a} \oplus \bar{n}]C[\bar{n} \oplus \bar{r}] \\
 &= \left( \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] B[\bar{n} \oplus \bar{r}] \right)
    +
    \left( \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}]C[\bar{n} \oplus \bar{r}] \right) \\
 &= \left( A \mmult{n} B \right)[\bar{a} \oplus \bar{r}] +
     \left( A \mmult{n} C \right)[\bar{a} \oplus \bar{r}]
\end{align*}
So $A \mmult{n} (B + C) = A \mmult{n} B + A \mmult{n} C$
\end{ppart}
\begin{ppart}
Let $|C| = \vec{n} \oplus \vec{c}$ and $|A|=|B|=\vec{l} \oplus \vec{n}$.
So, $\forall \bar{c} \in \Dim(\vec{c}), \bar{l} \in \Dim(\vec{l}) :$
\begin{align*}
 \left( (A + B) \mmult{n} C \right)[\bar{l} \oplus \bar{c}]
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    (A+B)[\bar{l} \oplus \bar{n}] C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    (A[\bar{l} \oplus \bar{n}]+B[\bar{l} \oplus \bar{n}])C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]
    +B[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}] \\
 &= \left(\sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]\right)
    +
    \left(\sum_{\forall \bar{n} \in \Dim(\vec{n})}
    B[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]\right) \\
 &= \left(A \mmult{n} C\right)[\bar{l} \oplus \bar{c}]
    +\left(B \mmult{n} C\right)[\bar{l} \oplus \bar{c}]
\end{align*}
So $(A+B) \mmult{n} C = A \mmult{n} C + B \mmult{n} C$ 
\end{ppart}
\end{proof}
\end{landscape}

But it should be obvious that that the same applies to scalar 
multiplication distributing over multimatrix addition..
\begin{theorem}[Scalar on Multimatrix Distributive Property]
If $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$ then,
\[ s(A + B) = sA + sB \]
\end{theorem}
\begin{proof}
Let $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$.
Then for all $\bar{x} \in \Dim(\vec{x})$
\begin{align*}
(s(A+B))[\bar{x}]
&= (s)((A+B)[\bar{x}]) \\
&= (s)(A[\bar{x}] + B[\bar{x}]) \\
&= (s)(A[\bar{x}]) + s(B[\bar{x}])
\end{align*}
Therefore $s(A+B) = sA + sB$
\end{proof}

\subsection{Transposition}

\begin{definition}[Multimatrix Transposition]
Let $|X| = \vec{l} \oplus \vec{r}$ where $|\vec{l}| = l$ and $|\vec{r}| = r$
\[ |\Tran(X, l)| = \vec{r} \oplus \vec{l} \]
And,
\[ \forall \bar{l} \in \Dim(\vec{l}), \bar{r} \in \Dim(\vec{r}) : \]
\[ \Tran(X, l)[\bar{r} \oplus \bar{l}] = X[\bar{l} \oplus \bar{r}] \]
\end{definition}

\subsection*{Sample Problems}
TODO

\section{Derivative Rules}

\begin{theorem}[Right Multiplicative Derivative]
\[ \frac{d(A \mmult{n} X)}{dX} = A \mmult{n} \Ident^2(|X|) \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[Left Multiplicative Derivative]
\[ \frac{d(X \mmult{n} A)}{dX} = \Ident^2(|X \mmult{n} A|) \mmult{n} \Tran(A, n) \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[Transpose Derivative]
$|X| = \vec{l} \oplus \vec{r}$
\[
 \frac{d\Tran(X, |\vec{l}|)}{dX} =
 \Tran(\Ident^2(\vec{r}) \mmult{0} \Ident^2(\vec{l}), |\vec{r}|)
\]
\end{theorem}
\begin{proof}
\begin{align*}
 \frac{d\Tran(X, |\vec{l}|)}{dX}
 [\bar{r_1} \oplus \bar{l_1} \oplus \bar{l_2} \oplus \bar{r_2}]
 &= \delta_{\bar{r_1}\bar{r_2}} \delta_{\bar{l_1}\bar{l_2}} \\
 &= \delta_{\bar{r_2}\bar{r_1}} \delta_{\bar{l_1}\bar{l_2}} \\
 &=
  \Ident^2(|\vec{r}|)[\bar{r_2}\oplus\bar{r_1}]
  \Ident^2(|\vec{l}|)[\bar{l_1}\oplus\bar{l_2}] \\
 &=
   \left(
    \Ident^2(|\vec{r}|)
    \mmult{0}
    \Ident^2(|\vec{l}|)
  \right)
  [\bar{r_2}\oplus\bar{r_1}\oplus\bar{l_1}\oplus\bar{l_2}] \\
 &=
  \Tran(
    \Ident^2(|\vec{r}|)
    \mmult{0}
    \Ident^2(|\vec{l}|)
  , |\vec{r}|) 
 [\bar{r_1} \oplus \bar{l_1} \oplus \bar{l_2} \oplus \bar{r_2}]
\end{align*}
\end{proof}

\begin{theorem}[Addition Rule]
\[ \frac{d(F(X) + G(X))}{dX} = \frac{dF(X)}{dX} + \frac{dG(X)}{dX} \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[Scalar Multiplication Rule]
\[ \frac{d(sF(X))}{dX} = s\frac{dF(X)}{dX} \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{landscape}
\begin{theorem}[Multiplication Rule]
Let $|F| = \vec{f} \oplus \vec{c}$, $|G| = \vec{c} \oplus \vec{g}$,
and $|X| = \vec{x}$. Then,
\begin{align*}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX} =
 F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} +
 \Tran\left(
   \Tran(G(X), |\vec{c}|)
     \mmult{|\vec{c}|}
   \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right),
   |\vec{g} \oplus \vec{x}|
 \right)
\end{align*}
\end{theorem}
\begin{proof}
\[
 \forall
  \bar{f} \in \Dim(\vec{f}),
  \bar{g} \in \Dim(\vec{g}),
  \bar{x} \in \Dim(\vec{x})
 :
\]
\begin{align*}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
 &= \frac{
       \partial \left(F(X) \mmult{|\vec{c}|} G(X)\right)[\bar{f} \oplus \bar{g}]
    }{
       \partial X[\bar{x}]
    } \\
 &= \frac{
       \partial \left(
        \sum_{\forall \bar{c} \in \Dim(\vec{c})}
         F(X)[\bar{f} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{g}]
      \right)
    }{
       \partial X[\bar{x}]
    } \\
 &= \sum_{\forall \bar{c} \in \Dim(\vec{c})}
    \frac{
      \partial F(X)[\bar{f} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{g}]
    }{
      \partial X[\bar{x}]
    }\\
 &= \sum_{\forall \bar{c} \in \Dim(\vec{c})}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]}
    +
    \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
   \left(
    \sum_{\forall \bar{c}}
      F(X)[\bar{f} \oplus \bar{c}]
      \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]}
   \right)
   +
   \left(
    \sum_{\forall \bar{c}}
      \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
      G(X)[\bar{c} \oplus \bar{g}]
   \right) \\
 &=
    l + r
\end{align*}
Which each become,
\begin{align*}
 l
 &=
  \sum_{\forall \bar{c}}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]} \\
 &=
  \sum_{\forall \bar{c}}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{dG(X)}{dX}[\bar{c} \oplus \bar{g} \oplus \bar{x}] \\
 &=
   \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right)
   [\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{align*}
\begin{align*}
 r
 &=
  \sum_{\forall \bar{c}}
    \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
  \sum_{\forall \bar{c}}
    \frac{dF(X)}{dX}[\bar{f} \oplus \bar{c} \oplus \bar{x}]
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
  \sum_{\forall \bar{c}}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
      [\bar{c} \oplus \bar{x} \oplus \bar{f}]
    \Tran(G(X), |\vec{c}|)[\bar{g} \oplus \bar{c}] \\
 &=
  \sum_{\forall \bar{c}}
    \Tran(G(X), |\vec{c}|)[\bar{g} \oplus \bar{c}]
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
      [\bar{c} \oplus \bar{x} \oplus \bar{f}] \\
 &=
  \left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
  \right)[\bar{g} \oplus \bar{x} \oplus \bar{f}] \\
 &= 
  \Tran\left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
    , |\vec{g} \oplus \vec{x}|
  \right)[\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{align*}
Therefore 
\begin{alignat*}{3}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
  &=&&
  \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right)
  [\bar{f} \oplus \bar{g} \oplus \bar{x}] \\
  &&&+
  \Tran\left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
    , |\vec{g} \oplus \vec{x}|
  \right)[\bar{f} \oplus \bar{g} \oplus \bar{x}] \\
  &=&&
  \left\{
    \begin{array}{l}
      \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right) \\
      +
      \Tran\left(
        \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
        \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
        , |\vec{g} \oplus \vec{x}|
      \right)
    \end{array}
  \right\}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{alignat*}
\end{proof}
\end{landscape}

\subsection{Chain Rules}

\begin{theorem}[M.M.M. Chain Rule]
\label{mmm_chain_rule}
Let $F(G)$ be a function $\mathbb{R}^{\vec{g}} \rightarrow \mathbb{R}^{\vec{f}}$
and $G(X)$ be a function $\mathbb{R}^{\vec{x}} \rightarrow \mathbb{R}^{\vec{g}}$.
\[
\frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{|\vec{g}|} \frac{dG}{dX}
\]
\end{theorem}
\begin{proof}
By the definition of a derivative (\ref{mm_derivative}),
\[
\forall \bar{f} \in \Dim(\vec{f}), \bar{g} \in \Dim(\vec{g}):
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
= \frac{\partial F(G)[\bar{f}]}{\partial G[\bar{g}]}
\]
\[
\forall \bar{g} \in \Dim(\vec{g}), \bar{x} \in \Dim(\vec{x}):
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
= \frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
By the scalar total derivative rule,
\[
\frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{\partial F(G)[\bar{f}]}{\partial G[\bar{g}]}
\frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
Substituting in our above definitions this yields,
\[
\frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
And again, by the definition of a derivative (\ref{mm_derivative}),
\[
\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):
\frac{dF(G(X))}{dX}[\bar{f} \oplus \bar{x}]
= \frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
\]
Therefore,
\[
\frac{dF(G(X))}{dX}[\bar{f} \oplus \bar{x}]
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
Which, taking a look at the definition of multidimensional matrix multiplication
(\ref{mm_mult}), tells us that,
\[
\frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{|\vec{g}|} \frac{dG}{dX}
\]
\end{proof}

\begin{theorem}[S.M.M. Chain Rule]
\label{smm_chain_rule}
Let $f(G)$ be a function $\mathbb{R}^{\vec{g}} \rightarrow \mathbb{R}$
and $G(X)$ be a function $\mathbb{R}^{\vec{x}} \rightarrow \mathbb{R}^{\vec{g}}$.
\[
\frac{df(G(X))}{dX} = \frac{df}{dG} \mmult{|\vec{g}|} \frac{dG}{dX}
\]
\end{theorem}
\begin{proof}
By the definitions of a derivative
(\ref{sm_derivative} and \ref{mm_derivative} respectively),
\[
\forall \bar{g} \in \Dim(\vec{g}):
\frac{df(G)}{dG}[\bar{g}]
= \frac{\partial f(G)}{\partial G[\bar{g}]}
\]
\[
\forall \bar{g} \in \Dim(\vec{g}), \bar{x} \in \Dim(\vec{x}):
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
= \frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
By the scalar total derivative rule,
\[
\frac{\partial f(G(X))}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{\partial f(G)}{\partial G[\bar{g}]}
\frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
Substituting in our above definitions this yields,
\[
\frac{\partial f(G(X))}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{df(G)}{dG}[\bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
Again, by the definition of a derivative (\ref{sm_derivative}),
\[
\frac{df(G(X))}{dX}[\bar{x}] =
\frac{\partial f(G(X))}{\partial X[\bar{x}]}
\]
Therefore,
\[
\frac{df(G(X))}{dX}[\bar{x}]
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{df(G)}{dG}[\bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
Which, by the definition of multidimensional multipliction (\ref{mm_mult}),
implies that,
\[
\frac{df(G(X))}{dX} = \frac{df(G)}{dG} \mmult{|\vec{g}|} \frac{dG(X)}{dX}
\]
\end{proof}

\begin{theorem}[M.S.M. Chain Rule]
\label{msm_chain_rule}
Let $F(g)$ be a function $\mathbb{R} \rightarrow \mathbb{R}^{\vec{f}}$
and $g(X)$ be a function $\mathbb{R}^{\vec{x}} \rightarrow \mathbb{R}$.
\[
\frac{dF(g(X))}{dX} = \frac{dF}{dg} \mmult{0} \frac{dg}{dX}
\]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[M.M.S. Chain Rule]
\label{mms_chain_rule}
Let $F(G)$ be a function $\mathbb{R}^{\vec{g}} \rightarrow \mathbb{R}^{\vec{f}}$
and $G(x)$ be a function $\mathbb{R} \rightarrow \mathbb{R}^{\vec{g}}$.
\[
\frac{dF(G(x))}{dx} = \frac{dF}{dG} \mmult{|\vec{g}|} \frac{dG}{dx}
\]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[S.S.M. Chain Rule]
\label{ssm_chain_rule}
Let $f(g)$ be a function $\mathbb{R} \rightarrow \mathbb{R}$
and $g(X)$ be a function $\mathbb{R}^{\vec{x}} \rightarrow \mathbb{R}$.
\[
\frac{df(g(X))}{dX} = \frac{df}{dg} \frac{dg}{dX}
\]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[S.M.S. Chain Rule]
\label{sms_chain_rule}
Let $f(G)$ be a function $\mathbb{R}^{\vec{g}} \rightarrow \mathbb{R}$
and $G(x)$ be a function $\mathbb{R} \rightarrow \mathbb{R}^{\vec{g}}$.
\[
\frac{df(G(X))}{dx} = \frac{df}{dG} \mmult{|\vec{g}|} \frac{dG}{dx}
\]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[M.S.S. Chain Rule]
\label{mss_chain_rule}
Let $F(g)$ be a function $\mathbb{R} \rightarrow \mathbb{R}^{\vec{f}}$
and $g(x)$ be a function $\mathbb{R} \rightarrow \mathbb{R}$.
\[
\frac{dF(g(x))}{dx} = \frac{dF}{dg} \frac{dg}{dx}
\]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\section{Integration}

I think, I'm not sure, but I think that the integral of a multimatrix function should
be defined,

\[
 \int_A^B F(X) dX = \lim_{n \to \infty}
 \sum_{i=1}^n
  F\left(A+\frac{i}{n}(B-A)\right)
  \mmult{||X||}
  \left(\frac{B-A}{n}\right)
\]

Which, I have very little idea what that means geometriclly, but
if you let $G(X) = \int_A^X F(Y) dY$ you seem to end up with $dG/dX = F(X)$.

Maybe... well look at it this way. $F(X)$ is a function which produces some,
afine-like transformation multimatrix for multimatricies of shape $|X|$,
since it must have a right hand shape of $\ldots \oplus |X|$. This integration
occures over a linear path of $X$ as it moves from $A$ to $B$ applying that
transformation and adding up the results of the transformed multimatricies with
a weight proportional to the step size... and in the direction of the step.

So, it's sort of like a line integral but not really. They're related somehow.

\begin{landscape}
Let's see if I can prove that theory,
\begin{theorem}[Fundamental Rule of Multimatrix Calculus]
Let $G(X) = \int_A^X F(Y) dY$ then $dG(X)/dX = F(X)$.
\end{theorem}
\begin{proof}
\begin{align*}
  G(X)
  &= \int_A^X F(Y) dY \\
  &= 
    \lim_{n \to \infty}
     \sum_{i=1}^n
      F\left(A+\frac{i}{n}(X-A)\right)
      \mmult{||Y||}
      \left(\frac{X-A}{n}\right)
\end{align*}
\begin{alignat*}{3}
  \frac{dG(X)}{dX}
  &=&&
    \frac{
      d \left\{
      \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
        \mmult{||Y||}
        \left(\frac{X-A}{n}\right)
      \right\}
    }{dX} \\
  &=&&
    \lim_{n \to \infty}
    \frac{
      d \left\{
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
        \mmult{||Y||}
        \left(\frac{X-A}{n}\right)
      \right\}
    }{dX} \\
  &=&&
    \lim_{n \to \infty}
    \sum_{i=1}^n
      \frac{
        d \left\{
          F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \left(\frac{X-A}{n}\right)
        \right\}
      }{dX} \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \frac{d \left(\frac{X-A}{n}\right)}{dX}
        \\&&&+
        \Tran\left(
          \Tran\left(\frac{X-A}{n}, ||Y||\right)
          \mmult{||Y||}
          \Tran\left(\frac{
            d \left\{F\left(A+\frac{i}{n}(X-A)\right)\right\}
          }{dX}, |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right) \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \frac{1}{n}\Ident^2(|X|)
        \\&&&+
        \Tran\left(
          \frac{X-A}{n}
          \mmult{||Y||}
          \Tran\left(
            \frac{dF(X)}{dX}
            \mmult{|X|}
            \frac{i}{n}\Ident^2(|X|), |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right) \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        \frac{1}{n}F\left(A+\frac{i}{n}(X-A)\right)
        \\&&&+
        \Tran\left(
          \frac{X-A}{n}
          \mmult{||Y||}
          \Tran\left(
            \frac{i}{n}\frac{dF(X)}{dX}
            , |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right)
\end{alignat*}
At this point I'm fairly stuck.
\end{proof}
\end{landscape}
\end{document}
