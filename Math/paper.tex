\documentclass[12pt]{book}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{pdflscape}
\usepackage{etoolbox}
\usepackage{csquotes}
\usepackage[normalem]{ulem}

\title{Multimatrices and Their Derivatives}
\author{Ernest Kirstein}
\date{\today}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{exercise}{Exercise}[chapter]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}[chapter]

\newtheoremstyle{ppart}{}{}{}{}{}{:}{ }{}
\theoremstyle{ppart}
\newtheorem{ppart}{Part}

\newtheoremstyle{solution}{}{}{}{}{}{:}{\newline}{Exercise \thmnumber{#2} Solution}
\theoremstyle{solution}
\newtheorem{solution}{Solution}

\AtBeginEnvironment{section}{\setcounter{solution}{0}}
\AtBeginEnvironment{proof}{\setcounter{ppart}{0}}
\AtBeginEnvironment{align}{\setcounter{equation}{0}}
\AtBeginEnvironment{alignat}{\setcounter{equation}{0}}

\DeclareMathOperator{\Dim}{Dim}
\DeclareMathOperator{\Ident}{I}
\DeclareMathOperator{\Tran}{Tran}
\DeclareMathOperator{\SoS}{\mathbb{S}}
\DeclareMathOperator{\Softmax}{Softmax}
\DeclareMathOperator{\Sum}{\Sigma}
%\newcommand{\mmult}[1]{\stackrel{\times}{#1}}
%\newcommand{\mmult}[1]{\underset{#1}{\times}}
\newcommand{\mmult}[1]{\text{\raisebox{1ex}{$\underset{#1}{\times}$}}}

\begin{document}

\maketitle

\chapter*{Preface}
I was working my way through a hobby programming project, trying to manifest Skynet
as more than the apocalyptic fever dream of James Cameron, and ran into a problem.
The existing notation for taking derivatives of matrix equations is terrible.
It's so deliberately obtuse that I could only conclude that some hidden cabal of
mathematicians is actively working to sabotage our glorious yet-to-be-built
robot overlords.

That sort of justice cannot stand! 

So, I set out to invent a system of my own. Much to my surprise, some of the
results with were rather elegant. For example, I stumbled onto a
multidimensional version of the derivative chain rule (Thm. \ref{mmm_chain_rule})
that's nearly as consise as the same rule for scalars.

The ultimate goal of this notation system was to \sout{bring about the downfall
of humanity} figure out a way to take derivatives
of matrix equations without having to break apart matricies into their constituent
parts. I knew that I had succeeded when I was able to derive the derivative of the
softmax function completely in my own notation (Thm. \ref{softmax_derivative}).

Now, not knowing what else to do with the results, I've written this book
about my amature exploration into multidimensional matricies -
multimatricies - and how they can simplify derivatives of matrix equations.

\tableofcontents

\chapter{Defining the Derivative}

\begin{displayquote}
``If you're a bit touched, like I am, you may want to make a dedicated tool.'' -
This Old Tony, `Making Springs At Home'
\end{displayquote}

Let's consider a traditional matrix equation. As is traditional, one matrix
$X$ shall represent the scientific education level of undergraduates, who happen
to work in a densely packed 3 by 5 grid - which is also traditional. Then let
$B$ represent the belief each of those undergradutes has in a higher power,
which, as we all know, fluctuates with respect to their scientific education in
a sinusoidal pattern (Weinersmith, 2010 \cite{weinersmith:education}).

Formally, the shape of these two matricies are defined,
$|B| = |X| = \left< 3,5 \right>$.
And each element of $B$ will be the sine of the corresponding element of
$X$. That is, \[B_{r,c} = \sin(X_{r,c})\]
Which I'll write more simply as, \[B = \sin(X)\]

Now, I would call the derivative of any element of $B$ with respect to any element
of $X$ a partial derivative of that function. Since
$\partial B_{r_1, c_1}/\partial X_{r_2, c_2}$ is defined as the rate of change of
\textit{one} undergraduate's belief in a higher power with respect to \textit{one}
undergraduate's education level - it only represents a change to part of the whole
system. 
We can calculate this partial derivative easily enough,
\[
\frac{\partial B_{r_1,c_1}}{\partial X_{r_2,c_2}} = 
\left\{
  \begin{array}{ll}
    \cos(X_{r_2,c_2})  & \mbox{if } r_1 = r_2 \mbox{ and } c_1 = c_2 \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\]
Every student's belief in a higher power changes with respect to their own
scientific education level, but does not change with respect to other students'
scientific education. No mater how much I teach Jane about photosynthesis, I'm
not going to change Joe's mind about divine mysteries.

This can also be writen more simply using Kronecker deltas \cite{wiki:kronecker},
which should make one feel particularly intellegent as Kronecker deltas were used
by Einstein (and presumably someone named Kronecker) \cite{wiki:einstein}.
\[
\frac{\partial B_{r_1,c_1}}{\partial X_{r_2,c_2}} = 
\cos (X_{r_2, c_2}) \delta_{r_1 r_2} \delta_{c_1 c_2}
\]

But what about the complete deriative?
What sort of mathmatical object would represent the entire derivative
of $B$ with respect to $X$?
I need a mathematical object which perfectly
encapsulates the rate of change of \textit{all} the undergraduates' beliefs in a
higher power with respect to \textit{every} undergraduates' scientific education
and my current toolkit doesn't work!

\[\frac{dB}{dX} = \mbox{ ? }\]

If this were a function that mapped a vector to a scalar, you might say that the
complete derivate was the gradient. But in this case, our function's output is more
than a simple scalar and even our dependent variable is more complex than a vector.

By looking at the partial derivatives, we can come to some understanding of the
dimensionality of the complete derivative. Each combination of input and output
element has its own partial derivative, so it makes sense that the total size
of the complete derivative should be the product of the size of $B$ and the size
of $X$.

And so, the size of the complete derivate would be,
\[
\mbox{size}\left(\frac{dB}{dX}\right) = \mbox{size}(B) \mbox{size}(X)
\]
And perhaps we could even let this help us define the `shape' of the complete
derivative. This complete derivative would need to be composed of partial
derivatives, so it would make sense to just smush the shapes of the input
matrix and the output matrix together,
\begin{align*}
\left|\frac{dB}{dX}\right| &= |B| \oplus |X| \\
 &= \left< 3, 5 \right> \oplus \left< 3, 5 \right> \\
 &= \left< 3, 5, 3, 5 \right>
\end{align*}
Where $\oplus$ is the concatenate operator.

The ordering here is arbitrary, there is no reason why the dimensionality of
of the independent variable \textit{needs} to be concatenated to the right of the
dimensionality of the output. What's important is that the dimensionality of
both output and indepenedent variable are preserved. Let us declare this to
be our convention since, for the purpose of this work, mine is the divinely revealed
word of god, passed down to you lowly mortals who have yet to commit your
own mad ramblings to \LaTeX.

Our total derivate is therefore a multidimensional object with four indicies.
Suppose we try to pull a single partial derivative from this complete derivative,
what could our notation look like? Four underscript indicies would be unwieldy,
so maybe something like this:
\[
\frac{dB}{dX}[1,2,3,4] = \frac{\partial B_{1,2}}{\partial X_{3,4}}
\]

And since we're defining our own notation, let's splurge and reference our original
matricies in a similar manner:
\[
\frac{dB}{dX}[1,2,3,4] = \frac{\partial B[1,2]}{\partial X[3,4]}
\]

So what is this object we've defined? In all ways that matter, it seems to be
a multidimensional matrix. It has four dimensions of indices, rather than just
a row and column, but each uniquely indexed cell in this object may have a
unique value, just like a standard two dimensional matrix. Let's define this
bit of notation formaly.

\begin{definition}[Multimatrix]
A multimatrix $M$ is a multidimensional object that has some shape,
$\vec{v} \in \mathbb{N}^n$, where $n \in \mathbb{Z}^+ \cup \{0\}$
is the number of dimensions of $M$.

Let $\vec{v}$ be the natural number vector which describes the shape of
$M$, then $\vec{v} = |M|$. And for any other $\bar{v} \in \mathbb{N}^n$, $\bar{v}$
is a valid index of $M$ if and only if each value of $\bar{v}$ is less than or equal
to the value of it's corresponding index in $\vec{v}$.
That is, $\forall i: 1 \le \bar{v}_i \le \vec{v}_i$.

We can further simplify that notation for our set of valid indicies. Let's say we have
some $\vec{v} \in \mathbb{N}^n$ that defines the shape of some matrix. Then we'll
define the set of valid indicies of that matrix to be $\Dim(\vec{v})$. 

Each value of $M$ is a real value, so we'll define the set of all real valued
multidimensional matricies with shape $\vec{v}$ as $\mathbb{R}^{\vec{v}}$.

We'll use the notation $M[\bar{v}]$ to reference any individual scalar value of $M$
at indicies $\bar{v} \in \Dim(|M|)$. 
\end{definition}

Going back to our original motivating problem, we (that is to say, myself and my
split personalities whom I reference as to give my writing the percieved authority
of more than one author) can express our derivative notation thusly,

\begin{definition}[Multimatrix on Multimatrix Derivative]
\label{mm_derivative}
Let $F(X)$ be some function from $\mathbb{R}^{\vec{x}}$ to $\mathbb{R}^{\vec{f}}$.
The complete derivative of $F$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{f} \oplus \vec{x}}$. That is,
\[ |F(X)| = \vec{f} \]
\[ |X| = \vec{x} \]
\[ \left|\frac{dF}{dX}\right| = \vec{f} \oplus \vec{x} \]
\[
\forall \bar{f} \in \Dim(\vec{f}),
        \bar{x} \in \Dim(\vec{x}):
\]
\[
\frac{dF}{dX}[\bar{f} \oplus \bar{x}] =
\frac{\partial F[\bar{f}]}{\partial X[\bar{x}]}
\]
\end{definition}

But, of course, there are other combinations of functions than from one
multimatrix to another. You might have a function from a multimatrix to a
scalar (such as summing up all the values of a multimatrix), or from a scalar
to a multimatrix (such as filling a multimatrix with a single scalar value).

We can define the total derivative for those cases as well.

\begin{definition}[Scalar on Multimatrix Derivative]
\label{sm_derivative}
Let $f(X)$ be some function from $\mathbb{R}^{\vec{x}}$ to $\mathbb{R}$.
The complete derivative of $f$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{x}}$. That is,
\begin{align*}
f(X) &\in \mathbb{R} \\
|X| &= \vec{x} \\
\left|\frac{df}{dX}\right| &= \vec{x}
\end{align*}
\[
\forall \bar{x} \in \Dim(\vec{x}):
        \frac{df}{dX}[\bar{x}] =
        \frac{\partial f}{\partial X[\bar{x}]}
\]
\end{definition}

\begin{definition}[Multimatrix on Scalar Derivative]
\label{ms_derivative}
Let $F(x)$ be some function from $\mathbb{R}$ to $\mathbb{R}^{\vec{f}}$.
The complete derivative of $F$ with respect to $X$ is therefore in
$\mathbb{R}^{\vec{f}}$. That is,
\begin{align*}
|F(x)| &= \vec{f} \\
x &\in \mathbb{R} \\
\left|\frac{dF}{dx}\right| &= \vec{f}
\end{align*}
\[
\forall \bar{f} \in \Dim(\vec{f}):
        \frac{dF}{dx}[\bar{f}] =
        \frac{\partial F[\bar{f}]}{\partial x}
\]
\end{definition}

Ok, that's neat and all but what does that actually mean? Can we answer our motivating 
question?
\[\frac{dB}{dX} = ? \]
Well... sort of. We have to abuse the notation a little and say that $B$ and $X$ are
just multimatricies with 2 dimensions. Then we can say that the total derivative
of $B$ with respect to $X$ is a multimatrix with shape $|B| \oplus |X|$,
who's elements we can define as,
\[\forall \bar{b} \in \Dim(|B|), \bar{x} \in \Dim(|X|):\]
\begin{align*}
\left( \frac{dB}{dX} \right)[\bar{b} \oplus \bar{x}]
&= \frac{\partial B[\bar{b}]}{\partial X[\bar{x}]} \\
&= \delta_{\bar{b}\bar{x}}\cos(\bar{x})
\end{align*}

Which isn't a huge improvement over our original notation. What would be great is
if we could define the total derivative without going down into the partial derivatives
at all, which we'll be able to do in later chapters. As a sneak peak (a whole
chapter ahead), that'll look like this:
\[
\frac{dB}{dX} = \Ident^3(|X|) \mmult{||X||} \cos(X)
\]

But that single line will require defining multimatrix multiplication and
a mysterious, and possibly dangerious, entity call a cubic identity, $\Ident^3(|X|)$.

\section*{Exercises}

\begin{exercise}
Are the following assertions are true or false?
\begin{enumerate}
\item $\left<1,1,1\right> \in \Dim(\left<5,5,5\right>)$
\item $\left<1,1\right> \in \Dim(\left<5,5,5\right>)$
\item $\left<1,2,3\right> \in \Dim(\left<3,2,1\right>)$
\item $\left<3,7\right> \in \Dim(\left<3,7\right>)$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $F(X)$ be a function from $\mathbb{R}^{\left<3,7,2\right>}$ to
$\mathbb{R}^{\left<6, 7\right>}$. That is, $|F(X)| = \left<6,7\right>$
and for any valid input to $F(X)$, $|X| = \left<3,7,2\right>$.
Then what is the shape of $dF/dX$?
\end{exercise}

\begin{exercise}
Let $f(X)$ be the sum of all values of $X$, where
$X \in \mathbb{R}^{\left<2,3,2\right>}$. That is,
\[ f(X) = \sum_{\forall \bar{x} \in \Dim(\left<2,3,2\right>)} X[\bar{x}] \]
What is the shape of $df/dX$? If $\bar{v} \in \Dim(|df/dX|)$ then what is
\[ \frac{df}{dX}[\bar{v}] = ? \]
\end{exercise}

\chapter{Multimatrix Multiplication}

\begin{displayquote}
``The universe is a cruel, uncaring void. The key to being happy isn't a search
for meaning. It's to just keep yourself busy with unimportant nonsense, and eventually,
you'll be dead.'' - Mr. Peanutbutter, BoJack Horseman
\end{displayquote}

The multiplication of traditional matricies is a calculation that, when one first
encouters it, inspires thoughts such as ``By which satanic cult was this process
proscribed?''

After seeing the method used in practice, one slowly comes to understand the
reason behind the madness. That is, that madness is the natural state of things
and matrix multiplication is used to punish those few brave souls who dare to
fight against the darkness.

Multimatrix multiplication is much the same.

Just to review, for two matricies $A$ and $B$, multipication is only defined
for $A \times B = C$ if $A$ has the same number of columns as $B$ has rows.
That is, if $|A| = \left< r_a, c_a \right>$ and $|B| = \left< r_b, c_b \right>$
then $A \times B$ is valid if and only if $c_a = r_b$. And furthermore,
the remaining dimensions of $A$ and $B$ define the diminsions of $C$,
$|C| = \left< r_a, c_b \right>$. Each element of $C$ is a sum of the product of
every combination of the corresponding row of $A$ and the corresponding column of
$B$,

\[ C[r_a, c_b] = \sum_{\forall i} A[r_a, i] B[i, c_b] \]

Easily done. The hard part isn't doing the calculation. The hard part is
parlaying with Sheogorath, Daedric Prince of Madness for the intellectual
property rights.

So now consider how we would multiply two matricies $A$ and $B$ if they had more
than two dimensions. Let's use a concrete example to make the problems more apparent.
Let $|A| = \left<5,7,7\right>$ and $|B| = \left<7,7,2,4\right>$.
First of all, can we even multiply the two together? Hm... maybe? $|A|$ ends with
$\left<\ldots, 7\right>$ and $|B|$ starts with $\left<7,\ldots\right>$ so it would
make sense that we can somehow squish them together into something. By that reasoning,
the shape of our after-multiplication output would use the remaining dimensions
$\left<5, 7, 7, 2, 4\right> = |A \times B|$.

But $|A|$ also ends with $\left<\ldots, 7, 7\right>$ and $|B|$ starts with
$\left<7, 7\ldots\right>$ so maybe our multiplication operation should work on that
additional dimension, leaving us with $\left<5, 2, 4\right> = |A \times B|$.
But the first definition looks like it could still be useful! It could let us cover 
two dimensional matrix multiplication completely within our definition. This second
definition seems more... er... multidimensional... but it also seems to
arbitrarily depend on the shape of the two matricies.

What we really need is an operation which covers both lines of reasoning. To do
so, we need to compleately abandon conventional algebras, and indeed any hope of
returning to sanity. That's right, we're introducing a trinary operator.

I would argue that there shall always be some component of, 
``How many dimensions are you combining?'' inherent
in the operation. So we can distinguish our two types of multiplication, and indeed
an infinite number of types of multiplication, with a new operator, $\mmult{n}$ where
$|A \mmult{1} B| = \left<5, 7, 7, 2, 4\right>$ and 
$|A \mmult{2} B| = \left<5, 2, 4\right>$.

But the shape is only a small component of the multipliation  operation. Let's
just jump into the formal definition and I'll explain with an example:

\begin{definition}[Multimatrix Multiplication]
\label{mm_mult}
Let $A$ and $B$ be multimatricies such that $|A| = \vec{a} \oplus \vec{c}$,
$|B| = \vec{c} \oplus \vec{b}$, and $|\vec{c}| = n$ with
$n \in \{0\} \cup \mathbb{Z}^{+}$. We shall call $n$ the 'size' or the 'width'
or the 'overlap' of the multiplication operation.
The shape of the result is defined, $|A \mmult{n} B| = \vec{a} \oplus \vec{b}$
and the value of the result is defined,
\[
\forall \bar{a} \in \Dim(\vec{a}), \bar{b} \in \Dim(\vec{b}):
(A \mmult{n}  B)[\bar{a} \oplus \bar{b}] =
\sum_{\forall \bar{c} \in \Dim(\vec{c})}
  A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}]
\]
\end{definition}

\newpage
\begin{example}
Let $|A| = \left<2, 3, 2\right>$ and $|B| = \left<3, 2, 2\right>$. I would
define their values in a three dimensional grid but I fear that my three dimensional
\LaTeX skills aren't up to the task (and three dimensional monitors are quite hard to
come by). So here is where we leave behind nice graphical representations of
things, and I'll just define the elements of $A$ and $B$ in the tables below.
\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c | c}
$\bar{a}_1$ & $\bar{a}_2$ & $\bar{a}_3$ & $A[\bar{a}]$ \\
\hline
1           & 1           & 1           & -0.5         \\
1           & 1           & 2           & 1            \\
1           & 2           & 1           & 2            \\
1           & 2           & 2           & -1           \\
1           & 3           & 1           & 5            \\
1           & 3           & 2           & 1            \\
2           & 1           & 1           & 3            \\
2           & 1           & 2           & -7           \\
2           & 2           & 1           & 5            \\
2           & 2           & 2           & 2            \\
2           & 3           & 1           & -3           \\
2           & 3           & 2           & -1
\end{tabular}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c | c}
$\bar{b}_1$ & $\bar{b}_2$ & $\bar{b}_3$ & $B[\bar{b}]$ \\
\hline
1           & 1           & 1           & 1            \\
1           & 1           & 2           & 0            \\
1           & 2           & 1           & 1            \\
1           & 2           & 2           & 0            \\
2           & 1           & 1           & 1            \\
2           & 1           & 2           & 0            \\
2           & 2           & 1           & 1            \\
2           & 2           & 2           & 0            \\
3           & 1           & 1           & 1            \\
3           & 1           & 2           & 0            \\
3           & 2           & 1           & 1            \\
3           & 2           & 2           & 0
\end{tabular}
\end{center}
\end{table}

Given $|A|$ and $|B|$, and by our definition of multimatrix multiplication,
you can see that $|A \mmult{2} B| = \left<2, 2\right>$. And the values of the
resulting multimatrix are defined:

\begin{table}[h!]
\begin{center}
\begin{tabular}{c c | c | c | c}
$\bar{x}_1$ & $\bar{x}_2$ & $(A \mmult{2} B)[\bar{x}]$ & = & = \\
\hline
1 & 1 &
  $A[1,1,1]B[1,1,1] + A[1,1,2]B[1,2,1] +$ & $(-0.5)(1) + (1)(1) +$ & 7.5 \\
&&$A[1,2,1]B[2,1,1] + A[1,2,2]B[2,2,1] +$ & $(2)(1) + (-1)(1) +$ & \\
&&$A[1,3,1]B[3,1,1] + A[1,3,2]B[3,2,1]$   & $(5)(1) + (1)(1)$ & \\
1 & 2 &
  $A[1,1,1]B[1,1,2] + A[1,1,2]B[1,2,2] +$ & $(-0.5)(0) + (1)(0) +$ & 0 \\
&&$A[1,2,1]B[2,1,2] + A[1,2,2]B[2,2,2] +$ & $(2)(0) + (-1)(0) +$ & \\
&&$A[1,3,1]B[3,1,2] + A[1,3,2]B[3,2,2]$   & $(5)(0) + (1)(0)$ & \\
2 & 1 &
  $A[2,1,1]B[1,1,1] + A[2,1,2]B[1,2,1] +$ & $(3)(1) + (-7)(1) +$ & 1 \\
&&$A[2,2,1]B[2,1,1] + A[2,2,2]B[2,2,1] +$ & $(5)(1) + (2)(1) +$ & \\
&&$A[2,3,1]B[3,1,1] + A[2,3,2]B[3,2,1]$   & $(-3)(1) + (1)(1)$ & \\
2 & 2 &
  $A[2,1,1]B[1,1,2] + A[2,1,2]B[1,2,2] +$ & $(3)(0) + (-7)(0) +$ & 0 \\
&&$A[2,2,1]B[2,1,2] + A[2,2,2]B[2,2,2] +$ & $(5)(0) + (2)(0) +$ & \\
&&$A[2,3,1]B[3,1,2] + A[2,3,2]B[3,2,2]$   & $(-3)(0) + (1)(0)$ &
\end{tabular}
\end{center}
\end{table}

\end{example}
\newpage

Like two dimensional matrix multiplication, multimatrix multiplication is
\textit{not} communicative. And it is only weakly associative. That sounds like
nonsense, and ideed it is, but I shall prove it to you.

Multimatrix multiplication may not even make sense dimensionally when you move
around parentheses. Take for example multimatricies $A$, $B$, and $C$ such that,
$|A| = \left<5,3,2\right>$,
$|B| = \left<2,7\right>$, and
$|C| = \left<3,7,2\right>$ where you should be able to see that
$(A \mmult{1} B) \mmult{2} C$ is a valid operation since
$|A \mmult{1} B| = \left<5,3,7\right>$ and
$|(A \mmult{1} B) \mmult{2} C| = \left<5,2\right>$. But
$A \mmult{1} (B \mmult{2} C)$ isn't a valid set of operations since even
$B \mmult{2} C$ is an invalid operation.

But even when the dimensional analysis makes sense, multimatrix multiplication
may not be associative (Honestly, how could you be so naive?). 
Take, for example, multimatricies $A$, $B$, and $C$ such
that $|A| = \left<2,2,2,2\right>$, $|B| = \left<2,2,2\right>$, and
$|C| = \left<2,2,2,2\right>$. Let,
\[
 A[\bar{a}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{a} = \left<1,1,1,2\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
\[
 B[\bar{b}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{b} = \left<1,2,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
And
\[
 C[\bar{c}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{c} = \left<1,1,1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
Then,
\[
 (A \mmult{2} B)[\bar{x}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{x} = \left<1,1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
So,
\[
 ((A \mmult{2} B) \mmult{2} C)[\bar{x}] = 
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \bar{x} = \left<1,1\right> \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\]
But, $(B \mmult{2} C)[\bar{x}] = 0$ so $(A \mmult{2} (B \mmult{2} C))[\bar{x}] = 0$
Which goes to show that even though both $(A \mmult{2} B) \mmult{2} C$ and
$A \mmult{2} (B \mmult{2} C)$ are valid calculations which result in multimatrixes
of the same dimensionality, they are not equal so multiplicative associativity
doesn't hold. It also goes to show that your intuition is bad and you should feel bad.

However, there are a broad catagory of cases where associativity \textit{does} hold:

\begin{theorem}[Associativity of Multimatrix Multiplication]
\label{mm_associativity}
Let $A$, $B$, and $C$ be multimatricies and $m$ and $n$ in
$\left\{ 0 \right\} \cup \mathbb{Z}^{+}$.
If both operations $(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$
are valid dimensionally and $m+n \le ||B||$, then,
\[ (A \mmult{m} B) \mmult{n} C = A \mmult{m} (B \mmult{n} C) \]
\end{theorem}
\begin{proof}
Let $A$, $B$, and $C$ be matricies such that the calculations
$(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$ are both valid
and $m + n \le ||B||$.
Then the following must be true by the definiton of multimatrix multiplication
(Def. \ref{mm_mult}).

Since $A \mmult{m} B$ is defined, there exist vectors $\vec{a}, \vec{m},$ and
$\vec{b_1}$ such that 
$|\vec{m}| = m$,
$|A| = \vec{a} \oplus \vec{m}$,
and
$|B| = \vec{m} \oplus \vec{b_1}$

Since $B \mmult{n} C$ is defined, there exist vectors $\vec{b_2}, \vec{n},$ and
$\vec{c}$ such that
$|\vec{n}| = n$,
$|B| = \vec{b_2} \oplus \vec{n}$
and
$|C| = \vec{n} \oplus \vec{c}$

Since $|B| = \vec{m} \oplus \vec{b_1} = \vec{b_2} \oplus \vec{n}$ and
$||B|| \ge |\vec{m}| + |\vec{n}|$, it must follow that, there is some vector $\vec{b}$
such that,
\[
 |B| = \vec{m} \oplus \vec{b} \oplus \vec{n}
\]
Even if $\vec{b}$ is an empty vector. Then,
\[ |A \mmult{m} B| = \vec{a} \oplus \vec{b} \oplus \vec{n} \]
\[ |(A \mmult{m} B) \mmult{n} C| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
\[ |B \mmult{n} C| = \vec{m} \oplus \vec{b} \oplus \vec{c} \]
\[ |A \mmult{m} (B \mmult{n} C)| = \vec{a} \oplus \vec{b} \oplus \vec{c} \]
So,
\[ |(A \mmult{m} B) \mmult{n} C| = |A \mmult{m} (B \mmult{n} C)| \]
Which is necessary, but not sufficient, for our proof.

Looking at each element of the result matricies, let $j$ and $k$ be the values
of some parallel cells in $(A \mmult{m} B) \mmult{n} C$ and
$A \mmult{m} (B \mmult{n} C)$ respectively. Then for some indicies of those cells,
$\bar{a} \oplus \bar{b} \oplus \bar{c} \in
\Dim(\vec{a} \oplus \vec{b} \oplus \vec{c})$,
\begin{align*}
 j
 &= ((A \mmult{m} B) \mmult{n} C)[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
 (A \mmult{m} B)[\bar{a} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n}}
 \left(
  \sum_{\forall \bar{m} \in \Dim(\vec{m})}
  A[\bar{a} \oplus \bar{m}]B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 \right)
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n}, \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{m}, \bar{n}}
 A[\bar{a} \oplus \bar{m}]
 B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
 C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 \left(
 \sum_{\forall \bar{n}}
  B[\bar{m} \oplus \bar{b} \oplus \bar{n}]
  C[\bar{n} \oplus \bar{c}]
 \right) \\
 &= \sum_{\forall \bar{m}}
 A[\bar{a} \oplus \bar{m}]
 (B \mmult{n} C)[\bar{m} \oplus \bar{b} \oplus \bar{c}] \\
 &= (A \mmult{m} (B \mmult{n} C))[\bar{a} \oplus \bar{b} \oplus \bar{c}] \\
 &= k
\end{align*}
Since $(A \mmult{m} B) \mmult{n} C$ and $A \mmult{m} (B \mmult{n} C)$ are both
the same shape and every cell with the same index in each is equal, the two
values must be equal.
\end{proof}

Multiplicative identities for multimatricies depend on the shape of the
multimatrix they are being multiplied against, just as for regular matricies.
However, the multiplicative identity also greatly depends on the size of
the particular multiplication operation. Making a confusing and frightening
situation even more so, like loosing a troup of clowns in children's cancer ward.

\begin{definition}[Multimatrix Identities]
\label{mm_mult_ident}
Let us define $\Ident^n(\vec{v})$ as the $n^{\text{th}}$ order identity with base shape
$\vec{v}$. This $\Ident^n(\vec{v})$ is a multimatrix with shape,
\[ |\Ident^n(\vec{v})| = \bigoplus_{i \in [1, n]} \vec{v} \]
That is, it's shape is $n$ successive concatinations of $\vec{v}$.
Let each element of $\Ident^n(\vec{v})$ is defined as,
\[ \forall \bar{v_1}, \bar{v_2}, \ldots, \bar{v_n} \text{ each in } \Dim(\vec{v}) : \]
\[
 \Ident^n(\vec{v})[\bar{v_1}, \bar{v_2}, \ldots, \bar{v_n}]
 = \left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{v_1} = \bar{v_2} = \ldots = \bar{v_n} \\
    0 & \mbox{otherwise}
  \end{array}
 \right.
\]

In particular, let us call $\Ident^2(\vec{v})$ the square identity of $\vec{v}$ and
$\Ident^3(\vec{v})$ the cubic identity of $\vec{v}$.
\end{definition}

That definition seems rather arbitrary, but you'll see various orders of identity
pop up in derivatives down the line. For now, let's just look at how the
square identity it relates to a multiplication operation.

\begin{theorem}[Multimatrix Multiplicative Identity]
\label{mm_ident}
For all $\vec{m}$ and $\vec{n}$ on which the operations are valid,
\[
 A \mmult{|\vec{m}|} \Ident^2(\vec{m}) = A
\]
and
\[
 \Ident^2(\vec{n}) \mmult{|\vec{n}|} A = A
\]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $A$ be a matrix with shape $|A| = \vec{a} \oplus \vec{m}$.
Then by the definition of multimatrix multiplication (Def. \ref{mm_mult}),
\[ \forall \bar{a} \in \Dim(\vec{a}), \bar{m_1} \in \Dim(\vec{m}) : \]
\[
 (A \mmult{|\vec{m}|} \Ident^2(\vec{m}))[\bar{a} \oplus \bar{m_1}]
 =
 \sum_{\forall \bar{m_2} \in \Dim(\vec{m})}
 A[\bar{a} \oplus \bar{m_2}] \Ident^2(\vec{m})[\bar{m_2} \oplus \bar{m_1}]
\]
Then, by the definition of $\Ident^2(\vec{m})$ (Def. \ref{mm_ident}) it follows
that,
\begin{align*}
 (A \mmult{|\vec{m}|} \Ident^2(\vec{m}))[\bar{a} \oplus \bar{m_1}]
 &=
 \left(
  \sum_{\forall \bar{m_2} \ne \bar{m_1}}
  A[\bar{a} \oplus \bar{m_2}] (0)
 \right)
 +
 \left(
  \sum_{\forall \bar{m_2} = \bar{m_1}}
  A[\bar{a} \oplus \bar{m_2}] (1)
 \right) \\
 \\
 &= 0 + A[\bar{a} \oplus \bar{m_1}] \\
 &= A[\bar{a} \oplus \bar{m_1}]
\end{align*}
Therefore $A \mmult{|\vec{m}|} \Ident^2(\vec{m}) = A$
\end{ppart}
\begin{ppart}
Let $A$ be a matrix with shape $|A| = \vec{n} \oplus \vec{a}$.
Then by the definition of multimatrix multiplication (Def. \ref{mm_mult}),
\[ \forall  \bar{n_1} \in \Dim(\vec{n}), \bar{a} \in \Dim(\vec{a}) : \]
\[
 (\Ident^2(\vec{n}) \mmult{|\vec{n}|} A)[\bar{n_1} \oplus \bar{a}]
 =
 \sum_{\forall \bar{n_2} \in \Dim(\vec{n})}
 \Ident^2(\vec{n})[\bar{n_1} \oplus \bar{n_2}]A[\bar{n_2} \oplus \bar{a}] 
\]
Then, by the definition of $\Ident^2(\vec{n})$ (Def. \ref{mm_ident}) it follows
that,
\begin{align*}
 (\Ident^2(\vec{n}) \mmult{|\vec{n}|} A)[\bar{n_1} \oplus \bar{a}]
 &=
 \left(
  \sum_{\forall \bar{n_2} \ne \bar{n_1}}
  (0) A[\bar{n_2} \oplus \bar{a}]
 \right)
 +
 \left(
  \sum_{\forall \bar{n_2} = \bar{n_1}}
  (1) A[\bar{n_2} \oplus \bar{a}]
 \right) \\
 \\
 &= 0 + A[\bar{n_1} \oplus \bar{a}] \\
 &= A[\bar{n_1} \oplus \bar{a}]
\end{align*}
Therefore $\Ident^2(\vec{n}) \mmult{|\vec{n}|} A = A$
\end{ppart}
\end{proof}

The square identity also pops up in the derivative of a multimatrix
with respect to itself.

\begin{theorem}[Self Derivative]
Let $X$ be some multimatrix. Then,
\[ \frac{dX}{dX} = \Ident^2(|X|) \]
\end{theorem}
\begin{proof}
Let $X$ be a multimatrix of shape $|X|=\vec{x}$.
By the derivative definition (Def. \ref{mm_derivative}),
\[
 \forall \bar{x_1} \in \Dim(\vec{x}),
         \bar{x_2} \in \Dim(\vec{x}):
\]
\[
 \frac{dX}{dX}[\bar{x_1} \oplus \bar{x_2}] = 
 \frac{\partial X[\bar{x_1}]}{\partial X[\bar{x_2}]}
\]
Since each element of $X$ is independent with respect to itself,
\begin{align*}
 \frac{dX}{dX}[\bar{x_1} \oplus \bar{x_2}]
 &= \left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{x_1} = \bar{x_2} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \Ident^2(\vec{x})[\bar{x_1} \oplus \bar{x_2}]
\end{align*}
Therefore $dX/dX = \Ident^2(|X|)$
\end{proof}

The cubic identity, strangely enough, appear in the derivative of elementwise
functions. Which is one of the reaspons I needed to expand my definition of
identities beyond $\Ident^2(\vec{v})$, beyond my crushing identity crisis.

\begin{theorem}[Elementwise Derivative]
Let $M$ be an element wise function on $X$. That is, Let $M(X)$ be a function
such that,
\[ M(X) : \mathbb{R}^{\vec{x}} \to \mathbb{R}^{\vec{x}} \]
And which applies the same scalar function to each element of $X$, 
\[ M(X)[\bar{x}] = m(X[\bar{x}]) \]
Also, let $M'$ be the elementwise function on $X$ which applies the derivative
of $M$'s associated scalar function to each element of $X$. That is,
\[ M'(X)[\bar{x}] = m'(X[\bar{x}]) \]
Then,
\[ \frac{dM}{dX} = \Ident^3(|X|) \mmult{||X||} M'(X) \]
\end{theorem}
\begin{proof}
By the definition of a multimatrix derivative,
\[ \forall \bar{m} \in \Dim(|M|), \bar{x} \in \Dim(|X|) : \]
\begin{align*}
 \frac{dM}{dX}[\bar{m} \oplus \bar{x}]
 &= \frac{\partial M[\bar{m}]}{\partial X[\bar{x}]} \\
 &= \frac{\partial m(X[\bar{m}])}{\partial X[\bar{x}]} \\
 &= \left\{
  \begin{array}{ll}
    m'(X[\bar{m}]) & \mbox{if } \bar{m} = \bar{x} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \delta_{\bar{m} \bar{x}} m'(X[\bar{m}])
\end{align*}
Since, by our definition of elementwise function, $|M| = |X|$, we can
say that, 
\begin{align*}
 (\Ident^3(|X|) \mmult{||X||} M'(X))[\bar{m} \oplus \bar{x}]
 &= \sum_{\forall \bar{y} \in \Dim(|X|)}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] M'(X)[\bar{y}] \\
 &= \sum_{\forall \bar{y} \in \Dim(|X|)}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] m'(X[\bar{y}]) \\
 &= \sum_{\forall \bar{y} = \bar{m} = \bar{x}}
 \Ident^3(|X|)[\bar{m} \oplus \bar{x} \oplus \bar{y}] m'(X[\bar{y}]) \\
 &= \left\{
  \begin{array}{ll}
    m'(X[\bar{m}]) & \mbox{if } \bar{m} = \bar{x} \\
    0 & \mbox{otherwise}
  \end{array}
 \right. \\
 &= \delta_{\bar{m} \bar{x}} m'(X[\bar{m}])
\end{align*}
Therefore $dM/dX = \Ident^3(|X|) \mmult{||X||} M'(X)$
\end{proof}

Multimatrix identities also tend to cancel eachother out when multiplied by
multiples of their base shape lengths.

\begin{landscape}
\begin{theorem}[Identity Contraction]
For all $m, n, k \in \mathbb{Z}^+$ and $\vec{x}$ as a valid multimatrix shape,
where $m > k$ and $n > k$,
\[ \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) = \Ident^{m+n-2k}(\vec{x}) \]
\end{theorem}
\begin{proof}
Consider that for some indicies
$\{\bar{a_1}, \bar{a_2}, \ldots \bar{a}_{m-k}\}$
and $\{\bar{b_1}, \bar{b_2}, \ldots \bar{b}_{n-k}\}$ where each
$\bar{a_i} \in \Dim(\vec{x})$ and each $\bar{b_i} \in \Dim(\vec{x})$,

\begin{align*}
&\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] = \\
&\sum_{\forall \bar{c_1}, \bar{c_2}, \ldots \bar{c_k} \in \Dim(\vec{x})}
\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c_1} \oplus \bar{c_2} \oplus \ldots]
\Ident^n(\vec{x})
[\bar{c_1} \oplus \bar{c_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots]
\end{align*}

Consider that $\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c_1} \oplus \bar{c_2} \oplus \ldots]$
will be 0 whenever the any $\bar{c_i} \ne \bar{c_j}$ - we can simplify that to all
the same $c$.

\begin{align*}
&\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] = \\
&\sum_{\forall \bar{c} \in \Dim(\vec{x})}
\Ident^m(\vec{x})
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{c} \oplus \bar{c} \oplus \ldots]
\Ident^n(\vec{x})
[\bar{c} \oplus \bar{c} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots]
\end{align*}

Consider also that there can be at most one $\bar{c}$ for which
$\bar{c} = \bar{a_1} = \bar{a_2} \ldots$. And since our right hand side is
the sum of products of ones and zeros, we can therefore conclude it is either 1 or 0.

We can futher reason that the right hand side can only be one when all of the
$\bar{a_i}$ and all of the $\bar{b_i}$ equal $\bar{c}$. Furthermore, if all
$\bar{a_i}$ and all $\bar{b_i}$ are equal then that $\bar{c}$ must exist since it can
be any value in the range of all $\bar{a_i}$ and $\bar{b_i}$ and this is not a
zero width multiplication ($k >= 1$). Therefore,

\begin{align*}
\left( \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right)
[\bar{a_1} \oplus \bar{a_2} \oplus \ldots \bar{b_1} \oplus \bar{b_2} \oplus \ldots] =
\left\{
  \begin{array}{ll}
    1 & \mbox{if } \bar{a_1} = \bar{a_2} = \ldots = \bar{b_1} = \bar{b_2} = \ldots \\
    0 & \mbox{otherwise}
  \end{array}
\right.
\end{align*} 

Which is an identity matrix! And since the dimensional rules of multimatrix 
multiplication show that,

\[
 \left|\left| \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) \right|\right|
 =
 ||\Ident^m(\vec{x})|| + ||\Ident^n(\vec{x})|| - 2k|\vec{x}|
\]

We've shown that,

\[ \Ident^m(\vec{x}) \mmult{k|\vec{x}|} \Ident^n(\vec{x}) = \Ident^{m+n-2k}(\vec{x}) \]
\end{proof}
\end{landscape}

A final useful theorem (definition?) happens to involve 0 overlap multiplication.
You'll notice that I very carefully constructed my definition to allow for this
definition (theorem?). Does it still count as a theorem if it's only true because
I worked over the definition of multimatrix multiplication to make it true? I feel
a bit like Tommy Wiseau writing my own sex scene into the script... twice... God,
what a tool.

\begin{theorem}[Zero Overlap Multiplication]
Anyways, if $|A| = \vec{a}$ and $|B| = \vec{b}$, for every $\bar{a} \in \Dim(\vec{a})$
and $\bar{b} \in \Dim(\vec{b}):$

\[ (A \mmult{0} B)[\bar{a} \oplus \bar{b}] = A[\bar{a}] B[\bar{b}] \]

\end{theorem}
\begin{proof}
\begin{align*}
	(A \mmult{0} B)[\bar{a} \oplus \bar{b}]
	&=
	\sum_{\forall \bar{v} \in \Dim(\left<\right>)}
	A[\bar{a} \oplus \bar{v}] B[\bar{v} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{v} \in \left\{\left<\right>\right\}}
	A[\bar{a} \oplus \bar{v}] B[\bar{v} \oplus \bar{b}] \\
	&=
	A[\bar{a} \oplus \left<\right>] B[\left<\right> \oplus \bar{b}] \\
	&=
	A[\bar{a}] B[\bar{b}] \\
\end{align*}
\end{proof}

\section*{Exercises}

\begin{exercise}
Let $A$, $B$, and $C$ be multimatricies such that $|A| = \left<1,2,3\right>$,
$|B| = \left<3,1,2\right>$ and $|C| = \left<3,2,1\right>$. For each of these
multiplications determine if it is valid, and if so, what the resulting
shape would be:
\begin{enumerate}
\item $A \mmult{2} B$
\item $B \mmult{2} A$
\item $A \mmult{2} C$
\item $A \mmult{1} C$
\end{enumerate}
\end{exercise}

\begin{exercise}
In which of these cases does $A \mmult{m} (B \mmult{n} C) = (A \mmult{m} B) \mmult{n} C$?
\begin{enumerate}
\item When,
	\begin{itemize}
		\item $|A| = \left<3, 7, 9\right>$
		\item $|B| = \left<9, 5\right>$
		\item $|C| = \left<5, 7, 3\right>$
		\item and $m = n = 1$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $|A| = \left<6, 6, 6\right>$
		\item $|B| = \left<6, 6, 6\right>$
		\item $|C| = \left<6, 6, 6\right>$
		\item and $m = n = 2$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $|A| = \left<3, 5, 3\right>$
		\item $|B| = \left<5, 3, 11\right>$
		\item $|C| = \left<11, 2, 7\right>$
		\item $m = 2$
		\item and $n = 1$
	\end{itemize}
\item When,
	\begin{itemize}
		\item $|A| = \left<3, 5, 3\right>$
		\item $|B| = \left<5, 3, 3, 5\right>$
		\item $|C| = \left<3, 3, 5, 2\right>$
		\item $m = 2$
		\item and $n = 3$
	\end{itemize}
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $|X| = \vec{x}$. Simplify the following where possible:
\begin{enumerate}
\item $\Ident^5(\vec{x}) \mmult{3 |\vec{x}|} \Ident^3(\vec{x})$
\item $\Ident^2(\vec{x}) \mmult{|\vec{x}|} X$
\item $\Ident^3(\vec{x}) \mmult{2|\vec{x}|} X$
\item $\Ident^2(\vec{x}) \mmult{|\vec{x}|} \left(X \mmult{|\vec{x}|} \Ident^3(|\vec{x}|)
				\right) \mmult{2|\vec{x}|} \Ident^3(|\vec{x}|)$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $M$ be some multimatrix such that $|M| = \left<5,2,3\right>$. Then what is
$dM/dM$?
\end{exercise}

\begin{exercise}
Let $Y = \tan(X)$ be the elementwise application of the scalar tangent function to
the elements of $X$, that is, for all $\bar{v} \in \Dim(|X|)$
\[ Y[\bar{v}] = \tan(X[\bar{v}]) \]
What is $dY/dX$?
\end{exercise}

\chapter{Other Operations With Multimatricies}

\begin{displayquote}
``When the light turns green, you go. When the light turns red, you stop.
But what do you do when the light turns blue with orange and lavender spots?''
- Shel Silverstein, 'A Light in the Attic'
\end{displayquote}

Other operations include...

\section{Scalars}

\begin{definition}[Multimatrix Scalar Multiplication]
Let $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$. If
\[ sA = As = B \]
Then
\[ \forall \bar{x} \in \Dim(\vec{x}):
   B[\bar{x}] = (s)(A[\bar{x}]) \]
In other words, multiplying $A$ by a scalar $s$ simply multiplies each of
it's elements by that scalar.
\end{definition}

Note that by our definitions of scalar and multimatrix derivatives and multiplication,
we can model scalars in either case as multimatricies with shape $\{\}$.

\begin{theorem}[Empty Shape Multimatrix and Scalar Multiplication Equivalence]
\label{s_mm_mult_equiv}
For any multiplication operation defined between multimatricies and scalars,
the scalar may be replaced by an equivalent empty-shaped multimatrix who's
single value is that of the scalar.

In other words, let $s \in \mathbb{R}$ and $S \in \mathbb{R}^{\{\}}$ such that
$S[\{\}] = s$. Then, for any multimatrix $A$, 
\[ sA = As = S \mmult{0} A = A \mmult{0} S \]
\end{theorem}
\begin{proof}
Let $|A| = \vec{a}$. Then for all $\bar{a} \in \Dim(\vec{a})$,
\begin{align*}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= A[\bar{a}](s) \\
	&= (As)[\bar{a}]
\end{align*}
So $sA = As$. Also,
\begin{align}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= S[\{\}]A[\bar{a}] \\
	&= \sum_{\forall \bar{c} \in \Dim(\{\})}
		S[\{\} \oplus \bar{c}]A[\bar{c} \oplus \bar{a}] \\
	&= (S \mmult{0} A)[\{\} \oplus \bar{a}] \\
	&= (S \mmult{0} A)[\bar{a}] \\
\end{align}
Note that step 3 is valid because $\bar{c}$ can only be $\{\}$ and
$\{\} \oplus \bar{a} = \bar{a}$. So, $sA = S \mmult{0} A$.
Finally, by similiar reasoning,
\begin{align*}
	(sA)[\bar{a}]
	&= (s)A[\bar{a}] \\
	&= A[\bar{a}](s) \\
	&= A[\bar{a}]S[\{\}] \\
	&= \sum_{\forall \bar{c} \in \Dim(\{\})}
		A[\bar{a} \oplus \bar{c}] S[\bar{c} \oplus \{\}] \\
	&= (A \mmult{0} S)[\bar{a} \oplus \{\}] \\
	&= (A \mmult{0} S)[\bar{a}]
\end{align*}
So $sA = A \mmult{0} S$. Therefore we've shown that
$sA = As = S \mmult{0} A = A \mmult{0} S$
\end{proof}

Noticing this lead me to suspect that the same might be true of my definitions
of the derivative rules. A quick check confirmed that the dimensionality
worked out, and it seems intuitive that we would always be able to treat scalars
as empty-shape multimatricies if multiplication workes, but let's formalize that
assertion.

\begin{theorem}[Empty Shape Multimatrix and Scalar Domain Derivative Equivalence]
\label{s_mm_domain_equiv}
Let $F(x)$ be a function $F : \mathbb{R} \to \mathbb{R}^{\vec{f}}$. And let
$G(X)$ be an equivalent function, $G : \mathbb{R}^{\{\}} \to \mathbb{R}^{\vec{f}}$
such that for all $X \in \mathbb{R}^{\{\}}$ it is true that $G(X) = F(X[\{\}])$
Then,
\[ \frac{dF(x)}{dx} = \frac{dG(X)}{dX} \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[Empty Shape Multimatrix and Scalar Range Derivative Equivalence]
\label{s_mm_range_equiv}
Let $f(X)$ be a function $f : \mathbb{R}^{\vec{x}} \to \mathbb{R}$. And let
$F(X)$ be an equivalent function, $F : \mathbb{R}^{\vec{x}} \to \mathbb{R}^{\{\}}$
such that for all $X \in \mathbb{R}^{\vec{x}}$ it is true that $F(X)[\{\}] = f(X)$
Then,
\[ \frac{df(X)}{dX} = \frac{dF(X)}{dX} \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\section{Addition}

\begin{definition}[Multimatrix Addition]
Let $A, B, C \in \mathbb{R}^{\vec{x}}$. If
\[ A + B = C \]
Then
\[ \forall \bar{x} \in \Dim(\vec{x}):
   C[\bar{x}] = A[\bar{x}] + B[\bar{x}] \]
In other words, adding two multimatricies (which must be of the same shape) results
in a multimatrix of the same shape with elements which are the sum of the corresponding
elements of the added multimatricies.
\end{definition}

\begin{definition}[Multimatrix Subtraction]
Let $A, B \in \mathbb{R}^{\vec{x}}$. Then,
\[ A - B = A + (-1)B \]
It should be obvious that, as with scalars, multimatrix subtraction is the
inverse of addition. In other words, $(A + B) - B = A$
\end{definition}

As with scalar, vector, or regular matrix addition, multimatrix addition is
communicative. 
\begin{theorem}[Communicative Property of Multimatrix Addition]
Let $A, B \in \mathbb{R}^{\vec{x}}$. Then,
\[ A + B = B + A \]
\end{theorem}
\begin{proof}
Let $C = A + B$ then,
\[ \forall \bar{x} \in \Dim(\vec{x}) : C[\bar{x}] = A[\bar{x}] + B[\bar{x}] \]
By standard scalar addition rules that also means that,
$C[\bar{x}] = B[\bar{x}] + A[\bar{x}]$. So $C = B + A = A + B$.
\end{proof}

It's similarly trial to show that multimatrix addition is associative.

\begin{theorem}[Associative Property of Multimatrix Addition]
Let $A, B, C \in \mathbb{R}^{\vec{x}}$. Then,
\[ (A + B) + C = A + (B + C) \]
\end{theorem}
\begin{proof}
For all $\bar{x} \in \Dim(\vec{x})$,
\begin{align*}
\left( (A+B)+C \right)[\bar{x}]
&= (A[\bar{x}]+B[\bar{x}])+C[\bar{x}] \\
&= A[\bar{x}]+(B[\bar{x}]+C[\bar{x}]) \\
&= \left( A+(B+C) \right)[\bar{x}]
\end{align*}
\end{proof}

A little less obvious is that multimatrix multiplication can be distributed over
it's addition.

\begin{landscape}
\begin{theorem}[Multimatrix on Multimatrix Distributive Property]
If $|A| = \vec{a} \oplus \vec{n}$ and $|B|=|C|=\vec{n} \oplus \vec{r}$ then,
\[ A \mmult{n} (B + C) = A \mmult{n} B + A \mmult{n} C \]
Similarly, if $|C| = \vec{n} \oplus \vec{c}$ and $|A|=|B|=\vec{l} \oplus \vec{n}$ then,
\[ (A + B) \mmult{n} C = A \mmult{n} C + B \mmult{n} C \]
\end{theorem}
\begin{proof}
\begin{ppart}
Let $|A| = \vec{a} \oplus \vec{n}$ and $|B|=|C|=\vec{n} \oplus \vec{r}$.
So, $\forall \bar{a} \in \Dim(\vec{a}), \bar{r} \in \Dim(\vec{r}) :$
\begin{align*}
 \left( A \mmult{n} (B + C) \right)[\bar{a} \oplus \bar{r}]
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] (B + C)[\bar{n} \oplus \bar{r}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] (B[\bar{n} \oplus \bar{r}] + C[\bar{n} \oplus \bar{r}]) \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] B[\bar{n} \oplus \bar{r}]
  + A[\bar{a} \oplus \bar{n}]C[\bar{n} \oplus \bar{r}] \\
 &= \left( \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}] B[\bar{n} \oplus \bar{r}] \right)
    +
    \left( \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{a} \oplus \bar{n}]C[\bar{n} \oplus \bar{r}] \right) \\
 &= \left( A \mmult{n} B \right)[\bar{a} \oplus \bar{r}] +
     \left( A \mmult{n} C \right)[\bar{a} \oplus \bar{r}]
\end{align*}
So $A \mmult{n} (B + C) = A \mmult{n} B + A \mmult{n} C$
\end{ppart}
\begin{ppart}
Let $|C| = \vec{n} \oplus \vec{c}$ and $|A|=|B|=\vec{l} \oplus \vec{n}$.
So, $\forall \bar{c} \in \Dim(\vec{c}), \bar{l} \in \Dim(\vec{l}) :$
\begin{align*}
 \left( (A + B) \mmult{n} C \right)[\bar{l} \oplus \bar{c}]
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    (A+B)[\bar{l} \oplus \bar{n}] C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    (A[\bar{l} \oplus \bar{n}]+B[\bar{l} \oplus \bar{n}])C[\bar{n} \oplus \bar{c}] \\
 &= \sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]
    +B[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}] \\
 &= \left(\sum_{\forall \bar{n} \in \Dim(\vec{n})}
    A[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]\right)
    +
    \left(\sum_{\forall \bar{n} \in \Dim(\vec{n})}
    B[\bar{l} \oplus \bar{n}]C[\bar{n} \oplus \bar{c}]\right) \\
 &= \left(A \mmult{n} C\right)[\bar{l} \oplus \bar{c}]
    +\left(B \mmult{n} C\right)[\bar{l} \oplus \bar{c}]
\end{align*}
So $(A+B) \mmult{n} C = A \mmult{n} C + B \mmult{n} C$ 
\end{ppart}
\end{proof}
\end{landscape}

But it should be obvious that that the same applies to scalar 
multiplication distributing over multimatrix addition..
\begin{theorem}[Scalar on Multimatrix Distributive Property]
If $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$ then,
\[ s(A + B) = sA + sB \]
\end{theorem}
\begin{proof}
Let $s \in \mathbb{R}$ and $A, B \in \mathbb{R}^{\vec{x}}$.
Then for all $\bar{x} \in \Dim(\vec{x})$
\begin{align*}
(s(A+B))[\bar{x}]
&= (s)((A+B)[\bar{x}]) \\
&= (s)(A[\bar{x}] + B[\bar{x}]) \\
&= (s)(A[\bar{x}]) + s(B[\bar{x}])
\end{align*}
Therefore $s(A+B) = sA + sB$
\end{proof}

\section{Transposition}

\begin{definition}[Multimatrix Transposition]
Let $|X| = \vec{l} \oplus \vec{r}$ where $|\vec{l}| = l$ and $|\vec{r}| = r$
\[ |\Tran(X, l)| = \vec{r} \oplus \vec{l} \]
And,
\[ \forall \bar{l} \in \Dim(\vec{l}), \bar{r} \in \Dim(\vec{r}) : \]
\[ \Tran(X, l)[\bar{r} \oplus \bar{l}] = X[\bar{l} \oplus \bar{r}] \]
\end{definition}

\begin{theorem}[Inverse Transpose]
If $|X| = \vec{l} \oplus \vec{r}$
\[ \Tran(\Tran(X, |\vec{l}|), |\vec{r}|) = X \]
\end{theorem}
\begin{proof}
Let $|X| = \vec{l} \oplus \vec{r}$ then
$\forall \bar{l} \in \Dim(\vec{l}), \bar{r} \in \Dim(\vec{r}) : $
\[ \Tran(X, |\vec{l}|)[\bar{r} \oplus \bar{l}] = X[\bar{l} \oplus \bar{r}] \]
And so,
\begin{align*}
	\Tran(\Tran(X, |\vec{l}|), |\vec{r}|)[\bar{l} \oplus \bar{r}]
	&= \Tran(X, |\vec{l}|)[\bar{r} \oplus \bar{l}] \\
	&= X[\bar{l} \oplus \bar{r}] \\
\end{align*}
Therefore, 
\[ \Tran(\Tran(X, |\vec{l}|), |\vec{r}|) = X \]
\end{proof}

\begin{theorem}[Identity Transpose]
\[ \Tran(\Ident^n(\vec{x}), k|\vec{x}|) = \Ident^n(\vec{x}) \]
\end{theorem}
\begin{proof}
Consider that for some ordered set of indicies
$\{\bar{x}_1, \bar{x}_2, \ldots \bar{x}_n\} = X$ such that each
$\bar{x}_i \in X \implies \bar{x}_i \in \Dim(\vec{x})$.
Let $X'$ be some ordered permutation of $X$, then we can reason that,
\begin{align*}
 \Ident^n(\vec{x})\left[\bigoplus X'\right]
 &= 
	\left\{
  \begin{array}{ll}
    1 & \mbox{if all } \bar{x}_i \in X' \mbox{ are equal.}\\
    0 & \mbox{otherwise}
  \end{array}
	\right.\\
 &= 
	\left\{
  \begin{array}{ll}
    1 & \mbox{if all } \bar{x}_i \in X \mbox{ are equal.}\\
    0 & \mbox{otherwise}
  \end{array}
	\right.\\
 &=\Ident^n(\vec{n})\left[\bigoplus X\right]
\end{align*}
Since for
$X' = \left\{\bar{x}_{k+1}, \bar{x}_{k+2} \ldots \bar{x}_{n},
\bar{x}_1, \bar{x}_2 \ldots \bar{x}_k\right\}$
\begin{align*}
  \Tran(\Ident^n(\vec{x}), k|\vec{x}|)\left[\bigoplus X'\right]
	&=
  \Ident^n(\vec{x})\left[\bigoplus X\right]\\
	&=
  \Ident^n(\vec{x})\left[\bigoplus X'\right]
\end{align*}
Therefore
$\Tran(\Ident^n(\vec{x}), k|\vec{x}|) = \Ident^n(\vec{x})$
\end{proof}

\begin{theorem}[Transposition of Multimatrix Multiplication]
\label{mm_tran_mult}
Let $|A| = \vec{a} \oplus \vec{c}$ and $|B| = \vec{c} \oplus \vec{b}$ then
\[
 \Tran(A \mmult{|\vec{c}|} B, |\vec{a}|)
 =  \Tran(B, |\vec{c}|) \mmult{|\vec{c}|} \Tran(A, |\vec{a}|)
\]
\end{theorem}
\begin{proof}
Let $\bar{a} \in \Dim(\vec{a})$ and $\bar{b} \in \Dim(\vec{b})$.
Then,
\begin{align*}
	\Tran(A \mmult{|\vec{c}|} B, |\vec{a}|)[\bar{b} \oplus \bar{a}]
	&=
	\left(A \mmult{|\vec{c}|} B \right)[\bar{a} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	A[\bar{a} \oplus \bar{c}] B[\bar{c} \oplus \bar{b}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	B[\bar{c} \oplus \bar{b}] A[\bar{a} \oplus \bar{c}] \\
	&=
	\sum_{\forall \bar{c} \in \Dim(\vec{c})}
	\Tran(B, |\vec{c}|)[\bar{b} \oplus \bar{c}]
	\Tran(A, |\vec{a}|)[\bar{c} \oplus \bar{a}] \\
	&=
	\left(\Tran(B, |\vec{c}|) \mmult{|\vec{c}|} \Tran(A, |\vec{a}|) \right)
	[\bar{b} \oplus \bar{a}]
\end{align*}
\end{proof}

\begin{theorem}[Communicativity of scalar Multiplication and Transposition]
\[ s\Tran(A, x) = \Tran(sA, x) \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}[Stacked Transposition]
\[ \Tran(\Tran(A, x), y) = \Tran(A, x+y) \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\section*{Exercises}
TODO

\chapter{Generic Derivative Rules}

\begin{displayquote}
``It goes without saying that all writers owe a debt to their teachers... who
naturally bear all responsibility for any errors contained herein...''
- Paul Rentel, 'Manifolds, Tensors, and Forms'
\end{displayquote}

\begin{theorem}[Transpose Derivative]
\label{tran_derivative}
$|X| = \vec{l} \oplus \vec{r}$
\[
 \frac{d\Tran(X, |\vec{l}|)}{dX} =
 \Tran(\Ident^2(\vec{r}) \mmult{0} \Ident^2(\vec{l}), |\vec{r}|)
\]
\end{theorem}
\begin{proof}
\begin{align*}
 \frac{d\Tran(X, |\vec{l}|)}{dX}
 [\bar{r_1} \oplus \bar{l_1} \oplus \bar{l_2} \oplus \bar{r_2}]
 &= \delta_{\bar{r_1}\bar{r_2}} \delta_{\bar{l_1}\bar{l_2}} \\
 &= \delta_{\bar{r_2}\bar{r_1}} \delta_{\bar{l_1}\bar{l_2}} \\
 &=
  \Ident^2(|\vec{r}|)[\bar{r_2}\oplus\bar{r_1}]
  \Ident^2(|\vec{l}|)[\bar{l_1}\oplus\bar{l_2}] \\
 &=
   \left(
    \Ident^2(|\vec{r}|)
    \mmult{0}
    \Ident^2(|\vec{l}|)
  \right)
  [\bar{r_2}\oplus\bar{r_1}\oplus\bar{l_1}\oplus\bar{l_2}] \\
 &=
  \Tran(
    \Ident^2(|\vec{r}|)
    \mmult{0}
    \Ident^2(|\vec{l}|)
  , |\vec{r}|) 
 [\bar{r_1} \oplus \bar{l_1} \oplus \bar{l_2} \oplus \bar{r_2}]
\end{align*}
\end{proof}

\begin{theorem}[Right Multiplicative Derivative]
\label{right_mult_derivative}
\[ \frac{d(A \mmult{n} X)}{dX} = A \mmult{n} \Ident^2(|X|) \]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{landscape}
\begin{theorem}[Left Multiplicative Derivative]
\label{left_mult_derivative}
\[ \frac{d(X \mmult{n} A)}{dX} = \Ident^2(|X \mmult{n} A|) \mmult{n} \Tran(A, n) \]
\end{theorem}
\begin{proof}
Let $|X| = \vec{x} \oplus \vec{n}$ and $|A| = \vec{n} \oplus \vec{a}$. Then,
for all
$\bar{x}_1, \bar{x}_2 \in \Dim(\vec{x}), \bar{a}_1 \in \Dim(\vec{a}),
\bar{n}_1 \in \Dim(\vec{n}):$
\begin{align*}
	\frac{d(X \mmult{n} A)}{dX}
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	&= 
	\frac{
		\partial (X \mmult{n} A) [\bar{x}_1 \oplus \bar{a}_1]
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\frac{
		\partial \left(
			\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
			X[\bar{x}_1 \oplus \bar{n}_2] A[\bar{n}_2 \oplus \bar{a}_1]
		\right)
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
	\frac{
		\partial \left(
			X[\bar{x}_1 \oplus \bar{n}_2] A[\bar{n}_2 \oplus \bar{a}_1]
		\right)
	}{\partial X[\bar{x}_2 \oplus \bar{n}_1]} \\
	&= 
	\sum_{\forall \bar{n}_2 \in \Dim(\vec{n})}
	\delta_{\bar{x}_1\bar{x}_2}
	\delta_{\bar{n}_1\bar{n}_2}
	A[\bar{n}_2 \oplus \bar{a}_1] \\
	&= 
	\delta_{\bar{x}_1\bar{x}_2}
	A[\bar{n}_1 \oplus \bar{a}_1]
\end{align*}
We can also show that,
\begin{align*}
	\left(\Ident^2(|X \mmult{n} A|) \mmult{n} \Tran(A, n)\right)
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a}_1)}
	\Ident^2(|X \mmult{n} A|)[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{a}_2]
	\Tran(A, n)[\bar{a}_2 \oplus \bar{n}_1] \\
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a})}
	\delta_{\bar{x}_1 \bar{x}_2} \delta_{\bar{a}_1 \bar{a}_2}
	\Tran(A, n)[\bar{a}_2 \oplus \bar{n}_1] \\
	&=
	\sum_{\forall \bar{a}_2 \in \Dim(\vec{a})}
	\delta_{\bar{x}_1 \bar{x}_2} \delta_{\bar{a}_1 \bar{a}_2}
	A[\bar{n}_1 \oplus \bar{a}_2] \\
	&=
	\delta_{\bar{x}_1 \bar{x}_2}
	A[\bar{n}_1 \oplus \bar{a}_1]
\end{align*}
Then since,
\[
	\frac{d(X \mmult{n} A)}{dX}
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
	=
	\left(\Ident^2(|X \mmult{n} A|) \mmult{n} \Tran(A, n)\right)
	[\bar{x}_1 \oplus \bar{a}_1 \oplus \bar{x}_2 \oplus \bar{n}_1]
\]
we have shown that,
\[
	\frac{d(X \mmult{n} A)}{dX}
	=
	\Ident^2(|X \mmult{n} A|) \mmult{n} \Tran(A, n)
\]
\end{proof}
\end{landscape}


\begin{theorem}[Addition Rule]
\[ \frac{d(F(X) + G(X))}{dX} = \frac{dF(X)}{dX} + \frac{dG(X)}{dX} \]
\end{theorem}
\begin{proof}
Let $|F(X)| = |G(X)| = \vec{f}$ and $|X| = \vec{x}$. Then,
$\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):$
\begin{align*}
	\frac{d(F(X) + G(X))}{dX}[\bar{f} \oplus \bar{x}]
	&=
	\frac{\partial (F(X) + G(X))[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial (F(X)[\bar{f}] + G(X)[\bar{f}])}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial F(X)[\bar{f}]}{\partial X[\bar{x}]} +
	\frac{\partial G(X)[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{dF(X)}{dX}[\bar{f} \oplus \bar{x}]+
	\frac{dG(X)}{dX}[\bar{f} \oplus \bar{x}]
\end{align*}
So $d(F(X) + G(X))/dX = dF(X)/dX + dG(X)/dX$
\end{proof}

A rule for derivatives involving scalar multiplication also seems generally
useful. Of course, by the Theorem \ref{s_mm_mult_equiv}, this is just a very specific
case of Theorem \ref{right_mult_derivative} but the proof is simple and worth
the additional ink, I think,
\begin{theorem}[Scalar Multiplication Rule]
\[ \frac{d(sF(X))}{dX} = s\frac{dF(X)}{dX} \]
\end{theorem}
\begin{proof}
Let $|F(X)| = \vec{f}$ and $|X| = \vec{x}$. Then
$\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):$
\begin{align*}
	\frac{d(sF(X))}{dX}[\bar{f} \oplus \bar{x}]
	&=
	\frac{\partial (sF(X))[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	\frac{\partial (s)(F(X)[\bar{f}])}{\partial X[\bar{x}]} \\
	&=
	s\frac{\partial F(X)[\bar{f}]}{\partial X[\bar{x}]} \\
	&=
	s\frac{dF(X)}{dX}[\bar{f} \oplus \bar{x}]
\end{align*}
Therefore $d(sF(X))/dX = s(dF(X)/dX)$
\end{proof}

\begin{landscape}
\begin{theorem}[Multiplication Rule]
Let $|F| = \vec{f} \oplus \vec{c}$, $|G| = \vec{c} \oplus \vec{g}$,
and $|X| = \vec{x}$. Then,
\begin{align*}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX} =
 F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} +
 \Tran\left(
   \Tran(G(X), |\vec{c}|)
     \mmult{|\vec{c}|}
   \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right),
   |\vec{g} \oplus \vec{x}|
 \right)
\end{align*}
\end{theorem}
\begin{proof}
\[
 \forall
  \bar{f} \in \Dim(\vec{f}),
  \bar{g} \in \Dim(\vec{g}),
  \bar{x} \in \Dim(\vec{x})
 :
\]
\begin{align*}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
 &= \frac{
       \partial \left(F(X) \mmult{|\vec{c}|} G(X)\right)[\bar{f} \oplus \bar{g}]
    }{
       \partial X[\bar{x}]
    } \\
 &= \frac{
       \partial \left(
        \sum_{\forall \bar{c} \in \Dim(\vec{c})}
         F(X)[\bar{f} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{g}]
      \right)
    }{
       \partial X[\bar{x}]
    } \\
 &= \sum_{\forall \bar{c} \in \Dim(\vec{c})}
    \frac{
      \partial F(X)[\bar{f} \oplus \bar{c}] G(X)[\bar{c} \oplus \bar{g}]
    }{
      \partial X[\bar{x}]
    }\\
 &= \sum_{\forall \bar{c} \in \Dim(\vec{c})}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]}
    +
    \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
   \left(
    \sum_{\forall \bar{c}}
      F(X)[\bar{f} \oplus \bar{c}]
      \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]}
   \right)
   +
   \left(
    \sum_{\forall \bar{c}}
      \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
      G(X)[\bar{c} \oplus \bar{g}]
   \right) \\
 &=
    l + r
\end{align*}
Which each become,
\begin{align*}
 l
 &=
  \sum_{\forall \bar{c}}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{\partial G(X)[\bar{c} \oplus \bar{g}]}{\partial X[\bar{x}]} \\
 &=
  \sum_{\forall \bar{c}}
    F(X)[\bar{f} \oplus \bar{c}]
    \frac{dG(X)}{dX}[\bar{c} \oplus \bar{g} \oplus \bar{x}] \\
 &=
   \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right)
   [\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{align*}
\begin{align*}
 r
 &=
  \sum_{\forall \bar{c}}
    \frac{\partial F(X)[\bar{f} \oplus \bar{c}]}{\partial X[\bar{x}]}
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
  \sum_{\forall \bar{c}}
    \frac{dF(X)}{dX}[\bar{f} \oplus \bar{c} \oplus \bar{x}]
    G(X)[\bar{c} \oplus \bar{g}] \\
 &=
  \sum_{\forall \bar{c}}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
      [\bar{c} \oplus \bar{x} \oplus \bar{f}]
    \Tran(G(X), |\vec{c}|)[\bar{g} \oplus \bar{c}] \\
 &=
  \sum_{\forall \bar{c}}
    \Tran(G(X), |\vec{c}|)[\bar{g} \oplus \bar{c}]
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
      [\bar{c} \oplus \bar{x} \oplus \bar{f}] \\
 &=
  \left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
  \right)[\bar{g} \oplus \bar{x} \oplus \bar{f}] \\
 &= 
  \Tran\left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
    , |\vec{g} \oplus \vec{x}|
  \right)[\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{align*}
Therefore 
\begin{alignat*}{3}
 \frac{d\left(F(X) \mmult{|\vec{c}|} G(X)\right)}{dX}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
  &=&&
  \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right)
  [\bar{f} \oplus \bar{g} \oplus \bar{x}] \\
  &&&+
  \Tran\left(
    \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
    \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
    , |\vec{g} \oplus \vec{x}|
  \right)[\bar{f} \oplus \bar{g} \oplus \bar{x}] \\
  &=&&
  \left\{
    \begin{array}{l}
      \left( F(X) \mmult{|\vec{c}|} \frac{dG(X)}{dX} \right) \\
      +
      \Tran\left(
        \Tran(G(X), |\vec{c}|) \mmult{|\vec{c}|}
        \Tran\left(\frac{dF(X)}{dX}, |\vec{f}|\right)
        , |\vec{g} \oplus \vec{x}|
      \right)
    \end{array}
  \right\}
  [\bar{f} \oplus \bar{g} \oplus \bar{x}]
\end{alignat*}
\end{proof}
\end{landscape}

\begin{theorem}[Chain Rule]
\label{mmm_chain_rule}
Let $F(G)$ be a function $\mathbb{R}^{\vec{g}} \rightarrow \mathbb{R}^{\vec{f}}$
and $G(X)$ be a function $\mathbb{R}^{\vec{x}} \rightarrow \mathbb{R}^{\vec{g}}$.
Then,
\[
\frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{|\vec{g}|} \frac{dG}{dX}
\]
\end{theorem}
\begin{proof}
By the definition of a derivative (\ref{mm_derivative}),
\[
\forall \bar{f} \in \Dim(\vec{f}), \bar{g} \in \Dim(\vec{g}):
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
= \frac{\partial F(G)[\bar{f}]}{\partial G[\bar{g}]}
\]
\[
\forall \bar{g} \in \Dim(\vec{g}), \bar{x} \in \Dim(\vec{x}):
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
= \frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
By the scalar total derivative rule \cite{wiki:totalderiv},
\[
\frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{\partial F(G)[\bar{f}]}{\partial G[\bar{g}]}
\frac{\partial G(X)[\bar{g}]}{\partial X[\bar{x}]}
\]
Substituting in our above definitions this yields,
\[
\frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
And again, by the definition of a derivative (\ref{mm_derivative}),
\[
\forall \bar{f} \in \Dim(\vec{f}), \bar{x} \in \Dim(\vec{x}):
\frac{dF(G(X))}{dX}[\bar{f} \oplus \bar{x}]
= \frac{\partial F(G(X))[\bar{f}]}{\partial X[\bar{x}]}
\]
Therefore,
\[
\frac{dF(G(X))}{dX}[\bar{f} \oplus \bar{x}]
= \sum_{\forall \bar{g} \in \Dim(\vec{g})} 
\frac{dF(G)}{dG}[\bar{f} \oplus \bar{g}]
\frac{dG(X)}{dX}[\bar{g} \oplus \bar{x}]
\]
Which, taking a look at the definition of multidimensional matrix multiplication
(\ref{mm_mult}), tells us that,
\[
\frac{dF(G(X))}{dX} = \frac{dF}{dG} \mmult{|\vec{g}|} \frac{dG}{dX}
\]
\end{proof}

\chapter{Machine Learning Specific Derivatives}

\begin{displayquote}
``I'm not saying Machine Learning is a portal to a demon universe, I'm just saying
that some doors are best left unopened.'' - James Mickens, AKA `Galactic Viceroy of Research Excellence', USENIX Security '18-Q
\end{displayquote}

\begin{definition}[Sum]
Let $\Sum(X)$ be the function $\Sum : \mathbb{R}^{\vec{x}} \to \mathbb{R}$
such that \[ \Sum(X) = \sum_{\forall \bar{x} \in \Dim(\vec{x})} X[\bar{x}] \]
\end{definition}

\begin{theorem}[Derivative of Sum]
\[\frac{d\Sum(X)}{dX} = 1^{|X|}\]
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{definition}[Sum of Squares]
Let $\SoS(X)$ be the function $\SoS : \mathbb{R}^{\vec{x}} \to \mathbb{R}$
such that 
\[ \SoS(X) = \sum_{\forall \bar{x} \in \Dim(\vec{x})} X[\bar{x}]^2 \]
\end{definition}

\begin{theorem}[Derivative of Sum of Squares]
\[ \frac{d \SoS(X)}{dX} = 2X \]
\end{theorem}
\begin{proof}
Notice that $\SoS(X) = X \mmult{|\vec{x}|} X$...

TODO
\end{proof}

\begin{definition}[Softmax]
Let $\Softmax(X)$ be the function
$\Softmax : \mathbb{R}^{\vec{X}} \to \mathbb{R}^{\vec{X}}$
such that $\forall \bar{x} \in \Dim(\vec{x}):$
\[
  \Softmax(X)[\bar{x}] = \frac{
		\exp(X[\bar{x}])
	}{
		\sum_{\forall \bar{c} \in \Dim(\vec{x})} \exp(X[\bar{c}])
	}
\]
\end{definition}

\begin{landscape}
\begin{theorem}[Derivative of Softmax]
\label{softmax_derivative}
\begin{align*}
	\frac{d\Softmax(X)}{dX}
	&=
	\Ident^3(|X|) \mmult{||X||} \Softmax(X)
  -\Softmax(X) \mmult{0} \Softmax(X) \\
	&=
	\left(\Ident^3(|X|) - \Softmax(X) \mmult{0} \Ident^2(|X|) \right)
	\mmult{||X||} \Softmax(X)
\end{align*}
\end{theorem}
\begin{proof}
Notice that 
\[
	\Softmax(X) = \exp(X) \mmult{0} \left( \Sum(\exp(X)) \right)^{-1}
\]
So,
\begin{alignat}{3}
	\frac{d\Softmax(X)}{dX}
	&=&&
	\frac{d \left( \exp(X) \mmult{0} \left( \Sum(\exp(X)) \right)^{-1} \right)}{dX} \\
	&=&&
	\exp(X) \mmult{0} \frac{d \left( \Sum(\exp(X)) \right)^{-1}}{dX}
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\left( \Sum(\exp(X)) \right)^{-1}
			,0
		\right)
		\mmult{0}
		\Tran\left(
			\frac{d \exp(X)}{dX}
			,|\vec{x}|
		\right)
		,|\{\} \oplus \vec{x}|
	\right) \\
	&=&&
	\exp(X) \mmult{0} \frac{d \left( \Sum(\exp(X)) \right)^{-1}}{dX}
	\nonumber\\&&&+
	\Tran\left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Tran\left(
			\frac{d \exp(X)}{dX}
			,|\vec{x}|
		\right)
		,|\vec{x}|
	\right) \\
	&=&&
	\exp(X) \mmult{0} \left(
		-\left( \Sum(\exp(X)) \right)^{-2}
		\mmult{0}
		\frac{d\Sum(\exp(X))}{dX}
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Tran\left(
			\Ident^3(|X|) \mmult{||X||} \exp(X)
			,|\vec{x}|
		\right)
		,|\vec{x}|
	\right) \\
	&=&&
	\exp(X) \mmult{0} \left(
		-\left( \Sum(\exp(X)) \right)^{-2}
		\mmult{0}
		1^{|X|}
		\mmult{||X||}
		\Ident^3(|X|)
		\mmult{||X||}
		\exp(X)
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\left( \Sum(\exp(X)) \right)^{-1}
			\mmult{0}
			\Ident^3(|X|) \mmult{||X||} \exp(X)
		,|\vec{x}|
		\right)
		,|\vec{x}|
	\right) \\
	&=&&
	-\Softmax(X) \mmult{0} \left(
		\left( \Sum(\exp(X)) \right)^{-1}
		\mmult{0}
		\Ident^2(|X|)
		\mmult{||X||}
		\exp(X)
	\right)
	\nonumber\\&&&+
	\Tran\left(
		\Tran\left(
			\Ident^3(|X|) \mmult{||X||} \Softmax(X)
		,|\vec{x}|
		\right)
	,|\vec{x}|
	\right) \\
	&=&&
	-\Softmax(X) \mmult{0} \Softmax(X)
	+
	\Tran\left(
		\Ident^3(|X|) \mmult{||X||} \Softmax(X)
		,2|\vec{x}|
	\right) \\
	&=&&
	\Ident^3(|X|) \mmult{||X||} \Softmax(X)
  -\Softmax(X) \mmult{0} \Softmax(X) \\
	&=&&
	\Ident^3(|X|) \mmult{||X||} \Softmax(X)
	-\Softmax(X) \mmult{0} \Ident^2(|X|) \mmult{||X||} \Softmax(X) \\
	&=&&
	\left(\Ident^3(|X|) - \Softmax(X) \mmult{0} \Ident^2(|X|) \right)
	\mmult{||X||} \Softmax(X)
\end{alignat}
Let's just verify that against the Wikipedia \cite{wiki:softmax} result:
\begin{align*}
	\frac{\partial \Softmax(X)[\bar{x}_1]}{\partial X[\bar{x}_2]}
	&=
	\left\{
		\left(\Ident^3(|X|) - \Softmax(X) \mmult{0} \Ident^2(|X|) \right)
		\mmult{||X||} \Softmax(X)
	\right\}
	[\bar{x}_1 \oplus \bar{x}_2] \\
	&=
	\sum_{\forall \bar{x}_3}
	\left(\Ident^3(|X|) - \Softmax(X) \mmult{0} \Ident^2(|X|) \right)
	[\bar{x}_1 \oplus \bar{x}_2 \oplus \bar{x}_3]
	\Softmax(X)[\bar{x}_3] \\
	&=
	\sum_{\forall \bar{x}_3}
	\left(
		\delta_{\bar{x}_1 \bar{x}_2 \bar{x}_3}
		- \Softmax(X)[\bar{x}_1] \delta_{\bar{x}_2 \bar{x}_3}
	\right)
	\Softmax(X)[\bar{x}_3] \\
	&=
	(\delta_{\bar{x}_1 \bar{x}_2} - \Softmax(X)[\bar{x}_1])
	\Softmax(X)[\bar{x}_2] \\
	&= \left\{
  \begin{array}{ll}
    (1-\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    (0-\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= \left\{
  \begin{array}{ll}
    \Softmax(X)[\bar{x}_1]-\Softmax(X)[\bar{x}_1]^2
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    -\Softmax(X)[\bar{x}_1])\Softmax(X)[\bar{x}_2]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= \left\{
  \begin{array}{ll}
    (1-\Softmax(X)[\bar{x}_2])\Softmax(X)[\bar{x}_2]
			& \mbox{if } \bar{x}_1 = \bar{x}_2 \\
    (0-\Softmax(X)[\bar{x}_2])\Softmax(X)[\bar{x}_1]
			& \mbox{otherwise}
  \end{array}
	\right. \\
	&= (\delta_{\bar{x}_1 \bar{x}_2} - \Softmax(X)[\bar{x}_2])
	\Softmax(X)[\bar{x}_1]
\end{align*}
And it looks like we're good. I wrote this proof before backreferencing the wiki
page, so that's a good sign.
\end{proof}
\end{landscape}

\begin{definition}[Feed Forward Neural Net Layer]
Feed forward neural nets are described all over the place so I won't go into too
much detail here. In short, for every layer except the input layer there is some
previous layer which feeds into current layer via a fully-connected network of
weights.

Let $L_{i-1}$ be the activations of the the previous layer and $L_i$ the activations
of the current layer. Each of these can be modeled as a multimatrix. As an the weighted
connections between them, $W_i$. Finally, there is an activation function
$a(X) : \mathbb{R}^{|X|} \to \mathbb{R}^{|X|}$ which is an elementwise function,
often the sigmoid function, or ReLU, or some variant thereof. It's unimportant for this
definition.

\[ L_i = a(L_{i-1} \mmult{||L_{i-1}||} W_i) \]
\end{definition}

\begin{theorem}[Derivative of FFNN Layer]
\[ \frac{dL_i}{dW_j} = ? \]
\end{theorem}
\begin{proof}
\begin{ppart} When $i < j$,
$L_i$ does not depend on $W_j$, so 
$\forall \bar{x} \in \Dim(|L_i|), \bar{y} \in \Dim(|W_j|)$
$\frac{dL_i}{dW_j} = 0$
\end{ppart}
\begin{ppart} $i = j$
TODO
\end{ppart}
\begin{ppart} $i > j$
TODO
\end{ppart}
\end{proof}

\chapter{Integration}

I think, I'm not sure, but I think that the integral of a multimatrix function should
be defined,

\[
 \int_A^B F(X) dX = \lim_{n \to \infty}
 \sum_{i=1}^n
  F\left(A+\frac{i}{n}(B-A)\right)
  \mmult{||X||}
  \left(\frac{B-A}{n}\right)
\]

Which, I have very little idea what that means geometriclly, but
if you let $G(X) = \int_A^X F(Y) dY$ you seem to end up with $dG/dX = F(X)$.

Maybe... well look at it this way. $F(X)$ is a function which produces some,
afine-like transformation multimatrix for multimatricies of shape $|X|$,
since it must have a right hand shape of $\ldots \oplus |X|$. This integration
occures over a linear path of $X$ as it moves from $A$ to $B$ applying that
transformation and adding up the results of the transformed multimatricies with
a weight proportional to the step size... and in the direction of the step.

So, it's sort of like a line integral but not really. They're related somehow.

\begin{landscape}
Let's see if I can prove that theory,
\begin{theorem}[Fundamental Rule of Multimatrix Calculus]
Let $G(X) = \int_A^X F(Y) dY$ then $dG(X)/dX = F(X)$.
\end{theorem}
\begin{proof}
\begin{align*}
  G(X)
  &= \int_A^X F(Y) dY \\
  &= 
    \lim_{n \to \infty}
     \sum_{i=1}^n
      F\left(A+\frac{i}{n}(X-A)\right)
      \mmult{||Y||}
      \left(\frac{X-A}{n}\right)
\end{align*}
\begin{alignat*}{3}
  \frac{dG(X)}{dX}
  &=&&
    \frac{
      d \left\{
      \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
        \mmult{||Y||}
        \left(\frac{X-A}{n}\right)
      \right\}
    }{dX} \\
  &=&&
    \lim_{n \to \infty}
    \frac{
      d \left\{
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
        \mmult{||Y||}
        \left(\frac{X-A}{n}\right)
      \right\}
    }{dX} \\
  &=&&
    \lim_{n \to \infty}
    \sum_{i=1}^n
      \frac{
        d \left\{
          F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \left(\frac{X-A}{n}\right)
        \right\}
      }{dX} \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \frac{d \left(\frac{X-A}{n}\right)}{dX}
        \\&&&+
        \Tran\left(
          \Tran\left(\frac{X-A}{n}, ||Y||\right)
          \mmult{||Y||}
          \Tran\left(\frac{
            d \left\{F\left(A+\frac{i}{n}(X-A)\right)\right\}
          }{dX}, |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right) \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        F\left(A+\frac{i}{n}(X-A)\right)
          \mmult{||Y||}
          \frac{1}{n}\Ident^2(|X|)
        \\&&&+
        \Tran\left(
          \frac{X-A}{n}
          \mmult{||Y||}
          \Tran\left(
            \frac{dF(X)}{dX}
            \mmult{|X|}
            \frac{i}{n}\Ident^2(|X|), |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right) \\
  &=&&
    \lim_{n \to \infty}
      \sum_{i=1}^n
        \frac{1}{n}F\left(A+\frac{i}{n}(X-A)\right)
        \\&&&+
        \Tran\left(
          \frac{X-A}{n}
          \mmult{||Y||}
          \Tran\left(
            \frac{i}{n}\frac{dF(X)}{dX}
            , |\vec{f}|\right),
          |\vec{x} \oplus \vec{x}|
        \right)
\end{alignat*}
At this point I'm fairly stuck.

Ok, lets think of this in another way. Is this true?
\[
	\lim_{\epsilon \to 0}
	\frac{
		G(X+\epsilon H) - G(X)
	}{\epsilon}
	=
	F(X) \mmult{||X||} H
\]
Where $SS(H) = 1$. More specifically, $H = \frac{X-A}{\sqrt{SS(X-A)}}$

\begin{align*}
\lim_{\epsilon \to 0}
\frac{
	G(X+\epsilon H) - G(X)
}{\epsilon}
&= 
\lim_{\epsilon \to 0}
\frac{
	\left( \int_A^{X+\epsilon H} F(Y) dY \right) - \left( \int_A^X F(Y) dY \right)
}{\epsilon} \\
&= 
\lim_{\epsilon \to 0}
\frac{
	\int_X^{X+\epsilon H} F(Y) dY
}{\epsilon}
\end{align*}
\end{proof}
\end{landscape}

\chapter{Solutions to the Exercises}

\section*{Chapter 1 Solutions}

\begin{solution}
\begin{enumerate}
\item[]
\item True
\item False, $\Dim(\left<5,5,5\right>)$ contains only vectors in $\mathbb{N}^3$ 
\item False, the last value of $\left<1,2,3\right>$, $3$, is greater than it's
						 corresponding value in $\left<3,2,1\right>$, which is $1$.
\item True
\end{enumerate}
\end{solution}

\begin{solution}
\[ \left|\frac{dF}{dX}\right| = \left<6,7,3,7,2\right> \]
\end{solution}

\begin{solution}
\[ \left|\frac{df}{dX}\right| = |X| = \left<2,3,2\right> \]
\[ \frac{df}{dX}[\bar{v}] = 1 \]
\end{solution}

\begin{solution}
\[ \left|\frac{df}{dX}\right| = |X| = \left<2,3,2\right> \]
\[ \frac{df}{dX}[\bar{v}] = 1 \]
\end{solution}

\section*{Chapter 2 Solutions}

\begin{solution}
\begin{enumerate}
\item[]
\item Invalid
\item $\left<3, 3\right>$
\item Invalid
\item $\left<1, 2, 2, 1\right>$
\end{enumerate}
\end{solution}


\bibliography{paper}
\bibliographystyle{ieeetr}

\end{document}
